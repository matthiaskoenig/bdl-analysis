---
title: "Statistical Analysis of Pathobiochemical Signatures in Bile Duct Ligated Mice"
author: '[Matthias Koenig](http://www.charite.de/sysbio/people/koenig) (`r Sys.Date()`)'
output:
  pdf_document:
    fig_caption: yes
    keep_tex: yes
    toc: yes
  html_document:
    pandoc_args:
    - +RTS
    - -K64m
    - -RTS
  word_document: default
header-includes: \usepackage{graphicx}
---
<!--
%\VignetteEngine{knitr}
%\VignetteIndexEntry{BDL raw data processing}
-->
```{r, options, echo=FALSE}
# set knitr option
# knitr::opts_chunk$set()
knitr::knit_hooks$set(htmlcap = function(before, options, envir) {
  if(!before) {
    paste('<p class="caption">',options$htmlcap,"</p>",sep="")
    }
})
```


# Introduction

This document contains the statistical analysis performed in the publication *Pathobiochemical signatures of cholestatic liver disease in bile duct ligated mice*.

A comprehensive data set of serum markers, histological parameters and transcript profiles was compiled at 8 time points after bile duct ligation (BDL) in mice, comprising different stages of the disease. The data set consists of $N_{r}=5$ repeats ($N_{r}=3$ for the measured antibodies) for $N_{t}=8$ time points denoted by $t_1,..., t_{N_t}$ consisting of a total $N_{f}=154$ measured parameters in the following referred to as factors (Fluidigm gene expression, antibodies, serum markers, histological measurements).

The main steps of the analysis comprise

* **Explorative data anaysis**
* **Dimension reduction via ANOVA**
* **Correlation analysis** based on time course correlation measure
* **Hierarchical clustering** 
* **Decision trees** for prediction

The complete data set, source code and documentation of this analysis is available from  
https://github.com/matthiaskoenig/bdl-analysis .

The following naming conventions are used

* **factor** : one of the measured quantities/parameters over time, i.e. either 
    + gene expression of a single gene (e.g. Actb); 
    + one of the biomarkers (e.g. ALT, albumin, bilirubin)
    + one of the histological markers (e.g. BrdU-positive Kupffer cells)
    + one of the antibodies (e.g. CTGF, S100A4)
* **time point** : a single value $t_i$ from the measured time points 0h (control), 6h, 12h, 18h, 30h, 2d, 5d, 14d
* **sample** : one of the $N_{t} \cdot N_{r}=40$ mice, i.e. a one of the repeats for a given time point

The results of the analysis are written to the results directory defined via the `BDL_RESULTS` environment variable. To reproduce this analysis create the respective variable.

```{r resultsPath, eval=TRUE, strip.white=TRUE}
# read the folder for results from environment variable
resultsPath <- Sys.getenv("BDL_RESULTS")
if (identical(resultsPath, "")){
  stop("No results folder defined, set the BDL_RESULTS environment variable")
} else {
  print(resultsPath)
}
# create data subfolder to store analysis results
dir.create(file.path(resultsPath, 'data'), showWarnings=FALSE)
```

# Explorative data analysis
## Data import
In a first step the processed data set is loaded from the `data` folder. The data consists of the time course data for all factors (`BDLdata`), additional information for the factors (`BDLfactors`), the sample definition, i.e. the assignment of sample ids to respective time point and repeat (`BDLsamples`), and a mapping of the Fluidigm (gene) probe ids to UniProt identifiers (`BDLprobes`).

```{r BDLdata, eval=TRUE, strip.white=TRUE}
suppressPackageStartupMessages(library(BDLanalysis))
suppressPackageStartupMessages(library(calibrate))
suppressPackageStartupMessages(library(pander))
# path definition
baseLoc <- system.file(package="BDLanalysis")
extPath <- file.path(baseLoc, "extdata")
# load data
data(BDLdata)
data(BDLsamples)
data(BDLfactors)
data(BDLprobes)
# counters
Nr <- 5  # repeats
Nt <- length(levels(BDLsamples$time_fac))  # time points
# store all data sets in the results folder
save(BDLdata, file=file.path(resultsPath, "data", "BDLdata.Rdata"))
save(BDLsamples, file=file.path(resultsPath, "data", "BDLsamples.Rdata"))
save(BDLfactors, file=file.path(resultsPath, "data", "BDLfactors.Rdata"))
save(BDLprobes, file=file.path(resultsPath, "data", "BDLprobes.Rdata"))
```

In addition to the individual data points per time point, the mean data averaged over the $N_{r}$ repeats per time points is used in parts of the correlation analysis. This mean factor data set is calculated once at the beginning via
```{r BDLmean}
BDLmean <- bdl_mean_data(BDLdata, BDLsamples)
BDLmean.time <- as.numeric(levels(as.factor(BDLsamples$time)))
```

In total `r ncol(BDLdata)` factors were measured in the this BDL study falling in the categories: `r levels(BDLfactors$ftype)`. The majority of factors belongs herby to the 3 fluidigm chips with 47 probes per chip.

An overview of the number of factors per category is provided in the following table
```{r factors}
cat_table <- as.data.frame(table(BDLfactors$ftype))
colnames(cat_table) <- c("Category", "Freq")
set.caption(sub(".", " ", "Factors per category", fixed = TRUE))
pander(cat_table)
```

## Data visualization
### Time course of single factors
In a first step overview plots of the raw and mean data for all individual factors are generated. These are available in the `resultsPath/factors` folder

```{r factorPlots, eval=TRUE}
# Create figures for all factors
factors_path <- file.path(resultsPath, 'factors')
dir.create(factors_path, showWarnings=FALSE)
plot_all_factors(path=factors_path)
```

The example for a single factor is depicted below, here for the factor `bilirubin`.
``` {r bilirubin, htmlcap='<b>Figure Single factor</b>: Plot of the raw time course data for a single factor, here for bilirubin. On the left the data is plotted against the time [h], on the right against the different time classes. Individual data points are depecticed in blue with the respective sample number shown next to the data points. The mean averaged of the repeats per time point are depicted in red. Box-and-whisker plots were added with default R parameters of boxwex=0.8, staplewex=0.5, outwex=0.5.'}
plot_single_factor('bilirubin', path=NULL)
```

### Time course of all factors (Heatmap)
In a next step the heatmap of the full data set was generated, i.e. of all time points and repeats. This provides a first overview over the complete data set. Rows correspond to the individual factors (factor order corresponding to the original data set: `GE_ADME`, `GE_Cytokines`, `GE_Fibrosis`, `Biochemistry`, `Histology`, `Antibodies`). Columns correspond to the 40 samples with 5 subsequent samples belonging to one of the 8 time points (with from left to right: `r levels(BDLsamples$time_fac)`). The data is row scaled, i.e. every individual factor is scaled to have mean zero and standard deviation one.
```{r heatmap}
suppressPackageStartupMessages(library(gplots))
suppressPackageStartupMessages(library("RColorBrewer"))
colors <- HeatmapColors() 
dtmp <- BDLdata

# create better row names
rownames(dtmp) <- paste(rownames(BDLsamples), BDLsamples$time_fac, sep=" ")
# create vectors for the horizontal and vertical lines
v_lines <- ((1:8)*5+0.5)
f_types <- c("Antibodies", "Histology", "Biochemistry", "GE_Fibrosis", "GE_Cytokines", "GE_ADME")
f_table <- table(BDLfactors$ftype)
h_lines <- 0.5 + cumsum(f_table[f_types])
# colors for the different data types

col2 <- HeatmapColors()
# define colors for type of experimentell data, i.e. factor groups
colorset <- brewer.pal(length(f_types), "Set2")
color.map <- function(factor_id) {return(colorset[ which(f_types==BDLfactors$ftype[which(BDLfactors$id==factor_id)]) ])}
factorColors <- unlist(lapply(BDLfactors$id, color.map))
```

``` {r heatmap2, fig.width=10, fig.height=10, error=TRUE, htmlcap='<b>Figure All factors </b>: Heatmap of the complete data set, i.e. all factors and repeats over time. The data is row scaled, i.e. every individual factor is scaled to have mean zero and standard deviation one, with positive Z-score in blue, negative Z-score in red. The row order is according to the the factor categories, the order within the fluidigm chips according to the order of the probes on the chip. the respective categories are depicted on the left.'}
heatmap.2(t(as.matrix(dtmp)), col=colors(100), scale="row", dendrogram="none", Rowv=NULL, Colv=NULL,
          key=TRUE, trace="none", cexRow=0.5, keysize=0.8, density.info="none",
          RowSideColors=factorColors,
          add.expr=abline(v=v_lines, h=h_lines, col="black", lwd=0.5),
          main="Heatmap of BDL time course data")
          # xlab="sample", ylab="factor")
legend("left",      # location of the legend on the heatmap plot
    inset=c(-0.03,0),
    legend = rev(f_types), # category labels
    col = rev(colorset),  # color key
    lty= 1,             # line style
    lwd = 10,            # line width
    cex = 0.7,
    bty="n"
)
```
**Results**: Various patterns are visible in the plotted raw data:

* **Two main classes of response can be observed**. One class with an increase in the early phase up to 6h after BDL (many of the ADME genes fall into this class) and a second class increasing in the later stage after 2-5 days after BDL. Many of the genes on the Cytokines and Fibrosis  chips as well as some of the biochemical, histological and antibody fall in this second class.
* **The individual animals show heterogeneous responses to BDL**. Within one time point the 5 repeats can show very different patterns. For instance at time 6h after BDL 3/5 of the mice show a marked increase in the ADME genes, whereas 2/5 do not show such a marked increase. Another example is the mice sample 27 at time 2d, with a high increase in the genes on the Fibrosis chip, which is not observed in the other 4 samples at time 2d.

## Actb quality control
Actb (Actin, cytoplasmic 1) probes were included on all Fluidigm chips (`GE_ADME`, `GE_Cytokines`, `GE_Fibrosis`) and not used in the normalization of the gene expression data. Hence, ActB can serve as quality control for the technical reproducibility of the Fluidigm chips. If the data is reproducible between chips the pairwise correlation between all individual Actb measurements should have high correlation coefficients close to 1. Plotting the data of the Actb measurements of two chips against each other should lie on a straight line
```{r CheckActB}
# Actb control figure
plot_actb_control <- function(path=NULL){
  if (!is.null(path)){
    png(filename=file.path(path, "Actb_control.png"), width=1600, height=1000, res=150)  
  }
  par(mfrow=c(2,3))
  plot_single("Actb")
  plot_single("Actb.x")
  plot_single("Actb.y")
  plot_cor_pair("Actb", "Actb.x", single_plots=FALSE)
  plot_cor_pair("Actb", "Actb.y", single_plots=FALSE)
  plot_cor_pair("Actb.x", "Actb.y", single_plots=FALSE)
  par(mfrow=c(1,1))
  if (!is.null(path)){
    invisible(dev.off())
  }
}
plot_actb_control(path=resultsPath)

# calculate Spearman and Pearson correlation coefficients on N=8*5=40 data points 
actb.spearman <- cor(data.frame(Actb=BDLdata$Actb, 
                                Actb.x=BDLdata$Actb.x, 
                                Actb.y=BDLdata$Actb.y), method="spearman")
actb.pearson <- cor(data.frame(Actb=BDLdata$Actb, 
                               Actb.x=BDLdata$Actb.x, 
                               Actb.y=BDLdata$Actb.y), method="pearson")

# table of correlation coefficients
set.caption(sub(".", " ", "Spearman correlation of Actb controls", fixed = TRUE))
pander(round(actb.spearman, digits=3))
set.caption(sub(".", " ", "Pearson correlation of Actb controls", fixed = TRUE))
pander(round(actb.pearson, digits=3))
```

``` {r fig.width=10, fig.height=7,  htmlcap='<b>Figure Actb control</b>: Correlation plot of the Actb probes from the 3 Fluidigm chips: Actb (fibrosis), Actb.x (ADME), Actb.y (Cytokines). The top row shows the individual time courses, the bottom row the pair wise plot of individual data points.'}
plot_actb_control(path=NULL)
```
**Results**: The Actb Fluidigm gene expression measurements are highly reproducible for the measured chips,  with Spearman as well as Pearson correlation coefficients all > 0.9 for pairwise Actb comparison.

# Dimension reduction via ANOVA
## ANOVA for single factor
A one-way analysis of variance (ANOVA) was applied to reduce the factors to the subset showing significant ($p_{adjusted}< 0.05$) changes during the time course. In its simplest form, ANOVA provides a statistical test of whether or not the means of several groups are equal, and therefore generalizes the t-test to more than two groups, with the groups being the sampled time points. The Holm's procedure was used to correct the p-values for any artificial p-value inflation due to multiple testing. 

For every of the individual factors in the BDL data set an ANOVA was calculated. Dimension reduction of the BDL data set was than performed by filtering out factors which did not significantly changing over time.

The `BDLdata` data set is reshaped into matrix format for the ANOVA calculation, with time points in rows and repeats as columns for every factor. 
```{r BDLmatrix}
BDLmatrices <- bdl_matrix_data(BDLdata, BDLsamples)
```

The following shows the ANOVA calculation for a single factor, here for `bilirubin`. 
``` {r singleanova}
  # example ANOVA for one factor
  mat.anova <- t(BDLmatrices[['bilirubin']])
  colnames(mat.anova) <- levels(BDLsamples$time_fac)
  
  # concatenate the data rows of df1 into a single vector r .
  r = c(t(as.matrix(mat.anova)))  # response data 
  
  # assign new variables for the treatment levels and number of observations.
  f = levels(BDLsamples$time_fac)   # treatment levels 
  k = 8                          # number of treatment levels 
  n = 5                          # observations per treatment 
  
  # create a vector of treatment factors that corresponds to each element of r in step 3 with the gl function.
  tm <- gl(k, 1, n*k, factor(f))   # matching treatments 
  
  # apply the function aov to a formula that describes the response r by the treatment factor tm.
  # fit an analysis of variance model
  av <- aov(r ~ tm) 
  
  # print out the ANOVA table with the summary function. 
  summary(av)
  # print the corresponding p-value
  p.value <- summary(av)[[1]][["Pr(>F)"]][[1]]

  # show data matrix
  print(mat.anova)
```

## ANOVA for all factors
Analog to the single factor ANOVA, the ANOVA is performed on all the factors. Hereby, a multitude of tests are performed, namely an ANOVA for every single factor. Consequently, the reported p-values of the ANOVA have to be adjusted via multiple testing procedures. Using the p.adjust function which given a set of p-values, returns p-values adjusted using one of several methods. The Bonferroni, Holm, Hochberg, Hommel are designed to give strong control of the family-wise error rate. We used the Holm's method for adjustment (*Holm, S. (1979). A simple sequentially rejective multiple test procedure. Scandinavian Journal of Statistics 6, 65-70.*).


```{r ANOVA}
# Calculation of ANOVA for all factors
df.anova <- all_factor_anova()
df.anova$sig <- sapply(df.anova$p.value, significant_code)  # add significant codes

df.anova$p.holm <- p.adjust(df.anova$p.value, method ="holm" , n = length(df.anova$p.value))
df.anova$sig.holm <- sapply(df.anova$p.holm, significant_code) 

# order factors by adjusted p-values
df.anova.ordered <- df.anova[with(df.anova, order(p.holm)), ]
df.anova.ordered

# save results
write.table(df.anova.ordered, file=file.path(resultsPath, "data", 'BDLanova.csv'), sep="\t", quote=FALSE)
BDLanova <- df.anova
save(df.anova, file=file.path(resultsPath, "data", "BDLanova.Rdata"))
```

## Filter factors
The factors are filtered based on the respective acceptance level, with the cutoff for the adjusted p-value being $p_{accept}$, i.e. all factors with a ANOVA with $p_{adjusted} \ge p_{accept}$ are filtered out. The filtered raw data is available as `BDLdata.fil`, the filtered mean data set as `BDLmean.fil`. All subsequent analyses are performed on the filtered data set, which is depicted in the following heatmap
``` {r filter}
p.accept = 0.05  # acceptance level
idx.accept = (df.anova$p.holm < p.accept)  # accepted subset
# subset of filtered data
BDLdata.fil <- BDLdata[, idx.accept]
BDLmean.fil <- BDLdata[, idx.accept]

# accepted
table(df.anova$p.holm<p.accept)  # 64 rejected / 90 accepted (adjusted)

# which factors were accepted in the various categories
fil_tab <- data.frame(
  table(BDLfactors$ftype[idx.accept]),
  table(BDLfactors$ftype),
  round(table(BDLfactors$ftype[idx.accept])/table(BDLfactors$ftype), 2)
)
fil_tab <- fil_tab[, c('Var1', 'Freq', 'Freq.1', 'Freq.2')]
names(fil_tab) <- c('Category', 'Accepted', 'All', 'Percent')
fil_tab
```
Almost all `Cytokines` genes are retained in the data set whereas many of the `ADME` and `Fibrosis` genes are filtered.

### Heatmap of filtered time course data
The heatmap of the filtered raw data is depicted below
``` {r heatmapFiltered}
# prepare data with row names
dtmp <- BDLdata.fil
rownames(dtmp) <- paste(rownames(BDLsamples), BDLsamples$time_fac, sep=" ")
# create vectors for the horizontal and vertical lines
v_lines <- ((1:8)*5+0.5)
f_types <- c("Antibodies", "Histology", "Biochemistry", "GE_Fibrosis", "GE_Cytokines", "GE_ADME")

# define colors for categories
colorset <- brewer.pal(length(f_types), "Set2")
color.map <- function(factor_id) {return(colorset[ which(f_types==BDLfactors$ftype[which(BDLfactors$id==factor_id)]) ])}
factorColors <- unlist(lapply(colnames(BDLdata.fil), color.map))
```

``` {r heatmap_filtered, fig.width=10, fig.height=10, error=TRUE, htmlcap='<b>Figure Heatmap ANOVA:</b> Plot of ANOVA filtered data set.'}
plot_time_heatmap <- function(dtmp, path=NULL){
  if (!is.null(path)){
    png(filename=file.path(path, "Factor_time_heatmap_filtered.png"), width=2000, height=2000, res=250)  
  }
  heatmap.2(t(as.matrix(dtmp)), col=colors(100), scale="row", dendrogram="none", 
            Rowv=NULL, Colv=NULL,
            key=TRUE, trace="none", cexRow=0.5, keysize=0.8, density.info="none",
            RowSideColors=factorColors,
            add.expr=abline(v=v_lines, col="black", lwd=0.5),
            main="ANOVA Filtered BDL factors")
            # xlab="sample", ylab="factor")
  legend("left",      # location of the legend on the heatmap plot
      inset=c(-0.03,0),
      legend = rev(f_types), # category labels
      col = rev(colorset),  # color key
      lty= 1,             # line style
      lwd = 10,            # line width
      cex = 0.7,
      bty="n"
  )
  if (!is.null(path)){
    invisible(dev.off())
  }
}
plot_time_heatmap(dtmp)
plot_time_heatmap(dtmp, resultsPath)
```


# Correlation analysis
For the correlation analysis between factors and the subsequent cluster analysis a correlation measure for time series data {Son2008} in combination with Complete-Linkage hierarchical clustering was used. This combination of methods provided the best enrichments on gene-expression time-series in a recent comparisons of methods {Jaskowiak2014, Jaskowiak2013} testing various correlation measures and clustering algorithms. 

The calculation of correlation coefficients between factors i and j ($i,j=1, ...,N_p$) was performed using the slightly modified correlation coefficient based similarity measure developed for clustering of time-course data ($Y_{i,j}^{S2}$ and $Y_{i,j}^{R2}$) {Son2008}. $Y_{i,j}^{S2}$ and $Y_{i,j}^{R2}$ are linear combinations of (i) a classical correlation part based on Spearman correlation $S_{i,j}^{*}$ in case of $Y_{i,j}^{S2}$ or Pearson $R_{i,j}^{*}$ in case of $Y_{i,j}^{R2}$, (ii) a component $A_{i,j}^{*}$ accounting for the similarity in changes between two time courses, (iii) a component $M_{i,j}^{*}$ comparing the location of minimum and maximum values of the time course (see {Son2008} for definitions)
$$Y_{i,j}^{S2} = w_1 S_{i,j}^{*} + w_2 A_{i,j}^{*} + w_3 M_{i,j}^{*}$$
$$Y_{i,j}^{R2} = w_1 R_{i,j}^{*} + w_2 A_{i,j}^{*} + w_3 M_{i,j}^{*}$$
$R_{i,j}^{*}$ and $S_{i,j}^{*}$ are hereby calculated on the individual data points for the factors i and j, $A_{i,j}^{*}$ and $M_{i,j}^{*}$ on the mean time courses averaged over the $N_r$ repeated measurements. Throughout the analysis the following weights were used $w_1=0.5$, $w_2=0.3$, $w_3=0.2$.

In the calculation of the change component we used a Spearman correlation based measure ($A^{**}$) instead of the originally proposed Pearson measure ($A^{*}$) resulting in the correlation scores $Y_{i,j}^{S3}$ and $Y_{i,j}^{S3}$
$$Y_{i,j}^{S3} = w_1 S_{i,j}^{*} + w_2 A_{i,j}^{**} + w_3 M_{i,j}^{*}$$
$$Y_{i,j}^{R3} = w_1 R_{i,j}^{*} + w_2 A_{i,j}^{**} + w_3 M_{i,j}^{*}$$
Herein, $A_{i,j}^{**}$ calculates the correlation of changes between factors i and j based on Spearman correlation analog $A_{i,j}^{*}$ as
$$A_{i,j}^{**}=(S(d_i,d_j)+1)/2$$
$$A_{i,j}^{*}=(R(d_i,d_j)+1)/2$$
The reason for this adaption was that initial analysis showed a strong dependency of the change components on outliers.

All calculated correlation scores $Y^{S}$ and $Y^{R}$ are transformed from [0, 1] to [-1, 1] via
$$Y_{norm}^{S} = 2(Y^{S}-0.5)$$
$$Y_{norm}^{R} = 2(Y^{R}-0.5)$$

In addition to the used $Y^{S}$ and $Y^{R}$ correlation scores Pearson (R) and Spearman (S) correlations were calculated for comparison. 

## Pearson & Spearman correlation
In a first step Pearson $R_{i,j}$ and Spearman $S_{i,j}$ correlation was calculated for the filtered factors. 
Heatmaps of the resulting correlation matrices are depicted below with reordering of factors based on hierarchical clustering with Complete Linkage.

``` {r correlation}
suppressPackageStartupMessages(library(corrplot))
dir.create(file.path(resultsPath, 'correlation'), showWarnings=FALSE)

# correlation matrices
cor.pearson <- cor(BDLdata.fil, method="pearson", use="pairwise.complete.obs")
cor.spearman <- cor(BDLdata.fil, method="spearman", use="pairwise.complete.obs")

# Helper function for creating correlation plot and saving to results folder.
# TODO: update the corrplot to use the heatmap.2 (with color based on histopathology)
f_corrplot <- function(name, data, path=NULL,
                       width=3000, height=3000, res=200){
  if (!is.null(path)){
    png(filename=file.path(path, "correlation", sprintf("%s_original.png", name)),
        width=width, height=height, res=res)  
  }
  
  # Heatmap of correlation matrix
  col2 <- HeatmapColors()  # color palette for correlation (red - white - blue)
  heatmap.2(data, col=col2(10), scale="none",
          key=TRUE, symkey=FALSE, trace="none", cexRow=0.8, cexCol=0.8,
          main=name,
          density.info="none", dendrogram="none", Rowv=NULL, Colv=NULL, keysize=0.8,
          key.xlab = "ys3 correlation",
          #revC=TRUE,
          sepwidth=c(0.01,0.01),
          sepcolor="black",
          colsep=1:ncol(cor.cluster),
          rowsep=1:nrow(cor.cluster))

  if (!is.null(path)){
    invisible(dev.off())  
  }
}

# Spearman
f_corrplot("cor.spearman", data=cor.spearman, path=resultsPath)
# Pearson
f_corrplot("cor.pearson", data=cor.pearson, path=resultsPath)
```

```{r}
# Pearson correlation (no clustering)
f_corrplot("cor.pearson", data=cor.pearson, path=NULL)
```

```{r}
# Spearman correlation (no clustering)
f_corrplot("cor.spearman", data=cor.spearman, folder=NULL)
```
The pearson correlation is highly sensitive to outliers (correlation between factors, but also in the changes between time points). 


## YS & YR correlation
Here, the time-course based correlation measurements $Y^{S1}$, $Y^{S2}$, $Y^{S3}$, $Y^{R1}$, $Y^{R2}$ and $Y^{R3}$ are calculated for the factors in the filtered BDL data set. To create the correlation matrix all pairwise correlations between all factors are calculated.

``` {r ys1_yr1}
# calculate ys1, yr1 on mean data, i.e. correlation part (S*), slope part (A) and min/max part (M) are all calculated on the mean data of all repeats.
# w <- list(w1=0.5, w2=0.25, w3=0.25)
w <- list(w1=0.5, w2=0.3, w3=0.2)

# calculate the YSR component matrices on the filtered data set (A, A*, A**, M, M*)
# all components are calculated on the mean data
ysr.res <- ysr.matrices(BDLmean.fil, BDLmean.time, use="pairwise.complete.obs")

# Pearson & spearman correlation on individual data points
cor.S_star <- ( cor(BDLdata.fil, method="spearman", use="pairwise.complete.obs") + 1 )/2
cor.R_star <- ( cor(BDLdata.fil, method="pearson", use="pairwise.complete.obs") + 1 )/2

# Calculate the ys(1,2,3) and yr(1,2,3) with single time point correlation for
# Spearman, respectively Pearson (instead of mean)
# Takes the individual correlation, slope and min/max components for the respective
# score parts and weights with the provided weighting factors (w1,w2,w3)
cor.ys1.raw <- w$w1*cor.S_star + w$w2*ysr.res$A       + w$w3*ysr.res$M
cor.ys2.raw <- w$w1*cor.S_star + w$w2*ysr.res$A_star  + w$w3*ysr.res$M_star
cor.yr1.raw <- w$w1*cor.R_star + w$w2*ysr.res$A       + w$w3*ysr.res$M
cor.yr2.raw <- w$w1*cor.R_star + w$w2*ysr.res$A_star  + w$w3*ysr.res$M_star
# extended
cor.ys3.raw <- w$w1*cor.S_star + w$w2*ysr.res$A_star2 + w$w3*ysr.res$M_star
cor.yr3.raw <- w$w1*cor.R_star + w$w2*ysr.res$A_star2 + w$w3*ysr.res$M_star 

# scaling of correlation coefficient in interval [-1,1]
cor.ys1 <- 2*(cor.ys1.raw-0.5)
cor.ys2 <- 2*(cor.ys2.raw-0.5)
cor.ys3 <- 2*(cor.ys3.raw-0.5)
cor.yr1 <- 2*(cor.yr1.raw-0.5)
cor.yr2 <- 2*(cor.yr2.raw-0.5)
cor.yr3 <- 2*(cor.yr3.raw-0.5)

# plot subset for further analysis
f_corrplot("cor.ys3", data=cor.ys3, order="hclust", folder=NULL)

# create correlation plot on disk
f_corrplot("cor.ys1", data=cor.ys1, order="hclust", folder=resultsPath)
f_corrplot("cor.ys2", data=cor.ys2, order="hclust", folder=resultsPath)
f_corrplot("cor.ys3", data=cor.ys3, order="hclust", folder=resultsPath)
f_corrplot("cor.yr1", data=cor.yr1, order="hclust", folder=resultsPath)
f_corrplot("cor.yr2", data=cor.yr2, order="hclust", folder=resultsPath)
f_corrplot("cor.yr3", data=cor.yr3, order="hclust", folder=resultsPath)
```


# Hierarchical clustering
Based on the calculated correlation matrices between factors hierarchical clustering is performed on the correlation matrices using complete linkage (hclust).

The next step of dimension reduction is clustering of the correlation matrix. This groups the factors into sets with the correlation within sets being larger than between sets. This effectivly finds groups of factors which have similar time courses.

The hclust function in R was used for clustering with complete linkage method for hierarchical clustering. This particular clustering method defines the cluster distance between two clusters to be the maximum distance between their individual components. An overview of the clusters is given in the `results/cluster` folder.

In a first step all factors are normalized for comparison based on their range.

``` {r}
suppressPackageStartupMessages(library('matrixStats'))

# Normalize the individual factors so that the the individual factors are comparable
f_normalize_centering <- function(a, min.a, max.a, mean.a){
  res <- (a - mean.a)/(max.a - min.a)
}

# The result are normalization constants for all factors. These normalization
# factors are used in the following to normalized new observations for the predictions.
factor.norm <- data.frame(min=apply(BDLdata, 2, min, na.rm=TRUE), 
                            max=apply(BDLdata, 2, max, na.rm=TRUE),
                            mean=apply(BDLdata, 2, mean, na.rm=TRUE))

normalize_BDLdata <- function(data, factor.norm){
  dnorm <- data
  for (name in colnames(data)){
    dnorm[, name] <- f_normalize_centering(a=dnorm[, name], 
                                           min.a=factor.norm[name, "min"],
                                           max.a=factor.norm[name, "max"],
                                           mean.a=factor.norm[name, "mean"]) 
  }
  return(dnorm)
}

# Normalize the full data set
BDLdata.norm <- normalize_BDLdata(data=BDLdata, factor.norm=factor.norm)
# Calculate the mean of the normalized data set
BDLmean.norm <- bdl_mean_data(BDLdata.norm, BDLsamples)
```

Correlation matrices for multiple methods have been calculated in the analysis. 
The following function provides access to the respective correlation matrix based
on the method. Within the publication the `ys3` correlation is used.
```{r correlationMatrixForMethod}
# TODO: directly store in the creation of the matrices

dir.create(file.path(resultsPath, 'cluster'), showWarnings=FALSE)  # directory for cluster results
# list of used correlation methods
correlation_methods <- c("pearson", "spearman", "ys1", "ys2", "ys3", "yr1", "yr2", "yr3")

# Full set of correlation matrices based on the various used methods. Provides simple access to the respective correlation matrix. 
cor.matrices <- vector("list", length=length(correlation_methods))
names(cor.matrices) <- correlation_methods
cor.matrices$pearson <- cor.pearson
cor.matrices$spearman <- cor.spearman
cor.matrices$ys1 <- cor.ys1
cor.matrices$ys2 <- cor.ys2
cor.matrices$ys3 <- cor.ys3
cor.matrices$yr1 <- cor.yr1
cor.matrices$yr2 <- cor.yr2
cor.matrices$yr3 <- cor.yr3
# Save correlation matrices
save(cor.matrices, file=file.path(resultsPath, "data", "cor.matrices.Rdata"))
```

The hierarchical clustering is now calculated based on the respective correlation matrices. Hierarchical clustering based on the given correlation matrix is applied with `N=6` clusters. For every correlation measure the cluster plots are created.
``` {r hierarchical clustering}
hclust.res <- vector("list", length=length(correlation_methods))
names(hclust.res) <- correlation_methods

Ngroups <-  6  # number of clusters
for (method in correlation_methods){
  # get correlation matrix
  cor.cluster <- cor.matrices[[method]]
  # perform hierarchical clustering
  hc <- hclust(dist(cor.cluster, method="euclidian"), method="complete") 
  # cut into the clusters
  groups <- cutree(hc, k=Ngroups)
  groups.hc.order <- groups[hc$order]
  # store results
  hclust.res[[method]] <- list(hc=hc,
                               groups=groups,
                               groups.hc.order=groups.hc.order)
}
# save clustering
save(hclust.res, file=file.path(resultsPath, "data", "hclust.res.Rdata"))
```

Plots of the mean time course in the clusters and the reresentatives in the clusters.
``` {r plotClusters}
# Plot mean cluster with SD range and the individual representatives in the cluster.
plot_clusters_mean <- function(method, path=NULL){  
  hc.res <- hclust.res[[method]]  # clustering for correlation method
  groups.hc.order <- hc.res$groups.hc.order
  # create the figure
  if (!is.null(path)){
    fname <- sprintf("%s_cluster_mean.png", method)
    png(filename=file.path(path, fname), width=3000, height=2100, res=300)
  }
  # par(mfrow=c(ceiling(sqrt(Ngroups)),ceiling(sqrt(Ngroups))))
  par(mfrow=c(2,3))
  steps <- 1:Nt  # time points
    for (k in 1:Ngroups){
      # get representatives of cluster
      g <- groups.hc.order[groups.hc.order==k]
      dgroup <- BDLmean.norm[names(g)]  # normalized mean data for group (normalized within factor)
    
      # mean and sd for timepoints (i.e. over all factors in the cluster) 
      g.mean <- rowMeans(as.matrix(dgroup), na.rm=TRUE)
      g.sd <- rowSds(as.matrix(dgroup), na.rm=TRUE) 
      
      # plot sd range
      plot(factor(levels(BDLsamples$time_fac), levels=levels(BDLsamples$time_fac)), rep(-2, 8), type="n", xlab="", ylab="",
           xlim=c(1, Nt), ylim=1.1*c( min(min(dgroup, na.rm=TRUE), na.rm=TRUE), 
                                      max(max(dgroup, na.rm=TRUE), na.rm=TRUE) ),
           main=sprintf("%s : Cluster %s (N=%s)", method, k, ncol(dgroup)))
      polygon(c(steps, rev(steps)), c(g.mean+g.sd, rev(g.mean-g.sd)),
              col = rgb(0.5,0.5,0.5,0.5), border = NA)
      
      # individual data
      for (name in names(g)){
        points(steps, dgroup[, name], pch=16, col="black")
        lines(steps, dgroup[, name], col=rgb(0.5,0.5,0.5, 0.8), lwd=1)
      }
      # mean over factors in cluster
      lines(steps, g.mean, col="blue", lwd=2)
    }
    par(mfrow=c(1,1))
  if (!is.null(path)){
    invisible(dev.off())
  }
}

# plot of individual repeat clusters
plot_clusters_single <- function(method, path=NULL){  
  hc.res <- hclust.res[[method]]  # clustering for correlation method
  groups.hc.order <- hc.res$groups.hc.order
  
  if (!is.null(path)){
    fname <- sprintf("%s_cluster_individual.png", method)
    png(filename=file.path(path, fname), width=1600, height=1600, res=200)
  }
  
  par(mfrow=c(2,3))
  steps <- 1:Nt # time points
  for (k in 1:Ngroups){
    g <- groups.hc.order[groups.hc.order==k]
    N <- ceiling(sqrt(length(g)))
    dgroup <- BDLdata.norm[names(g)]  # get normalized individual data
    g.mean <- rowMeans(as.matrix(dgroup), na.rm=TRUE)  # mean for all factors for given time point and sample
    
    plot(1, type="n", xlab="", ylab="", xlim=c(1, Nt), ylim=c(-1, 1), main=sprintf("%s : Cluster %s (N=%s)", method, k, ncol(dgroup)))
    
    # individual data
    for (name in names(g)){
      points(BDLsamples$time_point, dgroup[, name], pch=16, col="black")
      for (kr in 1:Nr){
        lines(BDLsamples$time_point[(1:Nt -1)*Nr+kr], dgroup[(1:Nt -1)*Nr+kr, name], col=rgb(0.5,0.5,0.5, 0.8), lwd=1)
      }
    }
    # mean over factors in cluster
    # points(BDLsamples$time_point, g.mean, col="blue", lwd=1, cex=1)
    for (kr in 1:Nr){
      lines(BDLsamples$time_point[(1:Nt -1)*Nr+kr], g.mean[(1:Nt -1)*Nr+kr], col="blue", lwd=2, cex=1.1)
    }
  }
  par(mfrow=c(1,1))
  if (!is.null(path)){
    invisible(dev.off())
  }
}

# Plot individual time courses in cluster
plot_clusters_items <- function(method, path=NULL){
  # get the cluster assignment for the given method
  hc.res <- hclust.res[[method]]
  groups.hc.order <- hc.res$groups.hc.order
  
  for (k in 1:Ngroups){
    if (!is.null(path)){
      fname <- sprintf("%s_cluster_%s.png", method, k)
      png(filename=file.path(path, fname), width=3000, height=3000, res=200)  
    }
    
    g <- groups.hc.order[groups.hc.order==k]
    N <- ceiling(sqrt(length(g)))
    par(mfrow=c(N,N))
    for (name in names(g)){
      plot_single(name_A=name) 
    }
    par(mfrow=c(1,1))  
    if (!is.null(path)){
      invisible(dev.off())
    }
  }  
}

# create plots for clusters based on the different correlation methods
cluster_path <- file.path(resultsPath, "cluster")
for (method in correlation_methods){
  
  # plot all clusters on disk
  plot_clusters_mean(method=method, path=cluster_path) 
  plot_clusters_single(method=method, path=cluster_path) 

  # plot individual factors in clusters on disk
  plot_clusters_items(method=method, path=cluster_path)
}
```

All Pearson based correlation scores showed dependencies on data outliers. Consequently, we used $Y^{S3}$ which is based on a Spearman based correlation component $S^{*}$ and change component $A^{**}$.  
Now the hierarchical clustering based on complete linkage using euclidian distance is calcualted for the given correlation matrices and the factors are cut into the clusters.

``` {r cluster_mean, fig.width=10, fig.height=8, error=TRUE, htmlcap='<b>Figure Cluster mean:</b> Plot of mean cluster data.'}
# plot the ys3 clusters
plot_clusters_mean(method="ys3", path=NULL)
```
``` {r cluster_repeat, fig.width=10, fig.height=8, error=TRUE, htmlcap='<b>Figure Cluster repeat:</b> Plot of repeat cluster data.'}
plot_clusters_single(method="ys3", path=NULL)
```

The following factors are in the `ys3` time course clusters
``` {r}
# print representatives of the ys3 based clusters 
groups.ys3 <- (hclust.res$ys3)$groups
for (k in 1:Ngroups){
  g <- groups.ys3[groups.ys3==k]
  cat(sprintf("------------------------------------------\n", k, length(g)))
  cat(sprintf("Cluster %s (N=%s)\n", k, length(g)))
  cat(sprintf("------------------------------------------\n", k, length(g)))
  print(names(g))
  # print(paste(names(g), sep=", ", collapse =", " ))
}
```

Display the clusters with the heatmap
``` {r heatmap_ys3, fig.width=10, fig.height=10, error=TRUE, htmlcap='<b>Figure Heatmap Cluster:</b> Plot of ANOVA filtered data set.'}

# Create correlation heatmap with hierachical clustering results
plot_cluster_heatmap <- function(method){
  # matrix and cluster results
  cor.cluster <- cor.matrices[[method]]
  hc.res <- hclust.res[[method]]
  hc <- hc.res$hc
  groups <- hc.res$groups
  # correlation colors
  col2 <- HeatmapColors()
  # cluster colors
  colorset <- brewer.pal(Ngroups, "Set1")
  color.map <- function(cluster_id) {return(colorset[cluster_id])}
  clusterColors <- unlist(lapply(groups, color.map))
  
  heatmap.2(cor.cluster, col=col2(10), scale="none",
          key=TRUE, symkey=FALSE, trace="none", cexRow=0.8, cexCol=0.8,
          main=method,
          density.info="none", dendrogram="column", Rowv=as.dendrogram(hc), Colv=as.dendrogram(hc), keysize=0.8,
          key.xlab = "ys3 correlation",
          ColSideColors=clusterColors, revC=TRUE,
          sepwidth=c(0.01,0.01),
          sepcolor="black",
          colsep=1:ncol(cor.cluster),
          rowsep=1:nrow(cor.cluster),
          margins=c(12,8))
  legend("left", legend=paste("c", 1:6, sep=""), col= unlist(lapply(1:6, color.map)), pch=15, bty="n")
}

# plot to file
# png(filename=file.path(resultsPath, "correlation", "ys3_correlation.png"), width=2500, height=2500, res=200) 
pdf(file.path(resultsPath, "correlation", "ys3_correlation.pdf"), 
    width=10, height=10, pointsize=12) 
plot_cluster_heatmap(method="ys3")
invisible(dev.off())

# plot to report
plot_cluster_heatmap(method="ys3")
```

Plot correlation matrix for histological, biochemical and antibody markers. Calculation of the largest correlation between the non-transcript factors and other factors.
The columns in which no correlation coefficient has `abs(value) >= cor.cutoff` are filtered out, i.e. only factors are retained with a absolute correlation coeffient above this threshold.
``` {r correlation_histological}
# correlation subset for non RNA factors which were accepted by ANOVA
hist_facs <- BDLfactors$id[idx.accept & (BDLfactors$ftype %in% c("Antibodies", "Biochemistry", "Histology"))]
hist_facs

# get the indices of these factors in the correlation matrix
hist_idx <- rep(NA, length(hist_facs))
for (k in 1:length(hist_facs)){
  hist_idx[k] <- which(colnames(cor.matrices[["ys3"]]) == hist_facs[k])
}
# subset of correlation matrix for histological markers (corresponding rows)
hist_data <-  (cor.matrices[["ys3"]])[hist_idx, ]  # rows of correlation matrix for the factors
hist_data

# filter columns
cor.cutoff = 0.6
col.accept <- rep(NA, ncol(hist_data))
for (k in 1:ncol(hist_data)){
  col.accept[k] <- any( abs(hist_data[,k])>=cor.cutoff )
}
table(col.accept)  # how many factors are filtered based on correlation threshold
hist_accept <- hist_data[, col.accept]


# sort by the hierarchical cluster ordering
hist_gene_names <- colnames(hist_accept)[1:(ncol(hist_accept)-nrow(hist_accept))]
rownames(hist_accept)

hc.res <- hclust.res$ys3
hc.res$groups.hc.order

# create sort index for gene names based on clustering
sort_idx <- rep(NA, length(hist_gene_names))
for (k in 1:length(hist_gene_names)){
  sort_idx[k] <- which(names(hc.res$groups.hc.order) == hist_gene_names[k])
}
# first sorted genes than the histological factors
hist_sorted <- hist_accept[, c(hist_gene_names[order(sort_idx)], rownames(hist_data))]



# plot the subset of the correlation matrix to file
plot_hist_corr <- function(hist_data){
  corrplot(hist_data, method="circle", type="full", 
           tl.cex=0.7, tl.col="black", col=col2(10))  
}

# plot to file
pdf(file.path(resultsPath, "correlation", "histological_correlation.pdf"), 
    width=10, height=4, pointsize=12) 
plot_hist_corr(hist_sorted)
invisible(dev.off())

# plot in report
plot_hist_corr(hist_sorted)
# print correlation values
print(t(round(hist_sorted, digits=2)))
```

Calculation of top correlations for every non-RNA factor with all RNA factors sorted by absolute values.
``` {r}
plot_hist_topcors <- function(labels=TRUE, mfrow=c(10,1)){
  par(mfrow=mfrow)
  for (name in hist_facs){
    # get the elements which are not histological factors
    v <- cor.matrices$ys3[!(colnames(cor.ys3) %in% hist_facs), name]
    # sort by absolute correlation
    v.sorted <- rev(v[order(abs(v))])
    print('----------------------------')
    print(name)
    # print(v.sorted[v.sorted>0.5])
    print(v.sorted[2:11])
    mv <- t(as.matrix(v.sorted[2:11]))
    rownames(mv) <- c(name)
    # plot without labels to have identical size for figure
    if (labels==FALSE){
      rownames(mv) <- NULL
      colnames(mv) <- NULL
    }
    corrplot(mv, method="pie", type="full", 
             tl.cex=1.0, tl.col="black", col=col2(100), insig="p-value", sig.level=-1,
             p.mat=mv, cl.pos="n")
  }
  par(mfrow=c(1,1))
}
# files for figures
pdf(file.path(resultsPath, "correlation", "histological_topcors_1.pdf"), 
    width=10, height=10, pointsize=12)  
plot_hist_topcors(labels=TRUE)
dev.off()
pdf(file.path(resultsPath, "correlation", "histological_topcors_2.pdf"), 
    width=10, height=10, pointsize=12)  
plot_hist_topcors(labels=FALSE)
dev.off()


# plot in report
plot_hist_topcors(labels=TRUE, mfrow=c(5,2))
```


Calculation of the top cluster representatives.
``` {r topClusterRepresentatives}
# Get the ys3 cluster definition
hc.res <- hclust.res$ys3
groups.hc.order <- hc.res$groups.hc.order

plot_top_cluster_representatives <- function(Ntop=11, labels=TRUE){
  par(mfrow=c(Ngroups, 1))
  # for every cluster
  for (k in 1:Ngroups){
      # get members of cluster
      g <- groups.hc.order[groups.hc.order==k]
      # get normalized data for members of cluster
      dgroup <- BDLdata.norm[names(g)]
      # add the cluster mean as factor (mean of all factors in cluster for given time point and sample)
      g.mean <- rowMeans(as.matrix(dgroup), na.rm=TRUE)
      dgroup[["cluster.mean"]] <- g.mean
      
      # calculate mean averaged over repeats
      dgroup.mean <- bdl_mean_data(dgroup, BDLsamples)
      
      # now calculate the ys3 correlation with the created data.frame containing the mean cluster factor
      ysr.res <- ysr.matrices(dgroup.mean, BDLmean.time, use="pairwise.complete.obs")
      # Spearman correlation on individual data points
      cor.S_star <- ( cor(dgroup, method="spearman", use="pairwise.complete.obs") + 1 )/2
      # YS3
      cor.ys3.raw <- w$w1*cor.S_star + w$w2*ysr.res$A_star2 + w$w3*ysr.res$M_star
      group.ys3 <- 2*(cor.ys3.raw-0.5)
      
      # get correlation for the cluster mean
      idx.cl <- which(names(dgroup)=="cluster.mean")
      v <- group.ys3[, idx.cl]
      # sort by absolute correlation
      v.sorted <- rev(v[order(abs(v))])
      
      if (length(v.sorted)>=(Ntop+1)){
        v.sorted <- v.sorted[2:(Ntop+1)]
      } else {
        # fill short clusters with zeros to Ntop
        v.sorted <- c(v.sorted[2:length(v.sorted)], rep(0,Ntop-length(v.sorted)+1))
      }
      
      mv <- t(as.matrix(v.sorted))
      rownames(mv) <- c(sprintf("Cluster %s", k))
      if (labels == FALSE){
        colnames(mv) <- NULL
      }
      corrplot(mv, method="pie", type="full", 
             tl.cex=1.0, tl.col="black", col=col2(100), insig="p-value", sig.level=-1,
             p.mat=mv, cl.pos="n")
  }
  par(mfrow=c(1,1))
}

# plot to file (with and without names)
pdf(file.path(resultsPath, "cluster", "cluster_top_representatives_01.pdf"), 
    width=5, height=5, pointsize=12)
plot_top_cluster_representatives(labels=FALSE)
dev.off()
pdf(file.path(resultsPath, "cluster", "cluster_top_representatives_02.pdf"), 
    width=5, height=5, pointsize=12)
plot_top_cluster_representatives(labels=TRUE)
dev.off()

# plot in report
plot_top_cluster_representatives(labels=TRUE)
```

Overrepresentation analysis within the clusters.
One of the main uses of the GO is to perform enrichment analysis on gene sets. For example, given a set of genes that are up-regulated under certain conditions, an enrichment analysis will find which GO terms are over-represented (or under-represented) using annotations for that gene set.

Three subsets of GO, biological processes, cellular components and molecular function.
``` {r}
uniprot_from_name <- function(name){
  return(ProbeInformation(name)$Entry)
}


hc.res <- hclust.res[[method]]  # clustering for correlation method
groups.hc.order <- hc.res$groups.hc.order
for (k in 1:Ngroups){
    g <- groups.hc.order[groups.hc.order==k]
    print('---------------------------------')
    print(sprintf("Cluster %s", k))
    print(names(g))  # names for the cluster representatives
    uniprot <- lapply(names(g), uniprot_from_name)
    names(uniprot) <- names(g)
    # filter the non-existing UniProts, i.e. for non-gene data
    uniprot <- Filter(Negate(is.null), uniprot)
    print(as.data.frame(uniprot))
}

install.packages("clusterProfiler")

source("http://bioconductor.org/biocLite.R")
biocLite()
biocLite("topGO")

# make the topGO object
levels(BDLfactors$ftype)

all.genes <- BDLfactors[BDLfactors$ftype %in% c("GE_ADME", "GE_Cytokines", "GE_Fibrosis"),]$id
all.genes[all.genes]
names(all.genes) <- all.genes

names(g)

library("topGO")
GOdata.BP <- new("topGOdata", ontology='BP', 
                 allGenes = all.genes, 
                 geneSel = names(g),
                 nodeSize = 1,
                 annotationFun = annFUN.db, affyLib=affyLib)


# set of all genes, i.e. the genes on all fluidigm chips
BDLfactors$ftype


```





# Decision Trees
For the prediction of the phase/time after BDL from measured factors a decision tree was fitted on the data. The fitted decision tree model is a regression tree with the clusters as predictor variables. This allows to combine the information from multiple factors in the individual clusters, resulting in a predictive model which is not depening on single factor variables, but uses the combined information available from multiple factors. The fitted decision tree is applicable to any subset of factors measured.

The decision tree models are fitted using the packages `rpart` and `rpart.plot` with the 6 clusters as predictor variables in the model.

In a first step the data sets are prepared for model fitting and prediction. The used data sets for model evaluation are the mean cluster data set, i.e. the mean data of all factors averaged for the clusters. The data set of all combinations of single factors from the individual clusters and a data set using 2 randomly sampled factors from every cluster.


## Regression Tree
A regression tree with the time after BDL as target variable is fitted using the cluster data as predictors. The time data is log transformed to get aprroximately equidistant intervals between the trainings classes. All trees are fitted with the R package `rpart` implementing algorithms for recursive partitioning for classification following in most details closely Breiman et. al (1984). `rpart` is the open-source implementation of CART.

Breiman L., Friedman J. H., Olshen R. A., and Stone, C. J. (1984) Classification and Regression Trees. Wadsworth. 

The tree is built in a two-step process {Therneau2015}:  First the single variable is found which best
splits the data into two groups (`best' will be defined later). The data is separated based on the split, and then this process is applied
separately to each sub-group,  and so on recursively until the subgroups either reach a minimum size (`minbucket`) or until no improvement can be
made.
The resultant model is, with a certainty, too complex, and the question arises as it does
with all stepwise procedures of when to stop.  The second stage of the procedure consists of
using cross-validation to trim back the full tree. 

An Introduction to Recursive Partitioning Using the RPART Routines
TM. Therneau and E.J. Atkinson
Mayo Foundation (June 29, 2015)

The splitting criterion (for classication this is either the Gini or log-likelihood function), which is used to decide which variable gives the best split for nodes in the regression trees is $SS_{T}-(SS_{L} + SS_{R})$, with $SS_{T} = \sum{(y_{i}-<y>)^2}$ the sum of squares for the node and $SS_{R}$ and $SS_{L}$ the sums of squares for the left and right son. This is equivalent to choosing the split ot maximize the between-groups sum-of-squares in a simple analysis of variance (see {Therneau2015}).

Two important parameters controlling the resulting tree are 

* `minsplit` : The minimum number of observations in a node for which the routine
will even try to compute a split. The default is 20 and is set to 6 in the tree calculation ($N_{r}=5$ repeats per time point).
* `minbucket` : The  minimum  number  of  observations  in  a  terminal  node. This defaults to minsplit/3.


``` {r}
suppressPackageStartupMessages(library(rpart))
suppressPackageStartupMessages(library(rpart.plot))
suppressPackageStartupMessages(library(caret))
dir.create(file.path(resultsPath, 'decision_tree'), showWarnings=FALSE)
```

Used log transformations for the time points
```{r logTimeTransformation} 
# Transform data to log scale (for comparable time intervals)
log_transform <- function(data){
  log(data+1)
}
# Back transformation
log_transform_back <- function(log_data){
  exp(log_data)-1
}
```

## Trainings data
Generate the mean cluster trainings data for fitting the trees

```{r trainingsData}
# Hierarchical clusters based on ys3 to fit the regression tree
hc.res <- hclust.res$ys3
groups <- hc.res$groups

# Prepare training set for fitting the decision trees (mean cluster data set, 
# i.e. mean over normalized factors in cluster).
na.vec <- rep(NA, Nt*Nr)
treedata.mean <- data.frame(c1=na.vec, c2=na.vec, c3=na.vec, c4=na.vec, c5=na.vec, c6=na.vec)
# for every sample
for (ks in 1:(Nt*Nr)){
  # create the mean over the cluster
  for (kgroup in 1:Ngroups){
    # get factors in the cluster
    factors <- names(groups[groups==kgroup])
    # calculate mean over normalized data
    treedata.mean[ks, kgroup] <- mean(as.numeric(BDLdata.norm[ks, factors]), na.rm=TRUE)
  }
}
rm(na.vec)

# add class for classification
treedata.mean$class <- BDLsamples$time_fac
# add log transformed time [h] for regression
treedata.mean$regvalue <- log_transform(BDLsamples$time)

# mean cluster data set for model fitting
print(treedata.mean)
# save the trainings data
save(treedata.mean, file=file.path(resultsPath, "data", "treedata.mean.Rdata"))
```

## Fit regression tree
Fit the log transformed time against the mean cluster data for the time points. Set control parameters for the algorithm are `minsplit=6` and `minbucket=2` to account for the low number of samples (`N=40`). 
``` {r}
# formula for regression tree
formula.reg = paste("regvalue ~ c1 + c2 + c3 + c4 + c5 + c6")

# fitting regression tree with mean cluster data
tree.reg <- rpart(formula=formula.reg, 
                  data=treedata.mean, 
                  method="anova", 
                  control=rpart.control(minsplit=6, minbucket=2))

# overview resulting tree
print(tree.reg)
summary(tree.reg)
printcp(tree.reg)
tree.reg$frame

# pretty plot of tree to file
pdf(file.path(resultsPath, 'decision_tree', "regression_tree.pdf"), 
    width=10, height=5, pointsize=12)  
prp(tree.reg, type=0, extra=101, yesno=TRUE)
invisible(dev.off())

# and report
prp(tree.reg, type=0, extra=101, yesno=TRUE)

# visualize cross-validation results 
rsq.rpart(tree.reg) 

# predition on trainings data
pred.reg<- predict(tree.reg, newdata=treedata.mean, type="vector")
pred.reg
```


## Data sets for prediction
Prepare the evaluation data sets for the fitted trees consisting of all single factor combinations from the clusters used in the decision tree. Only the combinations are created which are from clusters used for decisions in the tree
``` {r}
# get the variables used for splitting
tree.nodes <- (tree.reg$frame)$var
tree.nodes <- tree.nodes[tree.nodes != "<leaf>"]
tree.vars <- as.character(sort(unique(tree.nodes)))
rm(tree.nodes)
# variables used for splitting decisions in tree
print(tree.vars)

# overview of member count per cluster
group_table <- table(groups)
print(group_table)

# names of factors in the clusters
group.factors <- vector("list", length=Ngroups)
for (k in 1:Ngroups){
  groups.factors[[k]] <- as.character(names(groups[groups==k]))
}
cluster_names <- paste('c', 1:Ngroups, sep="")
names(groups.factors) <- cluster_names

# create data.frame of all single combinations from clusters
single.combinations <- expand.grid(groups.factors, stringsAsFactors=TRUE)
names(single.combinations) <- names(groups.factors)

Nsingle <- nrow(single.combinations)
print(Nsingle)
head(single.combinations, 20)

# create all single factor data
print("Calculating single factor data (~ 3min) ... ")
ptm <- proc.time()  
treedata.single <- vector("list", Nsingle)  # list for all combinations
for (k in 1:Nsingle){
  # ----------------------------------------------------------------
  # THIS HAS TO BE FAST (<0.005 s)
  # ----------------------------------------------------------------
  # ptm <- proc.time() # Start the clock!
  
  # get factor data
  tmp <- BDLdata.norm[, t(single.combinations[k, ]) ]
  
  # add class and regression values
  tmp[c("class", "regvalue")] <- treedata.mean[ c("class", "regvalue")]
  
  # add factor fields 
  tmp[, paste(cluster_names, '.id', sep="")] <- single.combinations[k,]
  colnames(tmp) <- c(cluster_names, 'class', 'regvalue', paste(cluster_names, '.id', sep=""))
  
  # store data 
  treedata.single[[k]] <- tmp
  
  if (k%%500 == 0){print(k)} 
  # print(proc.time()-ptm)  # Stop the clock
}
# Stop the clock
print(proc.time() - ptm)
print("... calculated.")
treedata.single[[1]]

# which factor combinations only use genes
factor_is_gene <- BDLfactors$ftype %in% c("GE_ADME", "GE_Cytokines", "GE_Fibrosis")
names(factor_is_gene) <- BDLfactors$id

# vector for lookup if only genes were used
gene_only.single <- vector("logical", Nsingle)
for (k in 1:Nsingle){
  gene_only.single[k] <- all(factor_is_gene[t(single.combinations[k,])])
}
```

Create a sample of double combinations from the various clusters.
``` {r}
print("Calculating double factor data (~ 1min) ... ")
ptm <- proc.time()
set.seed(123456)
Ndouble <- 10000
treedata.double <- vector("list", Ndouble)  # list for sampled double combinations
for (k in 1:Ndouble){
  # sample from the 4 clusters without replacement
  n1 = sample(groups.factors[[1]], 2, replace=FALSE)  
  n2 = sample(groups.factors[[2]], 2, replace=FALSE)  
  n3 = sample(groups.factors[[3]], 2, replace=FALSE)  
  n4 = sample(groups.factors[[4]], 2, replace=FALSE)  
  n5 = sample(groups.factors[[5]], 2, replace=FALSE)  
  n6 = sample(groups.factors[[6]], 2, replace=FALSE)  
  
  # The mean of the combination is used (handle NAs)
  tmp <- 0.5 * (  BDLdata.norm[, c(n1[1], n2[1], n3[1], n4[1], n5[1], n6[1])] 
                + BDLdata.norm[, c(n1[2], n2[2], n3[2], n4[2], n5[2], n6[2])] )
  
   # add class and regression values
  tmp[c("class", "regvalue")] <- treedata.mean[ c("class", "regvalue")]
  
  # add factor fields 
  tmp[ , paste(cluster_names, '.id', sep="")] <- data.frame(paste(n1, collapse="__"), 
                                              paste(n2, collapse="__"),
                                              paste(n3, collapse="__"),
                                              paste(n4, collapse="__"),
                                              paste(n5, collapse="__"),
                                              paste(n6, collapse="__"))
  colnames(tmp) <- c(cluster_names, 'class', 'regvalue', paste(cluster_names, '.id', sep=""))
  # store data
  treedata.double[[k]] <- tmp
}
print(proc.time() - ptm)
print("... calculated.")
treedata.double[[5]]
```

##  Predict with the data sets
The fitted regression tree is now evaluated on multiple test sets.
### Mean cluster
Predict data with the tree, here the time class leaves based on the mean cluster data.
The regression tree predicts log classes, which are back-transformed to time in [h].
So how good is the tree performing on the trainings data set, i.e. the mean cluster data.
``` {r}
# mean cluster predictions
pred.mean.log <- predict(tree.reg, newdata=treedata.mean, type="vector")
# transformation to time in [h]
pred.mean <- log_transform_back( pred.mean.log )
# plot predicted ~ experimentell
plot(BDLsamples$time, pred.mean, pch=15, col=rgb(0,0,1, 0.2), main="Regression Tree:\nPredicted ~ experimentell time",
     xlab="experimentell time [h]", ylab="predicted time [h]")
```

Get the ranges of the various classes via the split points on log scale
``` {r}
# classes via predicted classes for cluster data
node_levels <- levels(as.factor(pred.mean))
# These are the predicted classes
node_classes <- round(as.numeric(node_levels), digits=1)
node_classes

# get the intervals of the time classes
node_mean <- as.numeric(levels(as.factor(pred.mean.log)))
node_midpoints <- (node_mean[2:length(node_mean)] + node_mean[1:(length(node_mean)-1)])/2
# minimum of range
node_min <- node_mean
node_min[2:length(node_min)] <- node_midpoints
# maximum of range
node_max <- node_mean
node_max[1:(length(node_min)-1)] <- node_midpoints
# ranges in log scale
node_ranges.log <- data.frame(node_mean, node_min, node_max)

node_ranges <- data.frame(mean=log_transform_back(node_mean), 
                          min=log_transform_back(node_min),
                          max=log_transform_back(node_max))
rownames(node_ranges) <- paste("class", 1:nrow(node_ranges))

# predicted classes by decision tree
round(node_ranges, digits=1)

# prediction on classification data
# TODO: create the matrix of how is predicted
# BDLsamples$time
# pred.matrix.mean <- matrix(NA, nrow=nrow(node_ranges), ncol=length(levels(as.factor(BDLsamples$time))))
```

Time class prediction with regression tree for single representative from each cluster
``` {r singlePrediction}
# single predictions
print("Predicting single factor data (~ 3min) ... ")
pred.res <- vector("list", length(treedata.single))
for (k in (1:length(treedata.single))){
  # back conversion
  pred.res[[k]] <- exp(predict(tree.reg, newdata=treedata.single[[k]], method="anova"))-1
}
pred.single <- do.call("rbind", pred.res)
```

Time class prediction with regression tree for random selection of two representatives from each clusters
```{r doublePrediction}
# double predictions
pred.res <- vector("list", length(treedata.double))
for (k in (1:length(treedata.double))){
  pred.res[[k]] <- exp(predict(tree.reg, newdata=treedata.double[[k]], method="anova"))-1
}
pred.double <- do.call("rbind", pred.res)
```


Display the performance of the single and double representative trees for prediction. Find the best single trees using all representatives and only genes. The best predictor minimizes hereby the distance between predicted and actual time classes.
``` {r predictionDistance}
# L2 (euclidian) distance measurement on the log transformed data. Analog to the distance measurement in fitting 
# the regression tree.
log_distance <- function(d1, d2){
  # sums over all the distances of the samples
  rmsd <- sqrt(sum( (log_transform(d1)-log_transform(d2) )^2 ))/length(d1)
  # corresponds to hours
  return(log_transform_back(rmsd))
}

# distance for all predictions on single factor per cluster
dist.single <- rep(NA, Nsingle)
for (k in 1:Nsingle){
  dist.single[k] <- log_distance(pred.single[k,], BDLsamples$time)
}

# distance for predictions on 2 sampled factors per cluster 
dist.double <- rep(NA, Ndouble)
for (k in 1:Ndouble){
  dist.double[k] <- log_distance(pred.double[k,], BDLsamples$time)
}
# distance on mean cluster
dist.mean <- log_distance(pred.mean, BDLsamples$time)
```

```{r bestTrees}
# Best decision tree using all factors
# Find unique combinations with minimal distance
dist.rep.best <- min(dist.single)
# best combination of representatives for the clusters (remove duplicates)
rep.best <- unique(single.combinations[which(dist.single==dist.rep.best), c("c1", "c3", "c4", "c5")])
rep.best.idx <- rownames(rep.best)[1]
# predictions of best representative
pred.rep.best <- pred.single[as.numeric(rep.best.idx), ]

print("Best single representatives for decision tree:")
print(rep.best)
print(dist.rep.best)

# Best decision tree using only gene factors, i.e. first reduce to the gene combinations
dist.single.genes <- dist.single[gene_only.single]
single.combinations.genes <- single.combinations[gene_only.single,]
dist.gene.best <- min(dist.single.genes)
# find best gene combination
gene.best <- unique(single.combinations.genes[which(dist.single.genes==dist.gene.best), c("c1", "c3", "c4", "c5")])
# predictions with best representative
gene.best.idx <- rownames(gene.best)
pred.gene.best <- pred.single[as.numeric(gene.best.idx), ]
pred.gene.best

print("Best single representatives based on genes for decision tree:")
print(gene.best)
print(dist.gene.best)

# Plot the distributions of all the distances, i.e. log transformed predictions
# vs. the real time class
breaks <- seq(from=0, to=0.40, by=0.0125)
hist(dist.single, freq=FALSE, breaks=breaks,
    main="Distance between predicted and experimentell time",
    xlab="RMSD(time.predicted, time.exp)",
    col=rgb(0.7,0.7,0.7, 1),
    ylim=c(0,15)
)
hist(dist.double, freq=FALSE, breaks=breaks,
     col=rgb(1,0,0, 0.5), add=TRUE)

abline(v=dist.mean, col=rgb(0,0,1, 0.5), lwd=2)
abline(v=dist.rep.best, col="black", lwd=2)
abline(v=dist.gene.best, col="grey", lwd=2)

legend("topright", legend=c("single factor", "double factor", "mean cluster"), 
           col=c(rgb(0.7, 0.7, 0.7, 1),rgb(1, 0, 0, 0.5),rgb(0, 0, 1, 0.5)),
           bty="n", cex=1.0, pch=15)
```


Evaluation of the predictions. Which experimental classes were predicted in which time classes of the regression tree for the mean cluster data (trainings data), single representative from each clusters and double representatives from each cluster.
``` {r}
# Predicted classes of the regression tree
node_levels <- levels(as.factor(pred.mean))
node_classes <- round(as.numeric(node_levels), digits=1)
node_classes

# Plot of the predicted classes with the decision tree
plot_predicted_classes <- function(){
  bar_colors <- brewer.pal(4, "Set3")
  par(mfrow=c(2,4))
  for (k in 1:Nt){
    # single factor predictions
    data <- as.vector(pred.single[, ((1:Nr)+Nr*(k-1))])
    tab.single <- table(factor(data, levels=node_levels))/length(data)
    
    # two factor predictions
    data <- as.vector(pred.double[, ((1:Nr)+Nr*(k-1))])
    tab.double <- table(factor(data, levels=node_levels))/length(data)
    
    # mean cluster predictions
    data <- as.vector(pred.mean[((1:Nr)+Nr*(k-1))])
    tab.mean <- table(factor(data, levels=node_levels))/length(data)
    
    # best single representative
    # data <- as.vector(pred.rep.best[((1:Nr)+Nr*(k-1))])
    # tab.rep.best <- table(factor(data, levels=node_levels))/length(data)
    
    # best single gene representative
    data <- as.vector(pred.gene.best[((1:Nr)+Nr*(k-1))])
    tab.gene.best <- table(factor(data, levels=node_levels))/length(data)
    
    # combined table
    tab <- rbind(tab.single, tab.double, tab.gene.best, tab.mean)
    colnames(tab) <- round(as.numeric(colnames(tab)), digits=1)
    
    # create the bar plot
    name <- sprintf("Time after BDL: %sh", levels(as.factor(BDLsamples$time))[k])
    barplot(tab, beside=TRUE,
            main=name, 
            xlab="predicted time class [h]", ylab="fraction of predictions",
            ylim=c(0,1), col=bar_colors) 
    if (k==1){
      legend("topright", legend=c("single factors", "double factors", 
                                  "best single gene", "mean cluster"), 
             col=bar_colors,
             bty="n", cex=1.0, pch=15)
    }
  }
  par(mfrow=c(1,1))
}

# barplot of the predicted classes
plot_predicted_classes()

# barplot to file
pdf(file.path(resultsPath, "decision_tree", "predicted_classes.pdf"), 
    width=14, height=8, pointsize=14)
plot_predicted_classes()
invisible(dev.off())
```

