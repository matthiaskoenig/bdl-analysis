---
title: "Statistical Analysis of Pathobiochemical Signatures in Bile Duct Ligated Mice"
author: '[Matthias Koenig](http://www.charite.de/sysbio/people/koenig) (`r Sys.Date()`)'
output:
  pdf_document:
    fig_caption: yes
    keep_tex: yes
    toc: yes
  html_document:
    pandoc_args:
    - +RTS
    - -K64m
    - -RTS
  word_document: default
header-includes: \usepackage{graphicx}
---
<!--
%\VignetteEngine{knitr}
%\VignetteIndexEntry{BDL raw data processing}
-->
```{r knitr_setup, options, echo=FALSE}
# set knitr option
# knitr::opts_chunk$set()
knitr::knit_hooks$set(htmlcap = function(before, options, envir) {
  if(!before) {
    paste('<p class="caption">',options$htmlcap,"</p>",sep="")
    }
})
```

# Introduction
This document contains the statistical analysis performed in the publication *Pathobiochemical signatures of cholestatic liver disease in bile duct ligated mice*.

A comprehensive data set of serum markers, histological parameters and transcript profiles was compiled at 8 time points after bile duct ligation (BDL) in mice, comprising different stages of the disease. The data set consists of $N_{r}=5$ repeats ($N_{r}=3$ for the measured antibodies) for $N_{t}=8$ time points denoted by $t_1,..., t_{N_t}$ consisting of a total $N_{f}=153$ measured parameters in the following referred to as factors (Fluidigm gene expression, immunostainings with antibodies, serum markers, histological measurements).

The following naming conventions are used

* **factor** : one of the measured quantities/parameters over time, i.e. either 
    + gene expression of a single gene (e.g. Actb); 
    + one of the biomarkers (e.g. ALT, albumin, bilirubin)
    + one of the histological markers (e.g. BrdU-positive Kupffer cells)
    + one of the immunostainings (e.g. CTGF, S100A4)
* **time point** : a single value $t_i$ from the measured time points 0h (control), 6h, 12h, 18h, 30h, 2d, 5d, 14d
* **sample** : one of the $N_{t}N_{r}=40$ mice, i.e. a one of the repeats for a given time point

The main steps of the analysis comprise

* **Explorative data anaysis**
* **Dimension reduction via ANOVA**
* **Correlation analysis**
* **Hierarchical clustering** 
* **Decision trees**

The complete data set, source code and documentation of this analysis is available from  
https://github.com/matthiaskoenig/bdl-analysis .

All results of this analysis are written to the results directory defined via the `BDL_RESULTS` environment variable. To reproduce this analysis create the respective variable and run the `Koenig_BDL_analysis.Rmd` file.

```{r results_path, eval=TRUE, strip.white=TRUE}
# read results directory from environment variable
resultsPath <- Sys.getenv("BDL_RESULTS")
if (identical(resultsPath, "")){
  stop("No results directory defined, set the BDL_RESULTS environment variable")
}
# create directory to store data sets of analysis
dir.create(file.path(resultsPath, 'data'), showWarnings=FALSE)
```

# Explorative data analysis
## Data import
In a first step the processed data sets are loaded from the `data` folder: These are the time course data for all factors (`BDLdata`), additional information for the factors (`BDLfactors`), the sample definition, i.e. the assignment of sample ids to respective time point and repeat (`BDLsamples`), and a mapping of the Fluidigm (gene) probe ids to UniProt identifiers and names (`BDLprobes`).

```{r bdl_data, eval=TRUE, strip.white=TRUE}
suppressPackageStartupMessages(library(BDLanalysis))
suppressPackageStartupMessages(library(calibrate))
suppressPackageStartupMessages(library(pander))
dir.create(file.path(resultsPath, 'control'), showWarnings=FALSE)
# path definition
baseLoc <- system.file(package="BDLanalysis")
extPath <- file.path(baseLoc, "extdata")
# load data
data(BDLdata)
data(BDLsamples)
data(BDLfactors)
data(BDLprobes)
# counters
Nr <- 5  # repeats
Nt <- length(levels(BDLsamples$time_fac))  # time points
# store all data sets in the results folder
save(BDLdata, file=file.path(resultsPath, "data", "BDLdata.Rdata"))
save(BDLsamples, file=file.path(resultsPath, "data", "BDLsamples.Rdata"))
save(BDLfactors, file=file.path(resultsPath, "data", "BDLfactors.Rdata"))
save(BDLprobes, file=file.path(resultsPath, "data", "BDLprobes.Rdata"))
```

In addition to the individual sample data, the mean data averaged over the $N_{r}$ repeats per time points is used in parts of the analysis. The mean factor data set is calculated once via
```{r bdl_mean_data}
BDLmean <- bdl_mean_data(BDLdata, BDLsamples)
BDLmean.time <- as.numeric(levels(as.factor(BDLsamples$time)))
```

In total `r ncol(BDLdata)` factors were measured in the this BDL study falling in the categories: `r levels(BDLfactors$ftype)`. The majority of factors belongs hereby to the 3 fluidigm chips with 47 probes per chip (one probe was filtered from GE fibrosis during preprocessing of the chips).

An overview of the number of factors per category is provided in the following table
```{r factor_table}
cat_table <- as.data.frame(table(BDLfactors$ftype))
colnames(cat_table) <- c("Category", "Freq")
set.caption(sub(".", " ", "Factors per category", fixed = TRUE))
pander(cat_table)
rm(cat_table)
```

## Gene Probes
In the following an overview of the gene probes is given providing full names and links to UniProt.
```{r gene_ids}

# create data frame with probe information
create_probe_df <- function() {
  # get the gene factors
  f_names <- colnames(BDLdata)[BDLfactors$ftype.short == ""]
  # get the probe information
  Nf = length(f_names)
  probe_names <- rep(NA, length=Nf)
  probe_uniprot <- rep(NA, length=Nf)
  probe_genes <- rep(NA, length=Nf)
  names(probe_names) <- f_names
  names(probe_uniprot) <- f_names
  names(probe_genes) <- f_names
  for (name in colnames(BDLdata)){
      probe_info <- ProbeInformation(name)
      if (!is.null(probe_info)){
        if (!is.null(probe_info$Protein.name)){
          probe_names[name] <- probe_info$Protein.name
          probe_uniprot[name] <- probe_info$Entry
          probe_genes[name] <- probe_info$Gene.name  
        }
      }
  }
  probe.df <- data.frame(name=probe_names, genes=probe_genes, uniprot=probe_uniprot)
  # sort
  probe.df <- probe.df[order(rownames(probe.df)), ]
  # remove the double Act probes
  probe.df <- probe.df[!(rownames(probe.df) %in% c("Actb.x", "Actb.y")) , ]  
  return(probe.df)
}
# print the probe information
probe.df <- create_probe_df()
print(probe.df)
```

## Data visualization
### Time course of single factors
In a first step overview plots of the raw and mean data for all individual factors were generated. These are available in the `resultsPath/factors` folder

```{r factor_plots}
# Create figures for all factors
factors_path <- file.path(resultsPath, 'factors')
dir.create(factors_path, showWarnings=FALSE)
plot_all_factors(path=factors_path)
rm(factors_path)
```

An example plot for a single factor is depicted below, here for the factor `bilirubin`.
``` {r bilirubin_plot, htmlcap='<b>Figure Single factor</b>: Plot of the raw time course data for bilirubin. On the left the data is plotted against the time [h], on the right against the different time classes. Individual data points are depecticed in blue with the respective sample number shown next to the data points. The mean averaged of the repeats per time point are depicted in red. Box-and-whisker plots were added with default R parameters of boxwex=0.8, staplewex=0.5, outwex=0.5.'}
plot_single_factor('bilirubin', path=NULL)
```

### Time course of all factors (Heatmap)
In a next step the heatmap of the full data set was generated, i.e. of all time points and repeats. This provides a first overview over the data set. Rows correspond to individual factors (factor order corresponding to the original data set: `GE_ADME`, `GE_Cytokines`, `GE_Fibrosis`, `Biochemistry`, `Histology`, `Antibodies`). Columns correspond to the 40 samples with 5 subsequent samples belonging to one of the 8 time points. The data is row scaled, i.e. every individual factor is scaled to have mean zero and standard deviation one.
```{r timecourse_heatmap}
suppressPackageStartupMessages(library(gplots))
suppressPackageStartupMessages(library("RColorBrewer"))

# define horizontal and vertical helper lines
v_lines <- ((1:Nt)*Nr+0.5)
factor_types <- c("Antibodies", "Histology", "Biochemistry", "GE_Fibrosis", "GE_Cytokines", "GE_ADME")
factor_table <- table(BDLfactors$ftype)
h_lines <- 0.5 + cumsum(factor_table[factor_types])

timecourse_heatmap <- function(){
  # create better row names
  dtmp <- BDLdata
  rownames(dtmp) <- paste(rownames(BDLsamples), BDLsamples$time_fac, sep=" ")
  # heatmap colors 
  hmap_colors <- HeatmapColors()
  # colors for factor groups
  colorset <- brewer.pal(length(factor_types), "Set2")
  color.map <- function(factor_id) {return(colorset[ which(factor_types==BDLfactors$ftype[which(BDLfactors$id==factor_id)]) ])}
  factorColors <- unlist(lapply(BDLfactors$id, color.map))
  # heatmap
  heatmap.2(t(as.matrix(dtmp)), col=hmap_colors(100), scale="row", dendrogram="none", Rowv=NULL, Colv=NULL,
            key=TRUE, trace="none", cexRow=0.5, keysize=0.8, density.info="none",
            RowSideColors=factorColors,
            add.expr=abline(v=v_lines, h=h_lines, col="black", lwd=0.5),
            main="Heatmap of BDL time course data")
            # xlab="sample", ylab="factor")
  # legend
  legend("left",
      inset=c(-0.03,0),
      legend = rev(factor_types), # category labels
      col = rev(colorset),  # color key
      lty= 1, lwd = 10, cex = 0.7, bty="n"
  )
}
# plot to file
pdf(file.path(resultsPath, "control", "timecourse_heatmap.pdf"), width=10, height=10, pointsize=12) 
timecourse_heatmap()
invisible(dev.off())
```

``` {r timecourse_heatmap_report, fig.width=10, fig.height=10, error=TRUE, htmlcap='<b>Figure All factors </b>: Heatmap of the complete data set, i.e. all factors and repeats over time. The data is row scaled, i.e. every individual factor is scaled to have mean zero and standard deviation one, with positive Z-score in blue, negative Z-score in red. The row order is according to the the factor categories, the order within the fluidigm chips according to the order of the probes on the chip. the respective categories are depicted on the left.'}
timecourse_heatmap()
```

``` {r heatmap_clean}
# cleanup
rm(factor_table, h_lines, v_lines)
```
**Results**: Various patterns are visible in the plotted raw data:

* **Two main classes of time course response were observed**. One class with an increase in the early phase up to 6h after BDL (many of the ADME genes fall in this class) and a second class increasing in the later stage after 2-5 days after BDL. Many of the genes on the Cytokines and Fibrosis  chips as well as some of the biochemical, histological and antibody factors fall in this second class.
* **The individual animals show heterogeneous responses to BDL**. Within one time point the 5 repeats can show very different patterns. For instance at time 6h after BDL 3/5 of the mice show a marked increase in the ADME genes, whereas 2/5 do not show such a marked increase. Another example is the mice sample 27 at time 2d, with a high increase in the genes on the Fibrosis chip, which is not observed in the other 4 samples at time 2d.

## Actb quality control
Actb (Actin, cytoplasmic 1) probes were included on all Fluidigm chips (`GE_ADME`, `GE_Cytokines`, `GE_Fibrosis`) and not used in the normalization of the transcription data. Hence, ActB can serve as quality control for the technical reproducibility of the Fluidigm chips. If the data is reproducible between chips the pairwise correlation between all individual Actb measurements should have high correlation coefficients close to 1. Plotting the data of the Actb measurements of two chips against each other should lie on a straight line
```{r check_actb}
# Actb control figure
plot_actb_control <- function(){
  par(mfrow=c(2,3))
  plot_single("Actb")
  plot_single("Actb.x")
  plot_single("Actb.y")
  plot_cor_pair("Actb", "Actb.x", single_plots=FALSE)
  plot_cor_pair("Actb", "Actb.y", single_plots=FALSE)
  plot_cor_pair("Actb.x", "Actb.y", single_plots=FALSE)
  par(mfrow=c(1,1))
}

# plot to file
pdf(file.path(resultsPath, "control", "Actb_control.pdf"), width=10, height=6, pointsize=12)  
plot_actb_control()
invisible(dev.off())

# calculate Spearman and Pearson correlation coefficients on N=8*5=40 data points 
actb.spearman <- cor(data.frame(Actb=BDLdata$Actb, 
                                Actb.x=BDLdata$Actb.x, 
                                Actb.y=BDLdata$Actb.y), method="spearman")
actb.pearson <- cor(data.frame(Actb=BDLdata$Actb, 
                               Actb.x=BDLdata$Actb.x, 
                               Actb.y=BDLdata$Actb.y), method="pearson")

# table of correlation coefficients
set.caption(sub(".", " ", "Spearman correlation of Actb controls", fixed = TRUE))
pander(round(actb.spearman, digits=3))
set.caption(sub(".", " ", "Pearson correlation of Actb controls", fixed = TRUE))
pander(round(actb.pearson, digits=3))
```

``` {r actb_control_plot, fig.width=10, fig.height=7,  htmlcap='<b>Figure Actb Control</b>: Correlation plot of the Actb probes from the 3 Fluidigm chips: Actb (fibrosis), Actb.x (ADME), Actb.y (Cytokines). The top row shows the individual time courses, the bottom row the pair wise plot of individual data points.'}
plot_actb_control()
rm(actb.pearson, actb.spearman)
```
**Results**: The Actb Fluidigm gene expression measurements are highly reproducible for the measured chips,  with Spearman as well as Pearson correlation coefficients all > 0.9 for pairwise Actb comparison.

# Dimension reduction
## Introduction
A one-way analysis of variance (ANOVA) was applied to reduce the factors to the subset showing significant ($p_{adjusted}< 0.05$) changes during the time course. In its simplest form, ANOVA provides a statistical test of whether or not the means of several groups are equal, and therefore generalizes the t-test to more than two groups, with the groups being the sampled time points. The Holm's procedure was used to correct the p-values for any artificial p-value inflation due to multiple testing. 

For every of the individual factors in the BDL data set an ANOVA was calculated. Dimension reduction of the BDL data set was than performed by filtering out factors which did not significantly changing over time.

The `BDLdata` data set is reshaped into matrix format for the ANOVA calculation, with time points in rows and repeats as columns for every factor. 
```{r bdl_matrices}
BDLmatrices <- bdl_matrix_data(BDLdata, BDLsamples)
```

## ANOVA for single factor
The following shows the ANOVA calculation for a single factor, here for `bilirubin`. 
``` {r single_anova}
  # example ANOVA for one factor
  mat.anova <- t(BDLmatrices[['bilirubin']])
  colnames(mat.anova) <- levels(BDLsamples$time_fac)
  
  # concatenate the data rows of df1 into a single vector r .
  r = c(t(as.matrix(mat.anova)))  # response data 
  
  # assign new variables for the treatment levels and number of observations.
  f = levels(BDLsamples$time_fac)   # treatment levels 
  k = 8                          # number of treatment levels 
  n = 5                          # observations per treatment 
  
  # create a vector of treatment factors that corresponds to each element of r in step 3 with the gl function.
  tm <- gl(k, 1, n*k, factor(f))   # matching treatments 
  
  # apply the function aov to a formula that describes the response r by the treatment factor tm.
  # fit an analysis of variance model
  av <- aov(r ~ tm) 
  
  # print out the ANOVA table with the summary function. 
  summary(av)
  # print the corresponding p-value
  p.value <- summary(av)[[1]][["Pr(>F)"]][[1]]

  # show data matrix
  print(mat.anova)
```
```{r}
  # cleanup
rm(r, f, k, n, tm, av, p.value, mat.anova) 
```

## ANOVA for all factors
Analog to the single factor ANOVA, the ANOVA is performed on all the factors. Hereby, a multitude of tests are performed, namely an ANOVA for every single factor. Consequently, the reported p-values of the ANOVA have to be adjusted via multiple testing procedures. Using the p.adjust function which given a set of p-values, returns p-values adjusted using one of several methods. The Bonferroni, Holm, Hochberg, Hommel are designed to give strong control of the family-wise error rate. We used the Holm's method for adjustment (*Holm, S. (1979). A simple sequentially rejective multiple test procedure. Scandinavian Journal of Statistics 6, 65-70.*).

```{r anova_all}
# Calculation of ANOVA for all factors
df.anova <- all_factor_anova()
df.anova$sig <- sapply(df.anova$p.value, significant_code)  # add significant codes

df.anova$p.holm <- p.adjust(df.anova$p.value, method ="holm" , n = length(df.anova$p.value))
df.anova$sig.holm <- sapply(df.anova$p.holm, significant_code) 
df.anova$ftype <- BDLfactors$ftype
df.anova$fshort <- BDLfactors$ftype.short

# order factors by adjusted p-values, and cleanup for printing
df.anova.ordered <- df.anova[with(df.anova, order(p.holm)), ]
rownames(df.anova.ordered) <- df.anova.ordered$factors

df.anova.ordered <- df.anova.ordered[c("p.holm", "sig.holm", "ftype", "fshort")]
df.anova.ordered$p.holm <- as.numeric(sprintf("%1.2E", df.anova.ordered$p.holm))
print(df.anova.ordered)

# save results
write.table(df.anova.ordered, file=file.path(resultsPath, "data", 'BDLanova.csv'), sep="\t", quote=FALSE)
save(df.anova, file=file.path(resultsPath, "data", "BDLanova.Rdata"))
# rm(df.anova.ordered)
```

## Filter factors
The factors are filtered based on the respective acceptance level, with the cutoff for the adjusted p-value being $p_{accept}$, i.e. all factors with a ANOVA with $p_{adjusted} \ge p_{accept}$ are filtered out. The filtered raw data is available as `BDLdata.fil`, the filtered mean data set as `BDLmean.fil`. All subsequent analyses are performed on the filtered data set, which is depicted in the following heatmap
``` {r filter_factors}
p.accept = 0.05  # acceptance level
anova.accept = (df.anova$p.holm < p.accept)  # accepted subset
# subset of filtered data
BDLdata.fil <- BDLdata[, anova.accept]
BDLmean.fil <- BDLdata[, anova.accept]

# accepted
table(anova.accept)  # 64 rejected / 90 accepted (adjusted)

# which factors were accepted in the various categories
fil_tab <- data.frame(
  table(BDLfactors$ftype[anova.accept]),
  table(BDLfactors$ftype),
  round(table(BDLfactors$ftype[anova.accept])/table(BDLfactors$ftype), 2)
)
fil_tab <- fil_tab[, c('Var1', 'Freq', 'Freq.1', 'Freq.2')]
names(fil_tab) <- c('Category', 'Accepted', 'All', 'Percent')
# overview of filtered factors
print(fil_tab)
rm(fil_tab)
```
Based on the adjusted p-values the data set was reduced from original `{r} ncol(BDLdata.fil)` factors to 90. Almost all `Cytokines` genes (inflammation panel) were retained in the data set whereas many of the `ADME` and `Fibrosis` genes are filtered.


### Heatmap of filtered time course data
The heatmap of the filtered raw data is depicted below

``` {r heatmap_filtered}
v_lines <- ((1:Nt)*Nr+0.5)

timecourse_heatmap_filtered <- function(){
  # prepare data with row names
  dtmp <- BDLdata.fil
  rownames(dtmp) <- paste(rownames(BDLsamples), BDLsamples$time_fac, sep=" ")
  
  # color definitions
  hmap_colors <- HeatmapColors()
  colorset <- brewer.pal(length(factor_types), "Set2")
  color.map <- function(factor_id) {return(colorset[ which(factor_types==BDLfactors$ftype[which(BDLfactors$id==factor_id)]) ])}
  factorColors <- unlist(lapply(colnames(BDLdata.fil), color.map))
  # heatmap
  heatmap.2(t(as.matrix(dtmp)), col=hmap_colors(100), scale="row", dendrogram="none", 
            Rowv=NULL, Colv=NULL,
            key=TRUE, trace="none", cexRow=0.5, keysize=0.8, density.info="none",
            RowSideColors=factorColors,
            add.expr=abline(v=v_lines, col="black", lwd=0.5),
            main="ANOVA Filtered BDL factors")
            # xlab="sample", ylab="factor")
  # legend
  legend("left",
      inset=c(-0.03,0),
      legend = rev(factor_types), # category labels
      col = rev(colorset),   # color key
      lty= 1,                # line style
      lwd = 10,              # line width
      cex = 0.7,
      bty="n"
  )
}

# plot to file
pdf(file.path(resultsPath, "control", "timecourse_heatmap_filtered.pdf"), 
    width=10, height=10, pointsize=12)  
timecourse_heatmap_filtered()
invisible(dev.off())
```

``` {r heatmap_filtered_plot, fig.width=10, fig.height=10, error=TRUE, htmlcap='<b>Figure Timcourse Filtered:</b> Plot of ANOVA filtered data set.'}
# plot to report
timecourse_heatmap_filtered()
rm(v_lines)
```

## t-test for initial phase
A t-test was performed to find significantly changed factors in the initial phase after BDL.
``` {r t_test}
# t-test for the initial phase changes
calculate_inital_phase_changes <- function(){
  # empty vectors
  p.t_test <- rep(NA, ncol(BDLdata))
  names(p.t_test) <- colnames(BDLdata)
  up_down <- rep(NA, ncol(BDLdata))
  names(up_down) <- colnames(BDLdata)
  
  for (name in colnames(BDLdata)){
    # data for control (0h) and initial response (6h)
    d0 <- BDLdata[BDLsamples$time_fac == "0h", name]
    d6 <- BDLdata[BDLsamples$time_fac == "6h", name]  
    # remove NA
    d0 <- d0[!is.na(d0)]
    d6 <- d6[!is.na(d6)]
    # unpaired two.sided t-test
    t.test.res <- t.test(d0, d6, alternative="two.sided", var.equal=FALSE)
    p.t_test[name] <- t.test.res$p.value
    # going up or down
    up_down[name] <- "-"
    if (mean(d6) > mean(d0)){
      up_down[name] <- "up"
    } else if (mean(d6) < mean(d0)){
      up_down[name] <- "down"
    }
  }
  
  # results data.frame
  p.df <- data.frame(p.value=p.t_test, up_down=up_down)
  p.df$sig <- sapply(p.df$p.value, significant_code) 
  rownames(p.df) <- colnames(BDLdata)
  
  # sort by p.value
  p.df.ordered <- p.df[order(p.df$p.value),]  
  
  return(p.df.ordered)
}

# top up and down regulated in initial phase
p.df.ordered <- calculate_inital_phase_changes() 
p.df.up <- p.df.ordered[p.df.ordered$p.value<0.05 & p.df.ordered$up_down=="up", ]
p.df.down <- p.df.ordered[p.df.ordered$p.value<0.05 & p.df.ordered$up_down=="down", ]
# top up
print(p.df.up)
# top down
print(p.df.down)
```


# Correlation analysis
## Introduction
For the correlation analysis between factors and the subsequent cluster analysis a correlation measure for time series data {Son2008} in combination with Complete-Linkage hierarchical clustering was used. This combination of methods provided the best enrichments on gene-expression time-series in a recent comparisons of methods {Jaskowiak2014, Jaskowiak2013} testing various correlation measures and clustering algorithms. 

The calculation of correlation coefficients between factors i and j ($i,j=1, ...,N_p$) was performed using the slightly modified correlation coefficient based similarity measure developed for clustering of time-course data ($Y_{i,j}^{S2}$ and $Y_{i,j}^{R2}$) {Son2008}. $Y_{i,j}^{S2}$ and $Y_{i,j}^{R2}$ are linear combinations of (i) a classical correlation part based on Spearman correlation $S_{i,j}^{*}$ in case of $Y_{i,j}^{S2}$ or Pearson $R_{i,j}^{*}$ in case of $Y_{i,j}^{R2}$, (ii) a component $A_{i,j}^{*}$ accounting for the similarity in changes between two time courses, (iii) a component $M_{i,j}^{*}$ comparing the location of minimum and maximum values of the time course (see {Son2008} for definitions)
$$Y_{i,j}^{S2} = w_1 S_{i,j}^{*} + w_2 A_{i,j}^{*} + w_3 M_{i,j}^{*}$$
$$Y_{i,j}^{R2} = w_1 R_{i,j}^{*} + w_2 A_{i,j}^{*} + w_3 M_{i,j}^{*}$$
$R_{i,j}^{*}$ and $S_{i,j}^{*}$ are hereby calculated on the individual data points for the factors i and j, $A_{i,j}^{*}$ and $M_{i,j}^{*}$ on the mean time courses averaged over the $N_r$ repeated measurements. Throughout the analysis the following weights were used $w_1=0.5$, $w_2=0.3$, $w_3=0.2$.

In the calculation of the change component we used a Spearman correlation based measure ($A^{**}$) instead of the originally proposed Pearson measure ($A^{*}$) resulting in the correlation scores $Y_{i,j}^{S3}$ and $Y_{i,j}^{S3}$
$$Y_{i,j}^{S3} = w_1 S_{i,j}^{*} + w_2 A_{i,j}^{**} + w_3 M_{i,j}^{*}$$
$$Y_{i,j}^{R3} = w_1 R_{i,j}^{*} + w_2 A_{i,j}^{**} + w_3 M_{i,j}^{*}$$
Herein, $A_{i,j}^{**}$ calculates the correlation of changes between factors i and j based on Spearman correlation analog $A_{i,j}^{*}$ as
$$A_{i,j}^{**}=(S(d_i,d_j)+1)/2$$
$$A_{i,j}^{*}=(R(d_i,d_j)+1)/2$$
The reason for this adaption was that initial analysis showed a strong dependency of the change components on outliers.

All calculated correlation scores $Y^{S}$ and $Y^{R}$ are transformed from [0, 1] to [-1, 1] via
$$Y_{norm}^{S} = 2(Y^{S}-0.5)$$
$$Y_{norm}^{R} = 2(Y^{R}-0.5)$$

In addition to the used $Y^{S}$ and $Y^{R}$ correlation scores Pearson (R) and Spearman (S) correlations were calculated for comparison. 

``` {r correlation_methods}
suppressPackageStartupMessages(library(corrplot))
dir.create(file.path(resultsPath, 'correlation'), showWarnings=FALSE)

# list of calculated correlation methods
correlation_methods <- c("pearson", "spearman", "ys1", "ys2", "ys3", "yr1", "yr2", "yr3")

# The calculated correlation matrices are stored in the `cor.matrices
# Provides simple access to the respective correlation matrix. 
cor.matrices <- vector("list", length=length(correlation_methods))
names(cor.matrices) <- correlation_methods
```

## Pearson & Spearman correlation
In a first step Pearson $R_{i,j}$ and Spearman $S_{i,j}$ correlation were calculated for the subset of filtered factors. 
``` {r correlation_pearson_spearman}
# Spearman and Pearson on individual data points
cor.matrices$pearson <- cor(BDLdata.fil, method="pearson", use="pairwise.complete.obs")
cor.matrices$spearman <- cor(BDLdata.fil, method="spearman", use="pairwise.complete.obs")
```

Heatmap plot of the correlation matrices with factors in original order
``` {r correlation_heatmap_plot}
# Heatmap of correlation matrix
correlation_heatmap <- function(method){
  data <- cor.matrices[[method]]
  if(is.null(data)){
    stop("Correlation matrix does not exist for method: ", method)
  }
  cor_colors <- HeatmapColors() # color palette for correlation (red - white - blue)
  heatmap.2(data, col=cor_colors(10), scale="none",
          key=TRUE, symkey=FALSE, trace="none", cexRow=0.8, cexCol=0.8,
          main=method,
          density.info="none", dendrogram="none", 
          Rowv=NULL, Colv=NULL, 
          keysize=0.8, key.xlab = method,
          #revC=TRUE,
          sepwidth=c(0.01,0.01),
          sepcolor="black",
          colsep=1:ncol(data),
          rowsep=1:nrow(data))
}
```

``` {r heatmap_pearson, fig.width=10, fig.height=10, error=TRUE, htmlcap='<b>Figure Heatmap Pearson:</b> Correlation matrix based on Pearson correlation.'}
# Pearson correlation (no clustering)
correlation_heatmap(method="pearson")
```

``` {r heatmap_spearman, fig.width=10, fig.height=10, error=TRUE, htmlcap='<b>Figure Heatmap Spearman:</b> Correlation matrix based on Spearman correlation.'}
# Spearman correlation (no clustering)
correlation_heatmap(method="spearman")
```
The pearson correlation is highly sensitive to outliers (correlation between factors, but also in the changes between time points). Prelimary analysis of $A^{*}$ showed similar problems, so we used $A^{**}$ in $Y^{S3}$ instead.

## YS & YR correlation
Here, the time-course based correlation measurements $Y^{S1}$, $Y^{S2}$, $Y^{S3}$, $Y^{R1}$, $Y^{R2}$ and $Y^{R3}$ are calculated for the factors in the filtered BDL data set. To create the correlation matrix all pairwise correlations between all factors are calculated.

``` {r ys_yr_calculation}
# weighting factors
w <- list(w1=0.5, w2=0.3, w3=0.2)

# calculate the YSR component matrices on the filtered data set (A, A*, A**, M, M*)
# all components are calculated on the mean data
ysr.res <- ysr.matrices(BDLmean.fil, BDLmean.time, use="pairwise.complete.obs")

# S* and R* (Pearson & Spearman correlation on individual data points)
cor.S_star <- (cor.matrices$spearman + 1)/2
cor.R_star <- (cor.matrices$pearson + 1)/2

# Calculate YS and YR scores based on the components
ysr_methods <- c("ys1", "ys2", "ys3", "yr1", "yr2", "yr3")
cor.ysr <- vector("list", length(ysr_methods))
names(cor.ysr) <- ysr_methods

# unnormalized correlations in [0, 1] as combination of weighted (S/R, A/A*/A**, M/M*)
cor.ysr$ys1 <- w$w1*cor.S_star + w$w2*ysr.res$A       + w$w3*ysr.res$M
cor.ysr$ys2 <- w$w1*cor.S_star + w$w2*ysr.res$A_star  + w$w3*ysr.res$M_star
cor.ysr$ys3 <- w$w1*cor.S_star + w$w2*ysr.res$A_star2 + w$w3*ysr.res$M_star

cor.ysr$yr1 <- w$w1*cor.R_star + w$w2*ysr.res$A       + w$w3*ysr.res$M
cor.ysr$yr2 <- w$w1*cor.R_star + w$w2*ysr.res$A_star  + w$w3*ysr.res$M_star
cor.ysr$yr3 <- w$w1*cor.R_star + w$w2*ysr.res$A_star2 + w$w3*ysr.res$M_star 

# scaling of ysr correlation coefficient to interval [-1,1]
for (method in ysr_methods){
  cor.matrices[[method]] <- 2*(cor.ysr[[method]]-0.5)
}

# Save correlation matrices
save(cor.matrices, file=file.path(resultsPath, "data", "cor.matrices.Rdata"))
rm(cor.ysr, cor.R_star, cor.S_star, ysr.res, ysr_methods)
```

Create heatmap plots for all calculated correlation matrices
``` {r all_correlation_plots}
# create all correlation heatmaps on disk
hmap.settings <- list(width=10, height=10, pointsize=12)
for (method in correlation_methods){
  fname <- sprintf('cor.%s.pdf', method)
  pdf(file.path(resultsPath, "correlation", fname),
        width=hmap.settings$width, height=hmap.settings$height, pointsize=hmap.settings$pointsize)
  correlation_heatmap(method=method)
invisible(dev.off())
rm(method, fname)
}
```

``` {r ys3_correlation_report, fig.width=10, fig.height=10, error=TRUE, htmlcap='<b>Figure Heatmap YS3:</b> Correlation matrix based on YS3 correlation.'}
# ys3 heatmap in report
correlation_heatmap(method="ys3")
```

# Hierarchical clustering
## Clustering
In the next step clusters are calculated on the correlation matrices using hierarchical clustering with complete linkage. Hereby, the factors are grouped into a number of clusters with the correlation within each cluster being larger than between clusters. This effectivly finds groups of factors which have similar time courses. The complete linkage method defines the cluster distance between two clusters to be the maximum distance between their individual components.

The hierarchical clustering is now calculated based on the respective correlation matrices. Hierarchical clustering based on the given correlation matrix is applied with `Ngroups=6` clusters. For every correlation measure the cluster plots are created. The number of clusters was choosen, so that at least `>1` members exist per cluster.
``` {r hierarchical_clustering}
hclust.res <- vector("list", length=length(correlation_methods))
names(hclust.res) <- correlation_methods

# Calculate the hierarical clustering: N clusters for correlation based on method
calculate_clusters <- function(cor_method, N){
  # get correlation matrix
  cor.cluster <- cor.matrices[[cor_method]]
  # perform hierarchical clustering (complete linkage & Euclidion distance measure)
  hc <- hclust(dist(cor.cluster, method="euclidian"), method="complete") 
  # cut into the clusters
  groups <- cutree(hc, k=Ngroups)
  groups.hc.order <- groups[hc$order]
  # store results
  return(list(hc=hc,
              groups=groups,
              groups.hc.order=groups.hc.order))
}

Ngroups <- 6   # number of clusters
for (method in correlation_methods){
  hclust.res[[method]] <- calculate_clusters(cor_method=method, N=Ngroups)
}
# save clustering
save(hclust.res, file=file.path(resultsPath, "data", "hclust.res.Rdata"))
rm(method)
```

In the next step the correlation matrices are plotted in combination with the clustering results
``` {r correlation_heatmap_cluster}
# Create correlation heatmap with hierachical clustering results
correlation_heatmap_cluster <- function(method){
  # matrix and cluster results
  cor.cluster <- cor.matrices[[method]]
  hc.res <- hclust.res[[method]]
  hc <- hc.res$hc
  groups <- hc.res$groups
  # colors
  hmap_colors <- HeatmapColors()
  colorset <- brewer.pal(Ngroups, "Set1")
  color.map <- function(cluster_id) {return(colorset[cluster_id])}
  clusterColors <- unlist(lapply(groups, color.map))
  
  heatmap.2(cor.cluster, col=hmap_colors(10), scale="none",
          key=TRUE, symkey=FALSE, trace="none", cexRow=0.8, cexCol=0.8,
          main=method,
          density.info="none", dendrogram="column", Rowv=as.dendrogram(hc), Colv=as.dendrogram(hc), keysize=0.8,
          key.xlab=method,
          ColSideColors=clusterColors, revC=TRUE,
          sepwidth=c(0.01,0.01),
          sepcolor="black",
          colsep=1:ncol(cor.cluster),
          rowsep=1:nrow(cor.cluster),
          margins=c(12,8))
  legend("left", legend=paste("c", 1:6, sep=""), col= unlist(lapply(1:6, color.map)), pch=15, bty="n")
}

# plot to files
for(method in correlation_methods){
  pdf(file.path(resultsPath, "correlation", sprintf("cor.%s.hclust.pdf", method)), 
      width=10, height=10, pointsize=12) 
  correlation_heatmap_cluster(method=method)
  invisible(dev.off())  
}
rm(method)
```

Display the clusters with the heatmap
``` {r heatmap_ys3, fig.width=10, fig.height=10, error=TRUE, htmlcap='<b>Figure Heatmap Cluster:</b> Correlation matrix based on YS3 correlation with hierachical clustering infromation.'}
# plot ys3 to report
correlation_heatmap_cluster(method="ys3")
```

The following factors are in the `ys3` time course clusters
``` {r list_cluster_members}
# print representatives of cluster
list_cluster_members <- function(method){
  groups <- hclust.res[[method]]$groups
  cat(sprintf("------------------------------------------\n"))
  cat(sprintf("Correlation method: *** %s ***\n", method))
  cat(sprintf("------------------------------------------\n"))
  for (k in 1:Ngroups){
    g <- groups[groups==k]
    cat(sprintf("Cluster %s (N=%s)\n", k, length(g)))
    print(names(g))
    cat("\n")
    # print(paste(names(g), sep=", ", collapse =", " ))
  }  
}
list_cluster_members(method="ys3")
```

## Correlations for non-RNA factors
Plot correlation matrix for histological, biochemical and antibody markers. Calculation of the largest correlation between the non-transcript factors and other factors. The columns in which no correlation coefficient has `abs(value) >= cor.cutoff` are filtered out, i.e. only factors are retained with a absolute correlation coeffient above this threshold.
``` {r correlation_histological}
get_histological_correlations <- function(method="ys3", cor.cutoff=0.6){
  # correlation matrix
  cor_mat <- cor.matrices[[method]]
  
  # non RNA factors which were accepted by ANOVA
  hist_facs <- BDLfactors$id[anova.accept & 
                               (BDLfactors$ftype %in% c("Antibodies", "Biochemistry", "Histology"))]
  # print(hist_facs)
  
  # get the indices of these factors in the correlation matrix
  hist_idx <- rep(NA, length(hist_facs))
  for (k in 1:length(hist_facs)){
    hist_idx[k] <- which(colnames(cor_mat) == hist_facs[k])
  }
  # subset of correlation matrix for histological markers (corresponding rows)
  hist_data <-  cor_mat[hist_idx, ]  
  
  # filter columns by correlation cutoff
  col.accept <- rep(NA, ncol(hist_data))
  for (k in 1:ncol(hist_data)){
    # if there is any correlation >= cutoff in the column, keep it
    col.accept[k] <- any( abs(hist_data[,k])>=cor.cutoff )
  }
  # print(table(col.accept))  # filtered factors based on correlation threshold
  hist_accept <- hist_data[, col.accept]
  
  # sort by the hierarchical cluster ordering
  hist_gene_names <- colnames(hist_accept)[1:(ncol(hist_accept)-nrow(hist_accept))]

  # correlation based cluster for ordering
  hc.res <- hclust.res[[method]]
  
  # create sort index for gene names based on clustering
  sort_idx <- rep(NA, length(hist_gene_names))
  for (k in 1:length(hist_gene_names)){
    sort_idx[k] <- which(names(hc.res$groups.hc.order) == hist_gene_names[k])
  }
  # first sorted genes than the histological factors
  hist_sorted <- hist_accept[, c(hist_gene_names[order(sort_idx)], rownames(hist_data))]  
  return(hist_sorted)
}

histological_correlations <- get_histological_correlations(method="ys3")

# plot the subset of the correlation matrix to file
plot_histological_correlations <- function(hist_data){
  hmap_colors <- HeatmapColors()
  corrplot(hist_data, method="circle", type="full", 
           tl.cex=0.7, tl.col="black", col=hmap_colors(10))  
}

# plot to file
pdf(file.path(resultsPath, "correlation", "histological_correlations.pdf"), 
    width=10, height=4, pointsize=12)
plot_histological_correlations(histological_correlations)
invisible(dev.off())

# print correlation values
print(round(t(histological_correlations), digits=2))
```

``` {r histological_correlations_plot, fig.width=10, fig.height=4, error=TRUE, htmlcap='<b>Figure Histological Correlations:</b> Correlations between non-RNA factors and all other factors.'}
# plot in report
plot_histological_correlations(histological_correlations)
```

### Top correlations for non-RNA factors
List the top correlations of every non-RNA factor, i.e. histological (H), biochemical (B) and immunohistochemical (A). The top correlations are sorted by absolute correlation values.
``` {r plot_hist_topcors}
plot_hist_topcors <- function(method="ys3", labels=TRUE, mfrow=c(10,1), 
                              only_RNA=FALSE, Ntop=10, printResults=FALSE){
  hist_facs <- rownames(histological_correlations)
  cor.mat <- cor.matrices[[method]]
  
  par(mfrow=mfrow)
  hmap_colors <- HeatmapColors()
  for (name in hist_facs){
    if (only_RNA==TRUE){
      # get the elements which are not histological factors
      v <- cor.mat[!(colnames(cor.mat) %in% hist_facs), name]  
    } else {
      # get all elements (including other non-RNA factors)
      v <- cor.mat[, name]  
    }
    
    # sort by absolute correlation
    v.sorted <- rev(v[order(abs(v))])
    
    # and get the Ntop values without the self-correlation (idx=1)
    mv <- t(as.matrix(v.sorted[2:(Ntop+1)]))
    rownames(mv) <- c(name)
    
    # overview
    if (printResults==TRUE){
      print(round(mv, digits=2)) 
    }
    
    # plot without labels to have identical size of figure
    if (labels==FALSE){
      rownames(mv) <- NULL
      colnames(mv) <- NULL
    }
    corrplot(mv, method="pie", type="full", 
             tl.cex=1.0, tl.col="black", col=hmap_colors(100), insig="p-value", sig.level=-1,
             p.mat=mv, cl.pos="n")
  }
  par(mfrow=c(1,1))
}

# plot to file
# necessary to plot once with and without labels so that all the plots are scaled equally
pdf(file.path(resultsPath, "correlation", "histological_topcors_1.pdf"), 
    width=10, height=10, pointsize=12)  
plot_hist_topcors(method="ys3", labels=TRUE)
invisible(dev.off())
pdf(file.path(resultsPath, "correlation", "histological_topcors_2.pdf"), 
    width=10, height=10, pointsize=12)  
plot_hist_topcors(method="ys3", labels=FALSE)
invisible(dev.off())
```

``` {r plot_hist_topcors_report, fig.width=10, fig.height=10, error=TRUE, htmlcap='<b>Figure Histological Top Correlations:</b> Top correlations between non-RNA factors and all other factors.'}
# plot in report
plot_hist_topcors(method="ys3", labels=TRUE, mfrow=c(5,2), printResults=TRUE)
```

## Mean cluster time course
In the next step, we were interested in the time courses of the time course clusters: What are the typical profiles found in the different clusters and which factors are in these clusters. For the comparison of individual factors against each other and against the mean time course of the cluster the factors were normalized. The normalization was hereby performed for every factor separately based on
$$f_{k}^{norm}(t_{i,r}) = \frac{f_{k}(t_{i,r})-<f_{k}>}{max(f_{x})-min(f_{x})}$$

``` {r normalize_bdl_data}
suppressPackageStartupMessages(library('matrixStats'))
dir.create(file.path(resultsPath, 'cluster'), showWarnings=FALSE)

# Normalize the individual factors
normalize_factor <- function(a, min.a, max.a, mean.a){
  res <- (a - mean.a)/(max.a - min.a)
}
# Calculate min, max and mean for all single factors (normalization constants)
factor.norm <- data.frame(min=apply(BDLdata, 2, min, na.rm=TRUE), 
                            max=apply(BDLdata, 2, max, na.rm=TRUE),
                            mean=apply(BDLdata, 2, mean, na.rm=TRUE))
# Function for normalizing subset of BDL data with factor normalization constants.
normalize_BDLdata <- function(data, factor.norm){
  dnorm <- data
  for (name in colnames(data)){
    dnorm[, name] <- normalize_factor(a=dnorm[, name], 
                                      min.a=factor.norm[name, "min"], 
                                      max.a=factor.norm[name, "max"],
                                      mean.a=factor.norm[name, "mean"]) 
  }
  return(dnorm)
}

# Normalize the full data set
BDLdata.norm <- normalize_BDLdata(data=BDLdata, factor.norm=factor.norm)
# Calculate the mean of the normalized data set
BDLmean.norm <- bdl_mean_data(BDLdata.norm, BDLsamples)
```

Based on the normalized factor data the mean time course of the clusters were calculated, i.e. the time course due to averaging over all factors in the individual clusters. In addition the mean time course averaged over the repeats for all factors are plotted.
``` {r plot_clusters_mean}
# Plot mean cluster with SD range and the individual representatives in the cluster.
plot_clusters_mean <- function(method){  
  # clusters for correlation method
  hc.res <- hclust.res[[method]]
  groups.hc.order <- hc.res$groups.hc.order

  # par(mfrow=c(ceiling(sqrt(Ngroups)),ceiling(sqrt(Ngroups))))
  par(mfrow=c(2,3))
  steps <- 1:Nt  # time points
  for (k in 1:Ngroups){
    # get representatives of cluster
    g <- groups.hc.order[groups.hc.order==k]
    dgroup <- BDLmean.norm[names(g)]  # normalized mean data for group (normalized within factor)
  
    # mean and sd for timepoints (i.e. over all factors in the cluster) 
    g.mean <- rowMeans(as.matrix(dgroup), na.rm=TRUE)
    g.sd <- rowSds(as.matrix(dgroup), na.rm=TRUE) 
    
    # plot sd range
    plot(factor(levels(BDLsamples$time_fac), levels=levels(BDLsamples$time_fac)), rep(-2, 8), type="n", xlab="", ylab="",
         xlim=c(1, Nt), ylim=1.1*c( min(min(dgroup, na.rm=TRUE), na.rm=TRUE), 
                                    max(max(dgroup, na.rm=TRUE), na.rm=TRUE) ),
         main=sprintf("%s : Cluster %s (N=%s)", method, k, ncol(dgroup)))
    polygon(c(steps, rev(steps)), c(g.mean+g.sd, rev(g.mean-g.sd)),
            col = rgb(0,0,1,0.2), border = NA)
    
    # individual data
    for (name in names(g)){
      points(steps, dgroup[, name], pch=16, col="black")
      lines(steps, dgroup[, name], col=rgb(0.5,0.5,0.5,1.0), lwd=1)
    }
    # mean over factors in cluster
    lines(steps, g.mean, col="blue", lwd=2)
  }
  par(mfrow=c(1,1))
}

# plot mean clusters to file
for (method in correlation_methods){
  pdf(file.path(resultsPath, 'cluster', sprintf("%s_cluster_mean.pdf", method)), 
        width=10, height=7.5, pointsize=12)
  plot_clusters_mean(method=method) 
  invisible(dev.off())
}
```

``` {r plot_clusters_mean_report, fig.width=10, fig.height=7.5, error=TRUE, htmlcap='<b>Figure Cluster mean:</b> Plot of mean cluster data.'}
# plot ys3 clusters to report
plot_clusters_mean(method="ys3")
```

In addition the plots of the individual factors in the clusters are generated. Despite being clustered together still quit large variance exists between the different factors in the clusters.
``` {r plot_clusters_items}
# Plot individual time courses in cluster
plot_clusters_items <- function(method, toFile=FALSE){
  # get the cluster assignment for the given method
  hc.res <- hclust.res[[method]]
  groups.hc.order <- hc.res$groups.hc.order
  
  for (k in 1:Ngroups){
    if (toFile==TRUE){
      pdf(file.path(resultsPath, "cluster", sprintf("%s_cluster_%s.pdf", method, k)), 
          width=10, height=10, pointsize=8)
    }
    
    g <- groups.hc.order[groups.hc.order==k]
    N <- ceiling(sqrt(length(g)))
    par(mfrow=c(N,N))
    for (name in names(g)){
      plot_single(name_A=name) 
    }
    par(mfrow=c(1,1))  
    if (toFile==TRUE){
      invisible(dev.off())
    }
  }  
}

# plot individual factors in clusters on disk
for (method in correlation_methods){
  plot_clusters_items(method=method, toFile=TRUE)  
}
rm(method)
```

## Top cluster representatives
In this step, the top cluster representatives are calculated for all clusters. The higher the correlation between the factor and the cluster mean, the higher the respective factor is scored. The used correlation measure is `YS3`.
``` {r top_cluster_representatives}

# Calculate and plot the top cluster representatives.
# Currently only ys3 based calculation is supported. This function should be refactored 
# to support all correlation measures.

hmap_colors <- HeatmapColors()

# calculates the correlation between the cluster mean and all other members in the cluster
calculate_cluster_correlations <- function(method="ys3"){
  if (!identical(method, "ys3")){
    stop("Only ys3 top correlation is currently supported")
  }
  # get the clusters
  hc.res <- hclust.res[[method]]
  groups.hc.order <- hc.res$groups.hc.order
  
  # mean cluster time course as additional factor
  name_cluster <- sprintf("cluster.mean")
  
  # calculate and plot the top correlations for every cluster
  cluster.cor <- vector("list", length=Ngroups)
  for (k in 1:Ngroups){
      
      # get members of cluster
      g <- groups.hc.order[groups.hc.order==k]
      # get normalized data for members of cluster
      dgroup <- BDLdata.norm[names(g)]
      # calculate cluster mean and add as factor (mean of all factors in cluster for given time point and sample)
      # (for S*)
      g.mean <- rowMeans(as.matrix(dgroup), na.rm=TRUE)
      dgroup[[name_cluster]] <- g.mean
      
      # calculate mean averaged over repeats (for A** and M*)
      dgroup.mean <- bdl_mean_data(dgroup, BDLsamples)

      # now calculate the ys3 correlation with the created data.frame containing the mean cluster factor
      ysr.res <- ysr.matrices(dgroup.mean, BDLmean.time, use="pairwise.complete.obs")
      # Spearman correlation on individual data points
      cor.S_star <- ( cor(dgroup, method="spearman", use="pairwise.complete.obs") + 1 )/2
      # YS3 in [-1,1]
      ys3 <- 2*( (w$w1*cor.S_star + w$w2*ysr.res$A_star2 + w$w3*ysr.res$M_star) - 0.5)
      
      # store the matrix
      cluster.cor[[k]] <- ys3
  }
  return(cluster.cor)
}
cluster.cor <- calculate_cluster_correlations(method="ys3")

# plot correlations for individual clusters
for (k in 1:Ngroups){
  cluster.cor.mat <- cluster.cor[[k]]
  if (ncol(cluster.cor.mat)<20){
    corrplot(cluster.cor.mat, method="pie", type="full", 
           main=sprintf("Cluster %s", k),
             tl.cex=0.8, tl.col="black", col=hmap_colors(100), 
             insig="p-value", sig.level=-1, p.mat=cluster.cor.mat,
             cl.pos="n")
  } else {
    corrplot(cluster.cor.mat, method="circle", type="full", 
           main=sprintf("Cluster %s", k),
             tl.cex=0.8, tl.col="black", col=hmap_colors(100), 
             cl.pos="n")
  }
}
rm(k, cluster.cor.mat)
```

Now the top correlations between the cluster mean and the members of the respective cluster are plotted.
``` {r top_cluster_correlations}
plot_top_cluster_representatives <- function(method="ys3", Ntop=11, labels=TRUE){
  par(mfrow=c(Ngroups,1))
  name_cluster <- sprintf("cluster.mean")
  for (k in 1:Ngroups){
      # YS3 in [-1,1]
      ys3 <- cluster.cor[[k]]
  
      # correlation for the cluster mean
      v <- ys3[, which(colnames(ys3)==name_cluster)]
      # sort by absolute correlation
      v.sorted <- rev(v[order(abs(v))])

      # reduce to Ntop candidates and fill short clusters with zeros tto Ntop
      if (length(v.sorted)>=(Ntop+1)){
        v.sorted <- v.sorted[2:(Ntop+1)]
      } else {
        v.sorted <- c(v.sorted[2:length(v.sorted)], rep(0,Ntop-length(v.sorted)+1))
      }
      # prepare data for corrplot
      mv <- t(as.matrix(v.sorted))
      rownames(mv) <- c(name_cluster)
      if (labels == FALSE){
        colnames(mv) <- NULL
      }
      corrplot(mv, method="pie", type="full", 
             tl.cex=1.0, tl.col="black", col=hmap_colors(100), 
             insig="p-value", sig.level=-1, p.mat=mv,
             cl.pos="n")
  }
  par(mfrow=c(1,1))
}

# plot to file (with and without names)
pdf(file.path(resultsPath, "cluster", "cluster_top_representatives_01.pdf"), 
    width=5, height=5, pointsize=12)
plot_top_cluster_representatives(labels=FALSE)
invisible(dev.off())
pdf(file.path(resultsPath, "cluster", "cluster_top_representatives_02.pdf"), 
    width=10, height=10, pointsize=12)
plot_top_cluster_representatives(labels=TRUE)
invisible(dev.off())

# plot in report
plot_top_cluster_representatives(labels=TRUE)
```

## YS3 cluster summary
Full overview over the members of the `{r} Ngroups` clusters with respective YS3 correlation to the cluster mean and ANOVA result.
``` {r ys3_cluster_table}
# print representatives of cluster
list_cluster_information <- function(method="ys3"){
  if(!identical(method, "ys3")){
    stop("Cluster correlation only calculated for ys3")
  }
  groups <- hclust.res[[method]]$groups
  cat(sprintf("------------------------------------------\n"))
  cat(sprintf("Correlation method: *** %s ***\n", method))
  cat(sprintf("------------------------------------------\n"))
  for (k in 1:Ngroups){
    cluster.cor.mat <- cluster.cor[[k]]
    # create data.frame with ANOVA & cluster correlation
    g <- groups[groups==k]
    cat(sprintf("Cluster %s (N=%s)\n", k, length(g)))
    Ng <- length(names(g))
    rows <- vector("list", length=Ng)
    for (kn in 1:Ng){
      name <- names(g)[kn]
      row <- df.anova[which(df.anova$factor==name),]
      row$cluster.cor <- cluster.cor.mat[name, "cluster.mean"]
      rows[[kn]] <- row
    }
    df <- do.call("rbind", rows)
    df <- df[order(-df$cluster.cor), ]

    # cleanup the data.frame
    rownames(df) <- df$factors
    df <- df[, c("p.holm", "sig.holm", "ftype", "fshort", "cluster.cor")]
    df$cluster.cor <- round(df$cluster.cor, digits=2)
    df$p.holm <- sprintf("%1.2E", df$p.holm)
    print(df)
    
    cat("\n")
    # create list for figure legend
    # print(paste(sprintf("%s (%s)", rownames(df), df$cluster.cor),
    #            sep=", ", collapse =", " ))
  }  
}
# List information in cluster
list_cluster_information(method="ys3")
```

# Decision trees
## Introduction
For the prediction of disease progression after BDL a decision tree was used. The regression tree was fitted with the R package `rpart`, being the open-source implementation of CART, implementing algorithms for recursive partitioning for classification following in most details closely Breiman et. al (1984) (*Breiman L., Friedman J. H., Olshen R. A., and Stone, C. J. (1984) Classification and Regression Trees. Wadsworth*). 

The predictor variables are the 6 mean time courses of the clusters, the dependent variable is the log transformed time class, to get aprroximately equidistant intervals between the trainings classes in the regression models. The tree is hereby built in a two-step process:  First the single variable is found which best splits the data into two groups. The data than separated based on the split, and the splitting process is applied
separately to each sub-group, and so on recursively until the subgroups either reach a minimum size (`minbucket`) or until no improvement can be made. The second stage of the procedure consists of using cross-validation to trim back the full tree. The splitting criterion, which is used to decide which variable gives the best split for nodes in the regression trees is $SS_{T}-(SS_{L} + SS_{R})$, with $SS_{T} = \sum{(y_{i}-<y>)^2}$ the sum of squares for the node and $SS_{R}$ and $SS_{L}$ the sums of squares for the left and right son. This is equivalent to choosing the split ot maximize the between-groups sum-of-squares in a simple analysis of variance (see *An Introduction to Recursive Partitioning Using the RPART Routines. T.M. Therneau and E.J. Atkinson, Mayo Foundation, 2015*). Two important parameters controlling the resulting tree are 

* `minsplit` : The minimum number of observations in a node for which the routine
will even try to compute a split. The default is 20 and is set to 6 in the tree calculation ($N_{r}=5$ repeats per time point).
* `minbucket` : The  minimum  number  of  observations  in  a  terminal  node. This defaults to minsplit/3.

``` {r decision_tree_folder}
suppressPackageStartupMessages(library(rpart))
suppressPackageStartupMessages(library(rpart.plot))
suppressPackageStartupMessages(library(caret))
dir.create(file.path(resultsPath, 'decision_tree'), showWarnings=FALSE)
```

To generate approximatly equally distant time classes for training the regression tree the log transformed time values are used. The transformation used is
$$\tilde{t_{i}} = log(t_{i} +1)$$
```{r log_transformation} 
# Transform data to log scale (for comparable time intervals)
log_transform <- function(data){
  log(data+1)
}
# Back transformation
log_transform_back <- function(log_data){
  exp(log_data)-1
}
# Resulting time transformations
time_transformation <- data.frame(time=BDLmean.time, log_time=log_transform(BDLmean.time))
print(round(time_transformation,2))
```

## Trainings data
In a first step the mean cluster trainings data for fitting the decision tree is prepared. The predictor variables are the mean samples of the clusters, the dependent variable is the log transformed time of the respective sample. The trainings set consists of the $N_{t}*N_{r}=40$ samples.
```{r tree_trainings_data}
  prepare_treedata_mean <- function(method="ys3"){
  # Hierarchical clusters based on ys3 to fit the regression tree
  hc.res <- hclust.res[[method]]
  groups <- hc.res$groups
  
  # Prepare training set for fitting the decision trees (mean cluster data set, 
  # i.e. mean over normalized factors in cluster).
  na.vec <- rep(NA, Nt*Nr)
  treedata.mean <- data.frame(c1=na.vec, c2=na.vec, c3=na.vec, c4=na.vec, c5=na.vec, c6=na.vec)
  # for every sample
  for (ks in 1:(Nt*Nr)){
    # create the mean over the cluster
    for (kgroup in 1:Ngroups){
      # get factors in the cluster
      factors <- names(groups[groups==kgroup])
      # calculate mean over normalized data
      treedata.mean[ks, kgroup] <- mean(as.numeric(BDLdata.norm[ks, factors]), na.rm=TRUE)
    }
  }
  # add log transformed time [h] for regression
  treedata.mean$logtime <- log_transform(BDLsamples$time)
  # add experimental time class
  treedata.mean$class <- BDLsamples$time_fac
  
  return(treedata.mean)
}
treedata.mean <- prepare_treedata_mean()

# mean cluster data set for model fitting
print(round(treedata.mean[, -8], digits=2))
# save the trainings data
save(treedata.mean, file=file.path(resultsPath, "data", "treedata.mean.Rdata"))
```

## Fit regression tree
Fit the log transformed time against the mean cluster data for the time points. Set control parameters for the algorithm are `minsplit=6` and `minbucket=2` to account for the low number of samples (`N=40`). In the following the regression tree is fitted and the information for the resulting tree is provided.
``` {r fit_regression_tree}
# formula for regression tree
formula.reg = paste("logtime ~ c1 + c2 + c3 + c4 + c5 + c6")

# fit regression tree with mean cluster data
tree.reg <- rpart(formula=formula.reg, 
                  data=treedata.mean, 
                  method="anova", 
                  control=rpart.control(minsplit=6, minbucket=2))

# detailed overview of the resulting tree
print(tree.reg)
summary(tree.reg)
printcp(tree.reg)
tree.reg$frame

# pretty plot of tree to file
pdf(file.path(resultsPath, 'decision_tree', "regression_tree.pdf"), 
    width=10, height=5, pointsize=12)  
prp(tree.reg, type=0, extra=101, yesno=TRUE)
invisible(dev.off())

# and report
prp(tree.reg, type=0, extra=101, yesno=TRUE)

# visualize cross-validation results 
# rsq.rpart(tree.reg) 
```

Only a subset of all mean clusters is used in the regression tree:
``` {r variables_in_tree}
# variables used for splitting in the decision tree
tree.nodes <- (tree.reg$frame)$var
tree.nodes <- tree.nodes[tree.nodes != "<leaf>"]
tree.vars <- as.character(sort(unique(tree.nodes)))
rm(tree.nodes)
# variables used for splitting decisions in tree
print(tree.vars)
```

Perform leave one out cross validation
``` {r leave_one_out_tree}
# fit the 5 repeat regression trees (leave one out)
# in total 40 cross validations ! because mice are not time followed
trees.test <- vector("list", length=Nr*Nt)
pred.test.log <- rep(NA, length=Nr*Nt)
for (k in 1:(Nr*Nt)){
  idx.subset <- 1:(Nr*Nt)
  idx.subset <- idx.subset[-k]
  print(idx.subset)
  
  # fit tree with the subset
  t.test <- rpart(formula=formula.reg, 
                  data=treedata.mean[idx.subset, ], 
                  method="anova", 
                  control=rpart.control(minsplit=6, minbucket=2))  
  trees.test[[k]] <- t.test
  
  # prediction on left out sample
  pred.test.log[k] <- predict(t.test, newdata=treedata.mean[k,], type="vector")
}
# transformation to time in [h]
pred.test <- log_transform_back( pred.test.log )

# plot the log prediction and error
par(mfrow=c(1,2))
plot(log_transform(BDLsamples$time), log_transform(pred.test),
     main="Leave one out cross validation")
abline(a=0, b=1, col="gray")
textxy(log_transform(BDLsamples$time), log_transform(pred.test), 1:(Nr*Nt), col="black", cex=0.6)

plot(log_transform(BDLsamples$time), log_transform(BDLsamples$time)-log_transform(pred.test),
     main="Leave one out cross validation error")
abline(a=0, b=0, col="gray")
textxy(log_transform(BDLsamples$time), log_transform(BDLsamples$time)-log_transform(pred.test), 
       1:(Nr*Nt), col="black", cex=0.6)
par(mfrow=c(1,1))

```

## Prediction on trainings data
Predict data with the tree, here the time class leaves based on the mean cluster data. The regression tree predicts log classes, which are back-transformed to time in [h]. So how good is the tree performing on the trainings data set, i.e. the mean cluster data. Predictons are evaluated based on the distance between the predicted and the experimental time classes based on the following distance measure on log scale
$$d = \frac{1}{N_{s}} \sqrt{\sum{TODO}}$$

``` {r prediction_distance}
# L2 (euclidian) distance measurement on the log transformed data. Analog to the distance measurement in fitting 
# the regression tree.
log_distance <- function(d1, d2){
  # sums over all the distances of the samples in log space
  log_rmsd <- sqrt(sum( (log_transform(d1)-log_transform(d2) )^2 ))/length(d1)
  return(log_rmsd)
}
```
Prediction on trainings data
``` {r predict_trainings_data}
# mean cluster predictions
pred.mean.log <- predict(tree.reg, newdata=treedata.mean, type="vector")
# transformation to time in [h]
pred.mean <- log_transform_back( pred.mean.log )

# TODO: create the overview matrix which experimental time classes are predicted in which
# tree time classes (on normal and log scale)
# TODO: heatmap of the matrix
# pred.matrix.mean <- matrix(NA, nrow=nrow(node_ranges), ncol=length(levels(as.factor(BDLsamples$time))))

# Distance calculation (predicted to experimental)
dist.mean.all <- treedata.mean$logtime - pred.mean.log
dist.mean <- log_distance(pred.mean, BDLsamples$time)

# plot predicted ~ experimentell
plot_mean_prediction <- function(){
  par(mfrow=c(1,2))
  plot(BDLsamples$time, pred.mean, pch=15, col=rgb(0,0,1, 0.2), main="Regression Tree:\nPredicted ~ experimentell time",
       xlab="experimentell time [h]", ylab="predicted time [h]")
  abline(a=0, b=1, col=rgb(0.5,0.5,0.5,0.5))
  hist(dist.mean.all, breaks=seq(from=-1.05, to=1.05, by=0.1), 
       main="Histogram prediction error:\nmean cluster data",
       col=rgb(0.5,0.5,0.5,0.5),
       xlab="logtime(exp)-logtime(pred)")
  par(mfrow=c(1,1))  
}
# plot to file
pdf(file.path(resultsPath, 'decision_tree', "prediction_mean.pdf"), 
    width=10, height=5, pointsize=12)  
plot_mean_prediction()
invisible(dev.off())
# and report
plot_mean_prediction()
```

The ranges of the predicted classes by the regression trees can be calculated based on the split points on log scale. These provides the information which time points would be classified in which class in the regression tree.
``` {r tree_class_ranges}
calculate_node_ranges <- function(){
  # classes via predicted classes for cluster data
  node_levels <- levels(as.factor(pred.mean))
  # These are the predicted classes
  node_classes <- round(as.numeric(node_levels), digits=1)
  
  # get the intervals of the time classes
  node_mean <- as.numeric(levels(as.factor(pred.mean.log)))
  node_midpoints <- (node_mean[2:length(node_mean)] + node_mean[1:(length(node_mean)-1)])/2
  # minimum of range
  node_min <- node_mean
  node_min[2:length(node_min)] <- node_midpoints
  # maximum of range
  node_max <- node_mean
  node_max[1:(length(node_min)-1)] <- node_midpoints
  # ranges in log scale
  node_ranges.log <- data.frame(node_mean, node_min, node_max)
  
  node_ranges <- data.frame(mean=log_transform_back(node_mean), 
                            min=log_transform_back(node_min),
                            max=log_transform_back(node_max))
  rownames(node_ranges) <- paste("class", 1:nrow(node_ranges))
  return(node_ranges)
}

# predicted time classes by decision tree
node_ranges <- calculate_node_ranges()
print(round(node_ranges, digits=1))
```

## Test data for evaluation
### Single factor per cluster
Prepare the evaluation data sets for the fitted trees consisting of all single factor combinations from the clusters used in the decision tree. Only the combinations are created which are from clusters used for decisions in the tree
``` {r single_factor_data} 
# names of factors in the clusters
cluster_names <- paste('c', 1:Ngroups, sep="")
cluster.factors <- vector("list", length=Ngroups)
groups <- (hclust.res$ys3)$groups  # get the ys3 groups
for (k in 1:Ngroups){
  cluster.factors[[k]] <- as.character(names(groups[groups==k]))
}
names(cluster.factors) <- cluster_names

# create data.frame of all single combinations from clusters
single.combinations <- expand.grid(cluster.factors, stringsAsFactors=TRUE)
names(single.combinations) <- names(cluster.factors)
# number of single combinations
Nsingle <- nrow(single.combinations)
print(Nsingle)  # 88572 combinations

# create all single factor data
print("Calculating single factor data (~ 3min) ... ")
ptm <- proc.time()  
treedata.single <- vector("list", Nsingle)  # list for all combinations
for (k in 1:Nsingle){
  # ----------------------------------------------------------------
  # THIS HAS TO BE FAST (<0.005 s)
  # ----------------------------------------------------------------
  # ptm <- proc.time() # Start the clock!
  
  # get factor data
  tmp <- BDLdata.norm[, t(single.combinations[k, ]) ]
  # add regression values
  tmp[c("class", "logtime")] <- treedata.mean[ c("class", "logtime")]
  
  # add factor fields 
  tmp[, paste(cluster_names, '.id', sep="")] <- single.combinations[k,]
  colnames(tmp) <- c(cluster_names, 'class', 'logtime', paste(cluster_names, '.id', sep=""))
  
  # store data 
  treedata.single[[k]] <- tmp
  
  # if (k%%500 == 0){print(k)} 
  # print(proc.time()-ptm)  # Stop the clock
}
# Stop the clock
print(proc.time() - ptm)
print("... calculated.")
rm(tmp,k)
# treedata.single[[1]]

# which factor combinations only use genes
factor_is_gene <- BDLfactors$ftype %in% c("GE_ADME", "GE_Cytokines", "GE_Fibrosis")
names(factor_is_gene) <- BDLfactors$id

# vector for lookup if only genes were used
gene_only.single <- vector("logical", Nsingle)
for (k in 1:Nsingle){
  gene_only.single[k] <- all(factor_is_gene[t(single.combinations[k,])])
}
rm(k)
```

### Double factor per cluster
Create a sample of double combinations from the various clusters.
``` {r double_factor_data}
print("Calculating double factor data (~ 1min) ... ")
ptm <- proc.time()
set.seed(123456)
Ndouble <- 10000
treedata.double <- vector("list", Ndouble)  # list for sampled double combinations
for (k in 1:Ndouble){
  # sample from the 4 clusters without replacement
  n1 = sample(cluster.factors[[1]], 2, replace=FALSE)  
  n2 = sample(cluster.factors[[2]], 2, replace=FALSE)  
  n3 = sample(cluster.factors[[3]], 2, replace=FALSE)  
  n4 = sample(cluster.factors[[4]], 2, replace=FALSE)  
  n5 = sample(cluster.factors[[5]], 2, replace=FALSE)  
  n6 = sample(cluster.factors[[6]], 2, replace=FALSE)  
  
  # The mean of the combination is used (handle NAs)
  tmp <- 0.5 * (  BDLdata.norm[, c(n1[1], n2[1], n3[1], n4[1], n5[1], n6[1])] 
                + BDLdata.norm[, c(n1[2], n2[2], n3[2], n4[2], n5[2], n6[2])] )
  
   # add class and regression values
  tmp[c("class", "logtime")] <- treedata.mean[ c("class", "logtime")]
  
  # add factor fields 
  tmp[ , paste(cluster_names, '.id', sep="")] <- data.frame(paste(n1, collapse="__"), 
                                              paste(n2, collapse="__"),
                                              paste(n3, collapse="__"),
                                              paste(n4, collapse="__"),
                                              paste(n5, collapse="__"),
                                              paste(n6, collapse="__"))
  colnames(tmp) <- c(cluster_names, 'class', 'regvalue', paste(cluster_names, '.id', sep=""))
  # store data
  treedata.double[[k]] <- tmp
}
print(proc.time() - ptm)
print("... calculated.")
rm(k, tmp)
# treedata.double[[5]]
```

## Prediction on test data
### Single representative predictions
Time class prediction with regression tree for single representative from each cluster
``` {r single_prediction}
print("Predicting single factor data (~ 2min) ... ")
pred.single.all <- vector("list", length(treedata.single))
for (k in (1:length(treedata.single))){
  # prediction and back transformation  
  pred.single.all[[k]] <- log_transform_back( predict(tree.reg, newdata=treedata.single[[k]], method="anova") )
}
pred.single <- do.call("rbind", pred.single.all)

# distance for all predictions on single factor per cluster
dist.single <- rep(NA, Nsingle)
for (k in 1:Nsingle){
  dist.single[k] <- log_distance(pred.single[k,], BDLsamples$time)
}
```

### Double representative predictions
Time class prediction with regression tree for random selection of two representatives from each clusters
```{r double_prediction}
print("Predicting double factor data (~ 1min) ... ")
pred.double.all <- vector("list", length(treedata.double))
for (k in (1:length(treedata.double))){
  pred.double.all[[k]] <- log_transform_back( predict(tree.reg, newdata=treedata.double[[k]], method="anova") )
}
pred.double <- do.call("rbind", pred.double.all)

# distance for predictions on 2 sampled factors per cluster 
dist.double <- rep(NA, Ndouble)
for (k in 1:Ndouble){
  dist.double[k] <- log_distance(pred.double[k,], BDLsamples$time)
}
```

## Best factor combinations
Finding the best regression trees based on i) all single representatives from the clusters; and ii) single representatives only consisting of gene probes. The best is defined by minimal euclidian distance between experimental classes and predicted classes on the log scale. The best tree is not refitted with the respective factors in the tree, but the mean cluster tree uses the respective factor data for prediction.
```{r best_trees}
# Best decision tree using all factors
# Find unique combinations with minimal distance
dist.rep.best <- min(dist.single)
# best combination of representatives for the clusters (remove duplicates)
rep.best <- unique(single.combinations[which(dist.single==dist.rep.best), c("c1", "c3", "c4", "c5")])
rep.best.idx <- rownames(rep.best)[1]
# predictions of best representative
pred.rep.best <- pred.single[as.numeric(rep.best.idx), ]

print("Best single representatives for decision tree:")
print(rep.best)
print(dist.rep.best)
# ------------------------
# Best decision tree using only gene factors, i.e. first reduce to the gene combinations
dist.single.genes <- dist.single[gene_only.single]
single.combinations.genes <- single.combinations[gene_only.single,]
dist.gene.best <- min(dist.single.genes)
# find best gene combination
gene.best <- unique(single.combinations.genes[which(dist.single.genes==dist.gene.best), c("c1", "c3", "c4", "c5")])
# predictions with best representative
gene.best.idx <- rownames(gene.best)
pred.gene.best <- pred.single[as.numeric(gene.best.idx), ]
pred.gene.best

print("Best single representatives based on genes for decision tree:")
print(gene.best)
print(dist.gene.best)
# ------------------------
# Plot time courses of the representatives/factors in provided tree combinations
plot_tree_representatives <- function(combination){
  Nc <- ncol(combination)
  Nr <- nrow(combination)
  par(mfrow=c(Nr, Nc))
  for (kr in 1:Nr){
    for (kc in 1:Nc){
      cluster <- colnames(combination)[kc]
      name <- as.character(combination[kr, kc])
      plot_single(name)
    }  
  }
  par(mfrow=c(1,1))
}

# plot to file
pdf(file.path(resultsPath, "decision_tree", "rep.best.representatives.pdf"), 
    width=10, height=6, pointsize=12)
plot_tree_representatives(rep.best)
invisible(dev.off())
pdf(file.path(resultsPath, "decision_tree", "gene.best.representatives.pdf"), 
    width=10, height=3, pointsize=12)
plot_tree_representatives(gene.best)
invisible(dev.off())

# plot to report
plot_tree_representatives(rep.best)
plot_tree_representatives(gene.best)
```

## Prediction errors
Plot the distance distributions of single and double representatives and the best gene and representative trees.
``` {r tree_distance_distribution}
# distance histogram
plot_tree_distance_dist <- function(){
  breaks <- seq(from=0, to=0.40, by=0.0125)
  hist(dist.single, freq=FALSE, breaks=breaks,
      main="Distance between predicted and experimentell time",
      xlab="RMSD(time.predicted, time.exp)",
      col=rgb(0.7,0.7,0.7, 1),
      ylim=c(0,15)
  )
  hist(dist.double, freq=FALSE, breaks=breaks,
       col=rgb(1,0,0, 0.5), add=TRUE)
  
  abline(v=dist.mean, col=rgb(0,0,1, 0.5), lwd=2)
  abline(v=dist.rep.best, col="black", lwd=2)
  abline(v=dist.gene.best, col="grey", lwd=2)
  
  legend("topright", legend=c("single factor", "double factor", "mean cluster"), 
             col=c(rgb(0.7, 0.7, 0.7, 1),rgb(1, 0, 0, 0.5),rgb(0, 0, 1, 0.5)),
             bty="n", cex=1.0, pch=15)
}
# plot to file
pdf(file.path(resultsPath, "decision_tree", "tree_distance_distribution.pdf"), 
    width=10, height=6, pointsize=14)
plot_tree_distance_dist()
invisible(dev.off())

# plot in report
plot_tree_distance_dist()
```

## Predictive performance
Evaluation of the predictions. Which experimental classes were predicted in which time classes of the regression tree for the mean cluster data (trainings data), single representative from each clusters and double representatives from each cluster.
``` {r barplot_predictions}
# Predicted classes of the regression tree
node_levels <- levels(as.factor(pred.mean))
node_classes <- round(as.numeric(node_levels), digits=1)
node_classes

# Plot of the predicted classes with the decision tree
plot_predicted_classes <- function(){
  # bar_colors <- brewer.pal(4, "Set3")
  bar_colors <- c(rgb(0.9,0.9,0.9), 
                  rgb(0.7,0.7,0.7),
                  rgb(1,0,0, 0.5),
                  rgb(0,0,1, 0.5))
  par(mfrow=c(2,4))
  for (k in 1:Nt){
    # single factor predictions
    data <- as.vector(pred.single[, ((1:Nr)+Nr*(k-1))])
    tab.single <- table(factor(data, levels=node_levels))/length(data)
    
    # two factor predictions
    data <- as.vector(pred.double[, ((1:Nr)+Nr*(k-1))])
    tab.double <- table(factor(data, levels=node_levels))/length(data)
    
    # mean cluster predictions
    data <- as.vector(pred.mean[((1:Nr)+Nr*(k-1))])
    tab.mean <- table(factor(data, levels=node_levels))/length(data)
    
    # best single representative
    # data <- as.vector(pred.rep.best[((1:Nr)+Nr*(k-1))])
    # tab.rep.best <- table(factor(data, levels=node_levels))/length(data)
    
    # best single gene representative
    data <- as.vector(pred.gene.best[((1:Nr)+Nr*(k-1))])
    tab.gene.best <- table(factor(data, levels=node_levels))/length(data)
    
    # combined table
    tab <- rbind(tab.single, tab.double, tab.gene.best, tab.mean)
    colnames(tab) <- round(as.numeric(colnames(tab)), digits=1)
    
    # create the bar plot
    name <- sprintf("Time after BDL: %sh", levels(as.factor(BDLsamples$time))[k])
    barplot(tab, beside=TRUE,
            main=name, 
            xlab="predicted time class [h]", ylab="fraction of predictions",
            ylim=c(0,1), col=bar_colors) 
    if (k==1){
      legend("topright", legend=c("single factors", "double factors", 
                                  "best single gene", "mean cluster"), 
             col=bar_colors,
             bty="n", cex=1.0, pch=15)
    }
  }
  par(mfrow=c(1,1))
}

# barplot of the predicted classes
plot_predicted_classes()
```

Reverse bar plots showing the classes of the prediction tree
``` {r}
# make the bar plot reverse
library(plyr)
library(reshape2)

# which experiments are predicted in which class
plot_predicted_classes2 <- function(){
  bar_colors <- c(rgb(1,1,1), 
                  rgb(0.8,0.8,0.8),
                  rgb(1,0,0, 0.5),
                  rgb(0,0,1, 0.5))
  # single
  df <- data.frame(exp=rep(BDLsamples$time, Nsingle), 
                 pre=as.vector(t(round(pred.single, digits=1))) )
  tmp <- count(df, c("exp", "pre"))
  tab.single <- acast(tmp, exp~pre, value.var="freq", fill=0)
  # double
  df <- data.frame(exp=rep(BDLsamples$time, Ndouble), 
                 pre=as.vector(t(round(pred.double, digits=1))) )
  tmp <- count(df, c("exp", "pre"))
  tab.double <- acast(tmp, exp~pre, value.var="freq", fill=0)
  # best gene
  df <- data.frame(exp=rep(BDLsamples$time, 1), 
                 pre=as.vector(t(round(pred.gene.best, digits=1))) )
  tmp <- count(df, c("exp", "pre"))
  tab.gene.best <- acast(tmp, exp~pre, value.var="freq", fill=0)
  # mean cluster
  df <- data.frame(exp=rep(BDLsamples$time, 1), 
                 pre=as.vector(t(round(pred.mean, digits=1))) )
  tmp <- count(df, c("exp", "pre"))
  tab.mean <- acast(tmp, exp~pre, value.var="freq", fill=0)  

  par(mfrow=c(2,3))  
  for (k in 1:length(node_classes)){
    # combined table (normalized within each class)
    tab <- rbind(tab.single[,k]/sum(tab.single[,k]),
                 tab.double[,k]/sum(tab.double[,k]), 
                 tab.gene.best[,k]/sum(tab.gene.best[,k]),
                 tab.mean[,k]/sum(tab.mean[,k]))

    # create the bar plot
    name <- sprintf("Predicted: %sh", node_classes[k])
    barplot(tab, beside=TRUE,
            main=name, 
            xlab="time after BDL [h]", ylab="fraction",
            ylim=c(0,1), col=bar_colors) 
    if (k==1){
      legend("topright", legend=c("single factors", "double factors", 
                                  "best gene", "mean cluster"), 
             col=bar_colors,
             bty="n", cex=1.0, pch=15)
    }
  }
  par(mfrow=c(1,1))
}

plot_predicted_classes2()

# barplot to file
pdf(file.path(resultsPath, "decision_tree", "predicted_classes2.pdf"), 
    width=12, height=7.5, pointsize=14)
plot_predicted_classes2()
invisible(dev.off())
```
