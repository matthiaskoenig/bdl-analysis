---
title: "Statistical Analysis of Pathobiochemical Signatures in Bile Duct Ligated Mice"
author: '[Matthias Koenig](http://www.charite.de/sysbio/people/koenig) (`r Sys.Date()`)'
output:
  html_document: default
  pdf_document:
    fig_caption: yes
    keep_tex: yes
    toc: yes
  word_document: default
header-includes: \usepackage{graphicx}
---
<!--
%\VignetteEngine{knitr}
%\VignetteIndexEntry{BDL raw data processing}
-->
```{r, options, echo=FALSE}
# set knitr option
# knitr::opts_chunk$set()
knitr::knit_hooks$set(htmlcap = function(before, options, envir) {
  if(!before) {
    paste('<p class="caption">',options$htmlcap,"</p>",sep="")
    }
})
```


# Introduction

This document contains the statistical analysis for the publication *Pathobiochemical signatures of cholestatic liver disease in bile duct ligated mice*.

To better understand the cascade of histological and biochemical alterations after bile duct ligation (BDL), a comprehensive data set of serum markers, histological parameters and transcript profiles was compiled at 8 time points after BDL in mice, comprising different stages of the disease. The analysed data set consists of $N_{r}=5$ repeats ($N_{r}=3$ for the measured antibodies) for $N_{t}=8$ time points denoted by $t_1,..., t_n$ consisting of a total $N_{f}=154$ measured factors (Fluidigm gene expression, antibodies, serum markers, histological measurements).

The main steps of the analysis comprise

* **Explorative data anaysis** and data quality control
* **Dimension reduction via ANOVA**
* **Correlation analysis** based on time course correlation measure
* **Hierarchical clustering** 
* **Decision trees** for prediction

The complete data set, source code and documentation of this analysis is available from  
https://github.com/matthiaskoenig/bdl-analysis .

The following naming conventions are used

* **factor** : one of the measured quantities over time, i.e. either 
    + gene expression of a single gene (e.g. Actb); 
    + one of the biomarkers (e.g. ALT, albumin, bilirubin)
    + one of the histological markers (e.g. BrdU-positive Kupffer cells)
    + one of the antibodies (e.g. CTGF, S100A4)
* **time point** : a single value from the measured time points 0h (control), 6h, 12h, 18h, 30h, 2d, 5d, 14d
* **sample** : one of the $N_{t}*N_{r}=40$ mice, i.e. a one of the repeats for a given time point

# Explorative data analysis
## Data import
In a first step the processed data set is loaded from the `data` folder. The data consists of the time course data for all factors (`BDLdata`), additional information for the factors (`BDLfactors`), the sample definition, i.e. the assignment of sample ids to respective time point and repeat (`BDLsamples`), and a mapping of the Fluidigm (gene) probe ids to UniProt identifiers (`BDLprobes`).  
No other data sets are used in this analysis.

```{r BDLdata, eval=TRUE, strip.white=TRUE}
suppressPackageStartupMessages(library(calibrate))
suppressPackageStartupMessages(library(BDLanalysis))
suppressPackageStartupMessages(library(pander))
# path definition
baseLoc <- system.file(package="BDLanalysis")
extPath <- file.path(baseLoc, "extdata")
resultsPath <- "/home/mkoenig/git/bdl-analysis/results"
# load data
data(BDLdata)
data(BDLsamples)
data(BDLfactors)
data(BDLprobes)
# counters
Nr <- 5  # repeats
Nt <- length(levels(BDLsamples$time_fac))  # time points
```

In addition to the single measurement data for the factors, the mean data averaged over the $N_{r}$ repeats is used in parts of the correlation analysis. The mean factor data is calculated once at the beginning via
```{r BDLmean}
BDLmean <- bdl_mean_data(BDLdata, BDLsamples)
BDLmean.time <- as.numeric(levels(as.factor(BDLsamples$time)))
```

In total `r ncol(BDLdata)` factors were measured in the BDL study falling in the categories: `r levels(BDLfactors$ftype)`.  
The majority of factors belongs to the 3 fluidigm chips with 47 probes per chip.

An overview of the number of factors per category is given in the following table
```{r factors}
cat_table <- as.data.frame(table(BDLfactors$ftype))
colnames(cat_table) <- c("Category", "Freq")
set.caption(sub(".", " ", "Factors per category", fixed = TRUE))
pander(cat_table)
```

## Data visualization
### Time course of single factors
In a first step overview plots of the raw and mean data for all individual factors are generated. These are available in the `resultsPath/factors` folder

TODO: fix the plots if there are NA in the data set, like in S100A4

```{r factorPlots, eval=FALSE}
# Visualize all factors
plot_all_factors(path=resultsPath)
```

One example of a single factor plot is depicted below, here for the factor `bilirubin`.
``` {r bilirubin, htmlcap='<b>Figure Single factor</b>: Plot of the raw time course data for a single factor, here for bilirubin. On the left the data is plotted against the time [h], on the right against the different time classes. Individual data points are depecticed in blue with the respective sample number shown next to the data points. The mean averaged of the repeats per time point are depicted in red. Box-and-whisker plots were added with default R parameters of boxwex=0.8, staplewex=0.5, outwex=0.5.'}
plot_single_factor('bilirubin', path=NULL)
```

### Time course of all factors (Heatmap)
In a next step the heatmap of the full data set was generated, i.e. of all time points and repeats. This provides a first overview over the complete data set. Rows correspond to the individual factors (factor order corresponding to the original data set: `GE_ADME`, `GE_Cytokines`, `GE_Fibrosis`, `Biochemistry`, `Histology`, `Antibodies`). Columns correspond to the 40 samples with 5 subsequent samples belonging to one of the 8 time points (with from left to right: `r levels(BDLsamples$time_fac)`). The data is row scaled, i.e. every individual factor is scaled to have mean zero and standard deviation one.
```{r heatmap}
suppressPackageStartupMessages(library("gplots"))
colors <- HeatmapColors() 
dtmp <- BDLdata

# create better row names
rownames(dtmp) <- paste(rownames(BDLsamples), BDLsamples$time_fac, sep=" ")
# create vectors for the horizontal and vertical lines
v_lines <- ((1:8)*5+0.5)
f_types <- c("Antibodies", "Histology", "Biochemistry", "GE_Fibrosis", "GE_Cytokines", "GE_ADME")
f_table <- table(BDLfactors$ftype)
h_lines <- 0.5 + cumsum(f_table[f_types])
# colors for the different data types
library("RColorBrewer")
col2 <- HeatmapColors()
# define colors for type of experimentell data, i.e. factor groups
colorset <- brewer.pal(length(f_types), "Set2")
color.map <- function(factor_id) {return(colorset[ which(f_types==BDLfactors$ftype[which(BDLfactors$id==factor_id)]) ])}
factorColors <- unlist(lapply(BDLfactors$id, color.map))
```

``` {r heatmap2, fig.width=10, fig.height=10, error=TRUE, htmlcap='<b>Figure All factors </b>: Heatmap of the complete data set, i.e. all factors and repeats over time. The data is row scaled, i.e. every individual factor is scaled to have mean zero and standard deviation one, with positive Z-score in blue, negative Z-score in red. The row order is according to the the factor categories, the order within the fluidigm chips according to the order of the probes on the chip. the respective categories are depicted on the left.'}
heatmap.2(t(as.matrix(dtmp)), col=colors(100), scale="row", dendrogram="none", Rowv=NULL, Colv=NULL,
          key=TRUE, trace="none", cexRow=0.5, keysize=0.8, density.info="none",
          RowSideColors=factorColors,
          add.expr=abline(v=v_lines, h=h_lines, col="black", lwd=0.5),
          main="Heatmap of BDL time course data")
          # xlab="sample", ylab="factor")
legend("left",      # location of the legend on the heatmap plot
    inset=c(-0.03,0),
    legend = rev(f_types), # category labels
    col = rev(colorset),  # color key
    lty= 1,             # line style
    lwd = 10,            # line width
    cex = 0.7,
    bty="n"
)
```
**Results**: Various patterns are visible in the plotted raw data:

* **Two main classes of response can be observed**. One class with an increase in the early phase up to 6h after BDL (many of the ADME genes fall into this class) and a second class increasing in the later stage after 2-5 days after BDL. Many of the genes on the Cytokines and Fibrosis  chips as well as some of the biochemical, histological and antibody fall in this second class.
* **The individual animals show heterogeneous responses to BDL**. Within one time point the 5 repeats can show very different patterns. For instance at time 6h after BDL 3/5 of the mice show a marked increase in the ADME genes, whereas 2/5 do not show such a marked increase. Another example is the mice sample 27 at time 2d, with a high increase in the genes on the Fibrosis chip, which is not observed in the other 4 samples at time 2d.

## Actb quality control
Actb (Actin, cytoplasmic 1) probes were included on all Fluidigm chips (`GE_ADME`, `GE_Cytokines`, `GE_Fibrosis`) and not used in the normalization of the gene expression data. Hence, ActB can serve as quality control for the technical reproducibility of the Fluidigm chips. If the data is reproducible between chips the pairwise correlation between all individual Actb measurements should have high correlation coefficients close to 1. Plotting the data of the Actb measurements of two chips against each other should lie on a straight line
```{r CheckActB}
# Actb control figure
plot_actb_control <- function(path=NULL){
  if (!is.null(path)){
    png(filename=file.path(path, "Actb_control.png"), width=1600, height=800, res=200)  
  }
  par(mfrow=c(2,3))
  plot_single("Actb")
  plot_single("Actb.x")
  plot_single("Actb.y")
  plot_cor_pair("Actb", "Actb.x", single_plots=FALSE)
  plot_cor_pair("Actb", "Actb.y", single_plots=FALSE)
  plot_cor_pair("Actb.x", "Actb.y", single_plots=FALSE)
  par(mfrow=c(1,1))
  if (!is.null(path)){
    invisible(dev.off())
  }
}
plot_actb_control(path=resultsPath)

# calculate Spearman and Pearson correlation coefficients on N=8*5=40 data points 
actb.spearman <- cor(data.frame(Actb=BDLdata$Actb, 
                                Actb.x=BDLdata$Actb.x, 
                                Actb.y=BDLdata$Actb.y), method="spearman")
actb.pearson <- cor(data.frame(Actb=BDLdata$Actb, 
                               Actb.x=BDLdata$Actb.x, 
                               Actb.y=BDLdata$Actb.y), method="pearson")

# table of correlation coefficients
set.caption(sub(".", " ", "Spearman correlation of Actb controls", fixed = TRUE))
pander(round(actb.spearman, digits=3))
set.caption(sub(".", " ", "Pearson correlation of Actb controls", fixed = TRUE))
pander(round(actb.pearson, digits=3))
```

``` {r fig.width=10, fig.height=7,  htmlcap='<b>Figure Actb control</b>: Correlation plot of the Actb probes from the 3 Fluidigm chips: Actb (fibrosis), Actb.x (ADME), Actb.y (Cytokines). The top row shows the individual time courses, the bottom row the pair wise plot of individual data points.'}
plot_actb_control(path=NULL)
```
**Results**: The Actb Fluidigm gene expression measurements are highly reproducible for the measured chips,  with Spearman as well as Pearson correlation coefficients all > 0.9 for pairwise Actb comparison.

# Dimension reduction via ANOVA
## ANOVA for single factor
A one-way analysis of variance (ANOVA) was applied to reduce the factors to the subset showing significant ($p_{adjusted}< 0.05$) changes during the time course. In its simplest form, ANOVA provides a statistical test of whether or not the means of several groups are equal, and therefore generalizes the t-test to more than two groups, with the groups being the sampled time points. The Holm's procedure was used to correct the p-values for any artificial p-value inflation due to multiple testing. 

For every of the individual factors in the BDL data set an ANOVA was calculated. Dimension reduction of the BDL data set was than performed by filtering out factors which did not significantly changing over time.

The `BDLdata` data set is reshaped into matrix format for the ANOVA calculation, with time points in rows and repeats as columns for every factor. 
```{r BDLmatrix}
BDLmatrices <- bdl_matrix_data(BDLdata, BDLsamples)
```

The following shows the ANOVA calculation for a single factor, here for `bilirubin`. 
``` {r singleanova}
  # example ANOVA for one factor
  mat.anova <- t(BDLmatrices[['bilirubin']])
  colnames(mat.anova) <- levels(BDLsamples$time_fac)
  
  # concatenate the data rows of df1 into a single vector r .
  r = c(t(as.matrix(mat.anova)))  # response data 
  
  # assign new variables for the treatment levels and number of observations.
  f = levels(BDLsamples$time_fac)   # treatment levels 
  k = 8                          # number of treatment levels 
  n = 5                          # observations per treatment 
  
  # create a vector of treatment factors that corresponds to each element of r in step 3 with the gl function.
  tm <- gl(k, 1, n*k, factor(f))   # matching treatments 
  
  # apply the function aov to a formula that describes the response r by the treatment factor tm.
  # fit an analysis of variance model
  av <- aov(r ~ tm) 
  
  # print out the ANOVA table with the summary function. 
  summary(av)
  # print the corresponding p-value
  p.value <- summary(av)[[1]][["Pr(>F)"]][[1]]

  # show data matrix
  print(mat.anova)
```

## ANOVA for all factors
Analog to the single factor ANOVA, the ANOVA is performed on all the factors. Hereby, a multitude of tests are performed, namely an ANOVA for every single factor. Consequently, the reported p-values of the ANOVA have to be adjusted via multiple testing procedures. Using the p.adjust function which given a set of p-values, returns p-values adjusted using one of several methods. The Bonferroni, Holm, Hochberg, Hommel are designed to give strong control of the family-wise error rate. We used the Holm's method for adjustment (*Holm, S. (1979). A simple sequentially rejective multiple test procedure. Scandinavian Journal of Statistics 6, 65-70.*).


```{r ANOVA}
# Calculation of ANOVA for all factors
df.anova <- all_factor_anova()
df.anova$sig <- sapply(df.anova$p.value, significant_code)  # add significant codes

df.anova$p.holm <- p.adjust(df.anova$p.value, method ="holm" , n = length(df.anova$p.value))
df.anova$sig.holm <- sapply(df.anova$p.holm, significant_code) 

# order factors by adjusted p-values
df.anova.ordered <- df.anova[with(df.anova, order(p.holm)), ]
df.anova.ordered

# save results
write.table(df.anova.ordered, file=file.path(resultsPath, 'BDLanova.csv'), sep="\t", quote=FALSE)
BDLanova <- df.anova
save(df.anova, file=file.path(resultsPath, "BDLanova.Rdata"))
```

## Filter factors
The factors are filtered based on the respective acceptance level, with the cutoff for the adjusted p-value being $p_{accept}$, i.e. all factors with a ANOVA with $p_{adjusted} \ge p_{accept}$ are filtered out. The filtered raw data is available as `BDLdata.fil`, the filtered mean data set as `BDLmean.fil`. All subsequent analyses are performed on the filtered data set, which is depicted in the following heatmap
``` {r filter}
p.accept = 0.05  # acceptance level
idx.accept = (df.anova$p.holm < p.accept)  # accepted subset
# subset of filtered data
BDLdata.fil <- BDLdata[, idx.accept]
BDLmean.fil <- BDLdata[, idx.accept]

# accepted
table(df.anova$p.holm<p.accept)  # 64 rejected / 90 accepted (adjusted)

# which factors were accepted in the various categories
fil_tab <- data.frame(
  table(BDLfactors$ftype[idx.accept]),
  table(BDLfactors$ftype),
  round(table(BDLfactors$ftype[idx.accept])/table(BDLfactors$ftype), 2)
)
fil_tab <- fil_tab[, c('Var1', 'Freq', 'Freq.1', 'Freq.2')]
names(fil_tab) <- c('Category', 'Accepted', 'All', 'Percent')
fil_tab

```
Almost all `Cytokines` genes are retained in the data set whereas many of the `ADME` and `Fibrosis` genes are filtered.

### Heatmap of filtered time course data
The heatmap of the filtered raw data is depicted below
``` {r heatmapFiltered}
# prepare data with row names
dtmp <- BDLdata.fil
rownames(dtmp) <- paste(rownames(BDLsamples), BDLsamples$time_fac, sep=" ")
# create vectors for the horizontal and vertical lines
v_lines <- ((1:8)*5+0.5)
f_types <- c("Antibodies", "Histology", "Biochemistry", "GE_Fibrosis", "GE_Cytokines", "GE_ADME")

# define colors for categories
colorset <- brewer.pal(length(f_types), "Set2")
color.map <- function(factor_id) {return(colorset[ which(f_types==BDLfactors$ftype[which(BDLfactors$id==factor_id)]) ])}
factorColors <- unlist(lapply(colnames(BDLdata.fil), color.map))
```

``` {r heatmap_filtered, fig.width=10, fig.height=10, error=TRUE, htmlcap='<b>Figure Heatmap ANOVA:</b> Plot of ANOVA filtered data set.'}
heatmap.2(t(as.matrix(dtmp)), col=colors(100), scale="row", dendrogram="none", Rowv=NULL, Colv=NULL,
          key=TRUE, trace="none", cexRow=0.5, keysize=0.8, density.info="none",
          RowSideColors=factorColors,
          add.expr=abline(v=v_lines, col="black", lwd=0.5),
          main="Heatmap of BDL time course data")
          # xlab="sample", ylab="factor")
legend("left",      # location of the legend on the heatmap plot
    inset=c(-0.03,0),
    legend = rev(f_types), # category labels
    col = rev(colorset),  # color key
    lty= 1,             # line style
    lwd = 10,            # line width
    cex = 0.7,
    bty="n"
)
```


# Correlation analysis
For the correlation analysis between factors and the subsequent cluster analysis a correlation measure for time series data {Son2008} in combination with Complete-Linkage hierarchical clustering was used. This combination of methods provided the best enrichments on gene-expression time-series in a recent comparisons of methods {Jaskowiak2014, Jaskowiak2013} testing various correlation measures and clustering algorithms. 

The calculation of correlation coefficients between factors i and j ($i,j=1, ...,N_p$) was performed using the slightly modified correlation coefficient based similarity measure developed for clustering of time-course data ($Y_{i,j}^{S2}$ and $Y_{i,j}^{R2}$) {Son2008}. $Y_{i,j}^{S2}$ and $Y_{i,j}^{R2}$ are linear combinations of (i) a classical correlation part based on Spearman correlation $S_{i,j}^{*}$ in case of $Y_{i,j}^{S2}$ or Pearson $R_{i,j}^{*}$ in case of $Y_{i,j}^{R2}$, (ii) a component $A_{i,j}^{*}$ accounting for the similarity in changes between two time courses, (iii) a component $M_{i,j}^{*}$ comparing the location of minimum and maximum values of the time course (see {Son2008} for definitions)
$$Y_{i,j}^{S2} = w_1 S_{i,j}^{*} + w_2 A_{i,j}^{*} + w_3 M_{i,j}^{*}$$
$$Y_{i,j}^{R2} = w_1 R_{i,j}^{*} + w_2 A_{i,j}^{*} + w_3 M_{i,j}^{*}$$
$R_{i,j}^{*}$ and $S_{i,j}^{*}$ are hereby calculated on the individual data points for the factors i and j, $A_{i,j}^{*}$ and $M_{i,j}^{*}$ on the mean time courses averaged over the $N_r$ repeated measurements. Throughout the analysis the following weights were used $w_1=0.5$, $w_2=0.3$, $w_3=0.2$.

In the calculation of the change component we used a Spearman correlation based measure ($A^{**}$) instead of the originally proposed Pearson measure ($A^{*}$) resulting in the correlation scores $Y_{i,j}^{S3}$ and $Y_{i,j}^{S3}$
$$Y_{i,j}^{S3} = w_1 S_{i,j}^{*} + w_2 A_{i,j}^{**} + w_3 M_{i,j}^{*}$$
$$Y_{i,j}^{R3} = w_1 R_{i,j}^{*} + w_2 A_{i,j}^{**} + w_3 M_{i,j}^{*}$$
Herein, $A_{i,j}^{**}$ calculates the correlation of changes between factors i and j based on Spearman correlation analog $A_{i,j}^{*}$ as
$$A_{i,j}^{**}=(S(d_i,d_j)+1)/2$$
$$A_{i,j}^{*}=(R(d_i,d_j)+1)/2$$
The reason for this adaption was that initial analysis showed a strong dependency of the change components on outliers.

All calculated correlation scores $Y^{S}$ and $Y^{R}$ are transformed from [0, 1] to [-1, 1] via
$$Y_{norm}^{S} = 2(Y^{S}-0.5)$$
$$Y_{norm}^{R} = 2(Y^{R}-0.5)$$

In addition to the used $Y^{S}$ and $Y^{R}$ correlation scores Pearson (R) and Spearman (S) correlations were calculated for comparison. 

## Pearson & Spearman correlation
In a first step Pearson $R_{i,j}$ and Spearman $S_{i,j}$ correlation was calculated for the filtered factors. 
Heatmaps of the resulting correlation matrices are depicted below with reordering of factors based on hierarchical clustering with Complete Linkage.

``` {r correlation}
# correlation matrix
suppressPackageStartupMessages(require(corrplot))
cor.pearson <- cor(BDLdata.fil, method="pearson", use="pairwise.complete.obs")
cor.spearman <- cor(BDLdata.fil, method="spearman", use="pairwise.complete.obs")

# Helper function for creating correlation plot and saving to results folder.
# TODO: update the corrplot to use the heatmap.2
f_corrplot <- function(name, data, order, folder="../results",
                       width=1600, height=1600, res=200){
  # TODO: create the plot using the heatmap.2 with color based on histopathology
  fname <- sprintf("%s_%s.png", name, order)
  col2 <- HeatmapColors()
  if (!is.null(folder)){
    png(filename=file.path(folder, "correlation", fname), width=width, height=height, res=res)  
  }
  corrplot(data, order=order, hclust.method="complete", method="color", type="full", 
           tl.cex=0.3, tl.col="black", col=col2(10))
  if (!is.null(folder)){
    invisible(dev.off())  
  }
}

# Spearman
f_corrplot("cor.spearman", data=cor.spearman, order="original", folder=resultsPath)
f_corrplot("cor.spearman", data=cor.spearman, order="hclust", folder=resultsPath)
# Pearson
f_corrplot("cor.pearson", data=cor.pearson, order="original", folder=resultsPath)
f_corrplot("cor.pearson", data=cor.pearson, order="hclust", folder=resultsPath)
```

```{r}
# Pearson correlation with hierarchical clustering
f_corrplot("cor.pearson", data=cor.pearson, order="hclust", folder=NULL)
```

```{r}
# Spearman correlation with hierarchical clustering
f_corrplot("cor.spearman", data=cor.spearman, order="hclust", folder=NULL)
```

The pearson correlation is highly sensitive to outliers (correlation between factors, but also in the changes between time points). 


## YS & YR correlation
Here, the time-course based correlation measurements $Y^{S1}$, $Y^{S2}$, $Y^{S3}$, $Y^{R1}$, $Y^{R2}$ and $Y^{R3}$ are calculated for the factors in the filtered BDL data set. To create the correlation matrix all pairwise correlations between all factors are calculated.

``` {r ys1_yr1}
# calculate ys1, yr1 on mean data, i.e. correlation part (S*), slope part (A) and min/max part (M) are all calculated on the mean data of all repeats.
# w <- list(w1=0.5, w2=0.25, w3=0.25)
w <- list(w1=0.5, w2=0.3, w3=0.2)

# calculate the YSR component matrices on the filtered data set (A, A*, A**, M, M*)
# all components are calculated on the mean data
ysr.res <- ysr.matrices(BDLmean.fil, BDLmean.time, use="pairwise.complete.obs")

# Pearson & spearman correlation on individual data points
cor.S_star <- ( cor(BDLdata.fil, method="spearman", use="pairwise.complete.obs") + 1 )/2
cor.R_star <- ( cor(BDLdata.fil, method="pearson", use="pairwise.complete.obs") + 1 )/2

# Calculate the ys(1,2,3) and yr(1,2,3) with single time point correlation for
# Spearman, respectively Pearson (instead of mean)
# Takes the individual correlation, slope and min/max components for the respective
# score parts and weights with the provided weighting factors (w1,w2,w3)
cor.ys1.raw <- w$w1*cor.S_star + w$w2*ysr.res$A       + w$w3*ysr.res$M
cor.ys2.raw <- w$w1*cor.S_star + w$w2*ysr.res$A_star  + w$w3*ysr.res$M_star
cor.yr1.raw <- w$w1*cor.R_star + w$w2*ysr.res$A       + w$w3*ysr.res$M
cor.yr2.raw <- w$w1*cor.R_star + w$w2*ysr.res$A_star  + w$w3*ysr.res$M_star
# extended
cor.ys3.raw <- w$w1*cor.S_star + w$w2*ysr.res$A_star2 + w$w3*ysr.res$M_star
cor.yr3.raw <- w$w1*cor.R_star + w$w2*ysr.res$A_star2 + w$w3*ysr.res$M_star 

# scaling of correlation coefficient in interval [-1,1]
cor.ys1 <- 2*(cor.ys1.raw-0.5)
cor.ys2 <- 2*(cor.ys2.raw-0.5)
cor.ys3 <- 2*(cor.ys3.raw-0.5)
cor.yr1 <- 2*(cor.yr1.raw-0.5)
cor.yr2 <- 2*(cor.yr2.raw-0.5)
cor.yr3 <- 2*(cor.yr3.raw-0.5)

# plot subset for further analysis
f_corrplot("cor.ys3", data=cor.ys2, order="hclust", folder=NULL)

# create correlation plot on disk
f_corrplot("cor.ys1", data=cor.ys1, order="hclust", folder=resultsPath)
f_corrplot("cor.ys2", data=cor.ys2, order="hclust", folder=resultsPath)
f_corrplot("cor.ys3", data=cor.ys3, order="hclust", folder=resultsPath)
f_corrplot("cor.yr1", data=cor.yr1, order="hclust", folder=resultsPath)
f_corrplot("cor.yr2", data=cor.yr2, order="hclust", folder=resultsPath)
f_corrplot("cor.yr3", data=cor.yr3, order="hclust", folder=resultsPath)
```

Full correlation matrix with hierarchical clustering based on complete linkage
``` {r correlation_factors}

```


Plot correlation matrix for histological, biochemical and antibody markers
``` {r correlation_histological}
# correlation subset for non RNA factors which were accepted by ANOVA
hist_facs <- BDLfactors$id[idx.accept & (BDLfactors$ftype %in% c("Antibodies", "Biochemistry", "Histology"))]
hist_facs
# get the indices of these factors in the correlation matrix
hist_idx <- rep(NA, length(hist_facs))
for (k in 1:length(hist_facs)){
  hist_idx[k] <- which(colnames(cor.ys3) == hist_facs[k])
}
hist_data <-  cor.ys3[hist_idx, ] # rows of correlation matrix for the factors

# filter out all the columns where there is no correlation value >=0.7 or <=-0.7,
# i.e. only factors are retained with a absolute correlation coeffient above the
# threshold
cor.cutoff = 0.6
col.accept <- rep(NA, ncol(hist_data))
for (k in 1:ncol(hist_data)){
  col.accept[k] <- any(hist_data[,k]>=cor.cutoff) | any(hist_data[,k]<=-cor.cutoff)
}
# how many factors are filtered out based on correlation threshold
table(col.accept)

# plot the subset of the correlation matrix to file
plot_hist_corr <- function(){
  corrplot(hist_data[, col.accept], method="circle", type="full", 
           tl.cex=0.7, tl.col="black", col=col2(10))  
}
# plot to file
png(filename=file.path(resultsPath, "correlation", "histological_correlation.png"), width=3000, height=1000, res=300)  
plot_hist_corr()
invisible(dev.off())
# plot in report
plot_hist_corr()
# alternative plot
corrplot(hist_data[, col.accept], method="pie", type="full", 
           tl.cex=0.5, tl.col="black", col=col2(10))
```


# Hierarchical clustering
Based on the calculated correlation matrices between factors hierarchical clustering is performed on the correlation matrices using complete linkage (hclust).

The next step of dimension reduction is clustering of the correlation matrix. This groups the factors into sets with the correlation within sets being larger than between sets. This effectivly finds groups of factors which have similar time courses.

The hclust function in R was used for clustering with complete linkage method for hierarchical clustering. This particular clustering method defines the cluster distance between two clusters to be the maximum distance between their individual components.

An overview of the clusters is given in the `results/cluster` folder.

``` {r}
# Plot the clusters
suppressPackageStartupMessages(require('matrixStats'))

# normalize the individual factors
# centralize and normalize columns so that the the individual factors are comparable
# This has to be performed on new data.
f_normalize_centering <- function(a, min.a, max.a, mean.a){
  res <- (a - mean.a)/(max.a - min.a)
}

# normalization constants for all factors
# necessary to store for additional predictions: if new data becomes available
# the data points have to be measured analog to here
factor.norm <- data.frame(min=apply(BDLdata, 2, min, na.rm=TRUE), 
                            max=apply(BDLdata, 2, max, na.rm=TRUE),
                            mean=apply(BDLdata, 2, mean, na.rm=TRUE))

normalize_BDLdata <- function(data, factor.norm){
  dnorm <- data
  for (name in colnames(data)){
    dnorm[, name] <- f_normalize_centering(a=dnorm[, name], 
                                           min.a=factor.norm[name, "min"],
                                           max.a=factor.norm[name, "max"],
                                           mean.a=factor.norm[name, "mean"]) 
  }
  return(dnorm)
}

# Normalize the full data set
BDLdata.norm <- normalize_BDLdata(data=BDLdata, factor.norm=factor.norm)
# Calculate the mean of the normalized data set
BDLmean.norm <- bdl_mean_data(BDLdata.norm, BDLsamples)
```

``` {r}
# plot of mean clusters
plot_clusters <- function(method, folder=NULL){  
  # create the figure
  if (!is.null(folder)){
    fname <- sprintf("%s_cluster_overview.png", method)
    path <- file.path(folder, 'cluster', fname)
    png(filename=path, width=1600, height=1600, res=200)
  }
  # par(mfrow=c(ceiling(sqrt(Ngroups)),ceiling(sqrt(Ngroups))))
  par(mfrow=c(2,3))
  steps <- 1:Nt  # time points
    for (k in 1:Ngroups){
      g <- groups.hc.order[groups.hc.order==k]
      N <- ceiling(sqrt(length(g)))
    
      # normalized mean data (normalized within factor)
      dgroup <- BDLmean.norm[names(g)]
    
      # mean and sd for timepoints (i.e. over all factors in the cluster) 
      g.mean <- rowMeans(as.matrix(dgroup), na.rm=TRUE)
      g.sd <- rowSds(as.matrix(dgroup), na.rm=TRUE) 
      
      # plot sd range
      plot(1, type="n", xlab="", ylab="", xlim=c(1, Nt), ylim=c(-1, 1), main=sprintf("%s : Cluster %s (N=%s)", method, k, ncol(dgroup)))
      polygon(c(steps, rev(steps)), c(g.mean+g.sd, rev(g.mean-g.sd)),
              col = rgb(0.5,0.5,0.5,0.5), border = NA)
      
      # individual data
      for (name in names(g)){
        points(steps, dgroup[, name], pch=16, col="black")
        lines(steps, dgroup[, name], col=rgb(0.5,0.5,0.5, 0.8), lwd=1)
      }
      # mean over factors in cluster
      lines(steps, g.mean, col="blue", lwd=2)
    }
    par(mfrow=c(1,1))
  if (!is.null(folder)){
    invisible(dev.off())
  }
}

# plot of individual repeat clusters
plot_clusters2 <- function(method, folder=NULL){  
  # create the figure
  if (!is.null(folder)){
    fname <- sprintf("%s_cluster_overview2.png", method)
    path <- file.path(folder, 'cluster', fname)
    png(filename=path, width=1600, height=1600, res=200)
  }
  par(mfrow=c(2,3))
  
  steps <- 1:Nt # time points
  for (k in 1:Ngroups){
    g <- groups.hc.order[groups.hc.order==k]
    N <- ceiling(sqrt(length(g)))
    # get the normalized individual data
    dgroup <- BDLdata.norm[names(g)]
  
    # mean for all factors for given time point and sample
    g.mean <- rowMeans(as.matrix(dgroup), na.rm=TRUE)
    
    # plot
    plot(1, type="n", xlab="", ylab="", xlim=c(1, Nt), ylim=c(-1, 1), main=sprintf("%s : Cluster %s (N=%s)", method, k, ncol(dgroup)))
    
    # individual data
    for (name in names(g)){
      points(BDLsamples$time_point, dgroup[, name], pch=16, col="black")
      for (kr in 1:Nr){
        lines(BDLsamples$time_point[(1:Nt -1)*Nr+kr], dgroup[(1:Nt -1)*Nr+kr, name], col=rgb(0.5,0.5,0.5, 0.8), lwd=1)
      }
    }
    # mean over factors in cluster
    # points(BDLsamples$time_point, g.mean, col="blue", lwd=1, cex=1)
    for (kr in 1:Nr){
      lines(BDLsamples$time_point[(1:Nt -1)*Nr+kr], g.mean[(1:Nt -1)*Nr+kr], col="blue", lwd=2, cex=1.1)
    }
  }
  par(mfrow=c(1,1))
  if (!is.null(folder)){
    invisible(dev.off())
  }
}

# Plot individual time courses in cluster
plot_clusters_items <- function(folder=NULL){
  for (k in 1:Ngroups){
    if (!is.null(folder)){
      fname <- sprintf("%s_cluster_%s.png", method, k)
      path <- file.path(folder, 'cluster', fname)
      png(filename=path, width=3000, height=3000, res=200)  
    }
    g <- groups.hc.order[groups.hc.order==k]
    N <- ceiling(sqrt(length(g)))
    par(mfrow=c(N,N))
    for (name in names(g)){
      plot_single(name_A=name) 
    }
    par(mfrow=c(1,1))  
    if (!is.null(folder)){
      invisible(dev.off())
    }
  }  
}

correlation_matrix_for_method <- function(method){
   if (identical(method, "ys1")){
    cor.cluster <- cor.ys1  
  }else if (identical(method, "ys2")){
    cor.cluster <- cor.ys2
  }else if (identical(method, "ys3")){
    cor.cluster <- cor.ys3
  }else if (identical(method, "yr1")){
    cor.cluster <- cor.yr1
  }else if (identical(method, "yr2")){
    cor.cluster <- cor.yr2
  }else if (identical(method, "yr3")){
    cor.cluster <- cor.yr3
  }else if (identical(method, "pearson")){
    cor.cluster <- cor.pearson
  }else if (identical(method, "spearman")){
    cor.cluster <- cor.spearman
  }  
  return(cor.cluster)
}

# apply hirarchical clustering based on selected correlation measure
cluster_methods <- c("pearson", "spearman", "ys1", "ys2", "ys3", "yr1", "yr2", "yr3")
Ngroups <-  6

for (method in cluster_methods){
  cor.cluster <- correlation_matrix_for_method(method)
  
  # perform hierarchical clustering and cut into Ngroups clusters.
  hc <- hclust(dist(cor.cluster)) 
  groups <- cutree(hc, k=Ngroups)
  groups.hc.order <- groups[hc$order]
  
  # plot all clusters on disk
  plot_clusters(method=method, folder=resultsPath) 
  plot_clusters2(method=method, folder=resultsPath) 

  # plot individual factors in clusters on disk
  plot_clusters_items(folder=resultsPath)
}
```
All Pearson based correlation scores showed dependencies on data outliers. Consequently, we used $Y^{S3}$ which is based on a Spearman based correlation component $S^{*}$ and change component $A^{**}$.  

``` {r}
# plot ys3 clusters
# TODO: this dependency is not ideal, try to refactor, so that plot_clusters is clean
method = "ys3"
cor.cluster <- correlation_matrix_for_method(method)
  
# perform hierarchical clustering and cut into Ngroups clusters.
hc <- hclust(dist(cor.cluster)) 
groups <- cutree(hc, k=Ngroups)
groups.hc.order <- groups[hc$order]
```

``` {r cluster_mean, fig.width=10, fig.height=8, error=TRUE, htmlcap='<b>Figure Cluster mean:</b> Plot of mean cluster data.'}
plot_clusters(method=method, folder=NULL)
```
``` {r cluster_repeat, fig.width=10, fig.height=8, error=TRUE, htmlcap='<b>Figure Cluster repeat:</b> Plot of repeat cluster data.'}
plot_clusters2(method=method, folder=NULL)
```


Which factors are in which clusters?
``` {r}
# print the clusters 
# TODO: refactor the multiple clustering
cor.cluster <- correlation_matrix_for_method(method="ys3")
hc <- hclust(dist(cor.cluster)) 
groups <- cutree(hc, k=Ngroups)

for (k in 1:Ngroups){
  g <- groups[groups==k]
  cat(sprintf("------------------------------------------\n", k, length(g)))
  cat(sprintf("Cluster %s (N=%s)\n", k, length(g)))
  cat(sprintf("------------------------------------------\n", k, length(g)))
  print(names(g))
}
```

Display the clusters with the heatmap
``` {r heatmap_ys3, fig.width=10, fig.height=10, error=TRUE, htmlcap='<b>Figure Heatmap Cluster:</b> Plot of ANOVA filtered data set.'}

method <- 'ys3'
cor.cluster <- correlation_matrix_for_method(method)

col2 <- HeatmapColors()
hc <- hclust(dist(cor.cluster)) 
# get cluster IDs for the groups
groups <- cutree(hc, k=Ngroups)

# define colors for the Ngroups clusters
colorset <- brewer.pal(Ngroups, "Set1")
color.map <- function(cluster_id) {return(colorset[cluster_id])}
clusterColors <- unlist(lapply(groups, color.map))

# Create heatmap plot
plot_ys3_heatmap <- function(){
  heatmap.2(cor.cluster, col=col2(10), scale="none",
          key=TRUE, symkey=FALSE, trace="none", cexRow=0.8, cexCol=0.8,
          main=method,
          density.info="none", dendrogram="column", Rowv=as.dendrogram(hc), Colv=as.dendrogram(hc), keysize=0.8,
          key.xlab = "ys3 correlation",
          ColSideColors=clusterColors, revC=TRUE,
          sepwidth=c(0.01,0.01),
          sepcolor="black",
          colsep=1:ncol(cor.cluster),
          rowsep=1:nrow(cor.cluster))
  legend("left", legend=paste("c", 1:6, sep=""), col= unlist(lapply(1:6, color.map)), pch=15, bty="n")
}

# plot to file
png(filename=file.path(resultsPath, "correlation", "ys3_correlation.png"), width=2500, height=2500, res=200)  
plot_ys3_heatmap()
invisible(dev.off())

# plot to report
plot_ys3_heatmap()


# plot the cluster legend
plot(NA, NA, type="n")




```


Overview of the individual time courses in the clusters
```{r, eval=FALSE}
# TODO
```






# Decision Trees
For the prediction of the phase/time after BDL from measured factors a decision tree was fitted on the data. The fitted decision tree model is a regression tree with the clusters as predictor variables. This allows to combine the information from multiple factors in the individual clusters, resulting in a predictive model which is not depening on single factor variables, but uses the combined information available from multiple factors. The fitted decision tree is applicable to any subset of factors measured.

The decision tree models are fitted using the packages `rpart` and `rpart.plot` with the 6 clusters as predictor variables in the model.

In a first step the data sets are prepared for model fitting and prediction. The used data sets for model evaluation are the mean cluster data set, i.e. the mean data of all factors averaged for the clusters. The data set of all combinations of single factors from the individual clusters and a data set using 2 randomly sampled factors from every cluster.
``` {r}
require(rpart)
require(rpart.plot)

```{r}
# --- Mean cluster data set ---
# Get the hierarchical clusters for ys3 for the fitting the decision trees.
method <- "ys3"
cor.cluster <- correlation_matrix_for_method(method)
hc <- hclust(dist(cor.cluster)) 
groups <- cutree(hc, k=Ngroups)

# Prepare mean cluster data set (i.e. mean over normalized factors in cluster)
na.vec <- rep(NA, Nt*Nr)
treedata.mean <- data.frame(c1=na.vec, c2=na.vec, c3=na.vec, c4=na.vec, c5=na.vec, c6=na.vec)

# for every sample
for (ks in 1:(Nt*Nr)){
  # create the mean over the cluster
  for (kgroup in 1:Ngroups){
    # get the factors in the cluster
    factors <- names(groups[groups==kgroup])
    treedata.mean[ks, kgroup] <- mean(as.numeric(BDLdata.norm[ks, factors]), na.rm=TRUE)
  }
}

# add class for classification
treedata.mean$class <- BDLsamples$time_fac
# add log transformed time [h] for regression
treedata.mean$regvalue <- log(BDLsamples$time+1)

# mean cluster data set for model fitting
print(treedata.mean)
```

``` {r}
# --- Single factor data set ---
# overview of member count per cluster
group_table <- table(groups)
print(group_table)
# Number of combinations between single members in the clusters
prod(group_table)

# names of all clusters
facs_1 <- names(groups[groups==1])
facs_2 <- names(groups[groups==2])
facs_3 <- names(groups[groups==3])
facs_4 <- names(groups[groups==4])
facs_5 <- names(groups[groups==5])
facs_6 <- names(groups[groups==6])

# create all single factor data
ptm <- proc.time()  # Start timer
treedata.single <- vector("list", prod(group_table))  # list for all combinations
counter = 1
for (n1 in facs_1){
  for (n2 in facs_2){
    for (n3 in facs_3){
      for (n4 in facs_4){
        for (n5 in facs_5){
          for (n6 in facs_6){
            # THIS HAS TO BE FAST (<0.01 s)
            # cat(sprintf("%s, %s, %s, %s, %s, %s\n", n1, n2, n3, n4, n5, n6))
            
            # ptm <- proc.time() # Start the clock!
            
            # get factor data
            tmp <- BDLdata.norm[, c(n1, n2, n3, n4, n5, n6)]
            # add class and regression values
            tmp$class <- BDLsamples$time_fac
            tmp$regvalue <- log(BDLsamples$time+1)
            # add factor fields 
            tmp[, c('c1.id', 'c2.id', 'c3.id', 'c4.id', 'c5.id', 'c6.id')] <- matrix(rep(c(n1, n2, n3, n4, n5, n6), 40), nrow=40, ncol=6, byrow =TRUE)
            colnames(tmp) <- c('c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'class', 'regvalue', 'c1.id', 'c2.id', 'c3.id', 'c4.id', 'c5.id', 'c6.id')
            treedata.single[[counter]] <- tmp
            
            counter = counter + 1 
            print(counter)
            # proc.time() - ptm  # Stop the clock
          }
        }
      }
    }
  }
}
# Stop the clock
proc.time() - ptm      
```

``` {r}
# --- Double factor data set ---
set.seed(123456)
Nsample <- 10000
treedata.double <- vector("list", Nsample)  # list for sampled double combinations
for (k in 1:Nsample){
  n1 = sample(facs_1, 2, replace=FALSE)  
  n2 = sample(facs_2, 2, replace=FALSE)  
  n3 = sample(facs_3, 2, replace=FALSE)  
  n4 = sample(facs_4, 2, replace=FALSE)  
  n5 = sample(facs_5, 2, replace=FALSE)  
  n6 = sample(facs_6, 2, replace=FALSE)  
  tmp <- 0.5 * (BDLdata.norm[, c(n1[1], n2[1], n3[1], n4[1], n5[1], n6[1])] 
                + BDLdata.norm[, c(n1[2], n2[2], n3[2], n4[2], n5[2], n6[2])])
  # add class and regression values
  tmp$class <- BDLsamples$time_fac
  tmp$regvalue <- log(BDLsamples$time+1)
  # add factor fields 
  tmp[, c('c1.id', 'c2.id', 'c3.id', 'c4.id', 'c5.id', 'c6.id')] <- matrix(rep(c(paste(n1, collapse="_"), 
                                                                                 paste(n2, collapse="_"),
                                                                                 paste(n3, collapse="_"),
                                                                                 paste(n4, collapse="_"),
                                                                                 paste(n5, collapse="_"), 
                                                                                 paste(n6, collapse="_")), 40), nrow=40, ncol=6, byrow =TRUE)
  colnames(tmp) <- c('c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'class', 'regvalue', 'c1.id', 'c2.id', 'c3.id', 'c4.id', 'c5.id', 'c6.id')
  treedata.double[[k]] <- tmp
  print(k)
}
treedata.double[[1]]
```


## Classification Tree
To demonstrate the model fitting and model evaluation with the test data set a decicison tree is fitted for the class variables.
This does not take the order of the classes into account.
``` {r}
formula.mean = paste("class ~ c1 + c2 + c3 + c4 + c5 + c6")
tree.model <- rpart(formula=formula.mean, data=treedata.mean, method="class", control=rpart.control(minsplit=5)) 
tree.model <- rpart(formula=formula.mean, data=treedata.mean, method="class", control=rpart.control(minsplit=10)) 

# print information of the tree fit
printcp(tree.model)
print(tree.model)
prp(tree.model, type=0, extra=101, yesno=TRUE) # pretty plot

# probability prediction
pred <- predict(tree.model, newdata=treedata.mean, type="prob")
round(pred, digits=2)

# class prediction with data
pred.class <- predict(tree.model, newdata=treedata.mean, type="class")
pred.class

# evaluation of prediction
# http://stats.stackexchange.com/questions/49416/decision-tree-model-evaluation-for-training-set-vs-testing-set-in-r
library(caret)
confusionMatrix(data=pred.class, reference=treedata.mean$class)
```


## Regression Tree
A regression tree with the time after BDL as target variable is fitted using the cluster data as predictors. The time data is log transformed to get aprroximately equidistant intervals between the trainings classes. All trees are fitted with the R package `rpart` implementing algorithms for recursive partitioning for classification following in most details closely Breiman et. al (1984). `rpart` is the open-source implementation of CART.

Breiman L., Friedman J. H., Olshen R. A., and Stone, C. J. (1984) Classification and Regression Trees. Wadsworth. 

The tree is built in a two-step process {Therneau2015}:  First the single variable is found which best
splits the data into two groups (`best' will be dened later). The data is separated based on the split, and then this process is applied
separately to each sub-group,  and so on recursively until the subgroups either reach a minimum size (`minbucket`) or until no improvement can be
made.
The resultant model is, with a certainty, too complex, and the question arises as it does
with all stepwise procedures of when to stop.  The second stage of the procedure consists of
using cross-validation to trim back the full tree. 

An Introduction to Recursive Partitioning Using the RPART Routines
TM. Therneau and E.J. Atkinson
Mayo Foundation (June 29, 2015)

The splitting criterion (for classication this is either the Gini or log-likelihood function), which is used to decide which variable gives the best split for nodes in the regression trees is $SS_{T}-(SS_{L} + SS_{R})$, with $SS_{T} = \sum{(y_{i}-<y>)^2}$ the sum of squares for the node and $SS_{R}$ and $SS_{L}$ the sums of squares for the left and right son. This is equivalent to choosing the split ot maximize the between-groups sum-of-squares in a simple analysis of variance (see {Therneau2015}).

```{r} 
# Transform data to log scale (for comparable time intervals)
log_transform <- function(data){
  log(data+1)
}
# Back transformation
log_transform_back <- function(log_data){
  exp(log_data)-1
}
```

Two important parameters controlling the resulting tree are 

* `minsplit` : The minimum number of observations in a node for which the routine
will even try to compute a split. The default is 20 and is set to 6 in the tree calculation ($N_{r}=5$ repeats per time point).
* `minbucket` : The  minimum  number  of  observations  in  a  terminal  node. This defaults to minsplit/3.

``` {r}
# formula for regression tree
formula.reg = paste("regvalue ~ c1 + c2 + c3 + c4 + c5 + c6")

# full tree (complexity parameter set to -1)
# tree.reg <- rpart(formula=formula.fil.reg, data=treedata.reg, method="anova", control=rpart.control(minsplit=6, minbucket=2, cp=-1))

# fitting the regression tree with the mean cluster data
tree.reg <- rpart(formula=formula.reg, data=treedata.mean, method="anova", control=rpart.control(minsplit=6, minbucket=2))
# overview resulting tree
print(tree.reg)
summary(tree.reg)
printcp(tree.reg)
tree.reg$frame

rsq.rpart(tree.reg) # visualize cross-validation results 

library(caret)
pred.reg <- predict(tree.reg, newdata=treedata.mean, type="vector")
pred.reg
confusionMatrix(data=pred.class, reference=treedata.mean$class)


# create pretty plot of tree
png(filename=file.path(resultsPath, 'decision_tree', "regression_tree.png"), width=2000, height=1000, res=200)  
prp(tree.reg, type=0, extra=101, yesno=TRUE)
invisible(dev.off())

prp(tree.reg, type=0, extra=101, yesno=TRUE)
```

Predict data with the tree, here the time class leaves based on the mean cluster data.
The regression tree predicts log classes, which are back-transformed to time in [h].
So how good is the tree performing on the trainings data set, i.e. the mean cluster data.
``` {r}
# mean cluster predictions
pred.mean.log <- predict(tree.reg, newdata=treedata.mean, type="vector")
# transformation to time in [h]
pred.mean <- log_transform_back( pred.mean.log )
# plot predicted ~ experimentell
plot(BDLsamples$time, pred.mean, pch=15, col=rgb(0,0,1, 0.2), main="Regression Tree:\nPredicted ~ experimentell time",
     xlab="experimentell time [h]", ylab="predicted time [h]")
abline(a=0, b=1, col="grey")
```

Get the ranges of the various classes via the split points on log scale
``` {r}
# classes via predicted classes for cluster data
node_levels <- levels(as.factor(pred.mean))
# These are the predicted classes
node_classes <- round(as.numeric(node_levels), digits=1)
node_classes

# get the intervals of the time classes
node_mean <- as.numeric(levels(as.factor(pred.mean.log)))
node_midpoints <- (node_mean[2:length(node_mean)] + node_mean[1:(length(node_mean)-1)])/2
# minimum of range
node_min <- node_mean
node_min[2:length(node_min)] <- node_midpoints
# maximum of range
node_max <- node_mean
node_max[1:(length(node_min)-1)] <- node_midpoints
# ranges in log scale
node_ranges.log <- data.frame(node_mean, node_min, node_max)

node_ranges <- data.frame(mean=log_transform_back(node_mean), 
                          min=log_transform_back(node_min),
                          max=log_transform_back(node_max))
round(node_ranges, digits=1)
```



TODO: reduce the data sets to the clusters being part of the dataset.

Visualize the pairwise cluster data. The data should be classified/regressed in 
a 6D space, i.e. 6 predictor variables corresponding to the individual clusters.
The projection into 2D can provide first insights in problems of the task and
is useful for the later visualization of the tree decision cuts.
``` {r}
plot(treedata.mean[, 1:6], xlim=c(-0.5, 0.7), ylim=c(-0.5, 0.7), col=treedata.mean$class, pch=19)
```



``` {r}
# single predictions
pred.res <- vector("list", length(treedata.single))
for (k in (1:length(treedata.single))){
  print(k)
  # back conversion
  pred.res[[k]] <- exp(predict(tree.reg, newdata=treedata.single[[k]], method="anova"))-1
}
pred.single <- do.call("rbind", pred.res)


# double predictions
pred.res <- vector("list", length(treedata.double))
for (k in (1:length(treedata.double))){
  print(k)
  pred.res[[k]] <- exp(predict(tree.reg, newdata=treedata.double[[k]], method="anova"))-1
}
pred.double <- do.call("rbind", pred.res)
```


``` {r}
# Barplot for the various time points

# classes via predicted classes for cluster data
node_levels <- levels(as.factor(pred.mean))
# These are the predicted classes
node_classes <- round(as.numeric(node_levels), digits=1)
node_classes

# Plot of the predicted classes with the decision tree
plot_predicted_classes <- function(){
  par(mfrow=c(2,4))
  for (k in 1:Nt){
    # single factor predictions
    data <- as.vector(pred.single[, ((1:Nr)+Nr*(k-1))])
    tab.single <- table(factor(data, levels=node_levels))/length(data)
    
    # two factor predictions
    data <- as.vector(pred.double[, ((1:Nr)+Nr*(k-1))])
    tab.double <- table(factor(data, levels=node_levels))/length(data)
    
    # mean cluster predictions
    data <- as.vector(pred.mean[((1:Nr)+Nr*(k-1))])
    tab.mean <- table(factor(data, levels=node_levels))/length(data)
    
    # combined table
    tab <- rbind(tab.single, tab.double, tab.mean)
    colnames(tab) <- round(as.numeric(colnames(tab)), digits=1)
    
    # create the plot
    name <- sprintf("Time after BDL: %sh", 
                    levels(as.factor(BDLsamples$time))[k])
    barplot(tab[1,], ylim=c(0,1), col=rgb(0.7,0.7,0.7, 1.0),
            main=name, xlab="predicted time class [h]", ylab="fraction of predictions")  
    barplot(tab[2, ], ylim=c(0,1), col=rgb(1,0,0,0.5) , add=TRUE)  
    barplot(tab[3, ], ylim=c(0,1), col=rgb(0,0,1,0.5), add=TRUE)  
    
    if (k==1){
      legend("topright", legend=c("single factor", "double factor", "mean cluster"), 
             col=c(rgb(0.7, 0.7, 0.7, 1),rgb(1, 0, 0, 0.5),rgb(0, 0, 1, 0.5)),
             bty="n", cex=1.0, pch=15)
    }
  }
  par(mfrow=c(1,1))
}

# barplot of the predicted classes
png(filename=file.path(resultsPath, "decision_tree", "predicted_classes.png"), width=1800, height=1000, res=150)  
plot_predicted_classes()
invisible(dev.off())

plot_predicted_classes()
```

```
# Search the best single predictor combinations
# Best predictor minimizes the distance between predicted and actual time




# L2 (euclidian) distance measurement on the log transformed data
log_distance <- function(d1, d2){
  # sums over all the distances of the samples
  sum( (log_transform(d1)-log_transform(d2) )^2 )
}

# distance for all predictions on single factor per cluster
Nsingle <- nrow(pred.single)
dist.single <- rep(NA, Nsingle)
for (k in 1:Nsingle){
  dist.single[k] <- log_distance(pred.single[k,], BDLsamples$time)
}
# distance for predictions on 2 sampled factors per cluster 
Ndouble <- nrow(pred.double)
dist.double <- rep(NA, Ndouble)
for (k in 1:Ndouble){
  dist.double[k] <- log_distance(pred.double[k,], BDLsamples$time)
}
# distance on mean cluster
dist.mean <- log_distance(pred.mean, BDLsamples$time)

# Plot the distributions of all the distances, i.e. log transformed predictions
# vs. the real time class
hist(dist.single, breaks=((0:60)*2.5), freq=FALSE, col=rgb(0.7,0.7,0.7, 1), ylim=c(0, 0.05),
xlab="sum( (log(time.predicted+1)-log(time.exp+1))^2 )", main="Distance between predicted and experimentell time")
hist(dist.double, breaks=((0:60)*2.5), freq=FALSE, col=rgb(1,0,0, 0.5), add=TRUE)
hist(dist.mean, breaks=((0:60)*2.5), freq=FALSE, col=rgb(0,0,1, 0.5), add=TRUE)

legend("topright", legend=c("single factor", "double factor", "mean cluster"), 
           col=c(rgb(0.7, 0.7, 0.7, 1),rgb(1, 0, 0, 0.5),rgb(0, 0, 1, 0.5)),
           bty="n", cex=1.0, pch=15)


# Find the minimal distances
table(sort(dist[dist<15]))

which(dist==min(dist))
# 10721 10722 12063 12064 14747 14748 16089 16090

best.single 

treedata.single[[10721]]
# c1.id   c2.id c3.id  c4.id c5.id c6.id
# Cyp1a2 Cyp24a1   Fn1 S100A4 Il17a   Bad
treedata.single[[10722]]
# Cyp1a2 Cyp24a1   Fn1 S100A4 Il17a  Cdh2
treedata.single[[12063]]
# Cyp1a2 Cyp24a1  GLDH S100A4 Il17a   Bad
treedata.single[[12064]]
# Cyp1a2 Cyp24a1  GLDH S100A4 Il17a  Cdh2
treedata.single[[14747]]
# Cyp1a2 Nr0b2   Fn1 S100A4 Il17a   Bad
treedata.single[[14748]]
# Cyp1a2 Nr0b2   Fn1 S100A4 Il17a   Cdh2
treedata.single[[16089]]
# Cyp1a2 Nr0b2  GLDH S100A4 Il17a   Bad


# TODO: find the best single factors which are only gene expression factors
# -> add to the tree plot


# TODO: create plots of the best factors for prediction (supplement)

```

