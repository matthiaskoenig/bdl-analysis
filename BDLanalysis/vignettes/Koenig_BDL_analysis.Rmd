---
title: "Statistical Analysis of Pathobiochemical Signatures in Bile Duct Ligated Mice"
author: '[Matthias Koenig](http://www.charite.de/sysbio/people/koenig) (`r Sys.Date()`)'
output:
  html_document: default
  pdf_document:
    fig_caption: yes
    keep_tex: yes
    toc: yes
  word_document: default
header-includes: \usepackage{graphicx}
---
<!--
%\VignetteEngine{knitr}
%\VignetteIndexEntry{BDL raw data processing}
-->
```{r, options, echo=FALSE}
# set knitr option
# knitr::opts_chunk$set()
knitr::knit_hooks$set(htmlcap = function(before, options, envir) {
  if(!before) {
    paste('<p class="caption">',options$htmlcap,"</p>",sep="")
    }
})
```


# Introduction

This document contains the statistical analysis for the publication *Pathobiochemical signatures of cholestatic liver disease in bile duct ligated mice*.

To better understand the cascade of histological and biochemical alterations after bile duct ligation (BDL), a comprehensive data set of serum markers, histological parameters and transcript profiles was compiled in mice at 8 time points after BDL, comprising different stages of the disease. 
The analysed data set consists of $N=5$ repeats ($N=3$ for the measured antibodies) for $n=8$ time points denoted by $t_1,..., t_n$ consisting of a total $p=154$ measured factors (Fluidigm gene expression, antibodies, serum markers, histological measurements).


The main steps of the analysis comprise

* **Explorative data anaysis** and quality controls (using ActB)
* **Dimension reduction via ANOVA**
* **Correlation analysis** based on time course correlation measure
* **Hierarchical clustering** 
* **Decision trees** for prediction

The complete data set, source code and documentation are available from  
https://github.com/matthiaskoenig/bdl-analysis .

# Explorative data analysis
## Data import
In a first step the processed data set is loaded from the `data` folder. The data consists of the time course data for all factors (`BDLdata`), the sample definition, i.e. which sample belongs to which time point and repeat (`BDLsamples`), and a mapping of the Fluidigm (gene) probe ids to UniProt identifiers (`BDLprobes`). 

```{r BDLdata, eval=TRUE, strip.white=TRUE}
suppressPackageStartupMessages(library("calibrate"))
suppressPackageStartupMessages(library('BDLanalysis'))
# path definition
baseLoc <- system.file(package="BDLanalysis")
extPath <- file.path(baseLoc, "extdata")
resultsPath <- "/home/mkoenig/git/bdl-analysis/results"
# load data
data(BDLdata)
data(BDLsamples)
data(BDLfactors)
data(BDLprobes)
```

In addition to the single measurement data for the factors, the mean data averaged over the $N=5$ repeats is used in the correlation analysis.
```{r BDLmean}
BDLmean <- bdl_mean_data(BDLdata, BDLsamples)
BDLsd <- bdl_sd_data(BDLdata, BDLsamples)
BDLmean.time <- as.numeric(levels(as.factor(BDLsamples$time)))
```

In total `r ncol(BDLdata)` factors were measured falling in the following categories:
```{r factors}
table(BDLfactors$ftype)
```

## Data visualization
In a first step overview plots of the raw and mean data for all factors were generated. These are available in the `resultsPath/factors` folder
```{r factorPlots, eval=FALSE}
# Visualize single factor
plot_single_factor(name=colnames(BDLdata)[2])
# Visualize all factors
plot_all_factors(path=resultsPath)
```

The example plot for `bilirubin` is shown below. 
``` {r bilirubin, htmlcap='<b>Figure:</b> Example plot of raw time course data for factor bilirubin. The measured time points are not equidistant.'}
plot_single_factor('bilirubin', path =NULL)
```


### Time course heatmap of factors
A heatmap of the full data set, i.e. all time points and repeats, is generated. Rows correspond to the individual factors, with factor order corresponding to the original data set (`GE_ADME`, `GE_Cytokines`, `GE_Fibrosis`, `Biochemistry`, `Histology`, `Antibodies`). Columns correspond to the 40 samples with 5 subsequent samples belonging to one of the 8 time points (with from left to right: `r levels(BDLsamples$time_fac)`).
```{r heatmap}
suppressPackageStartupMessages(library("gplots"))
colors <- HeatmapColors() 
dtmp <- BDLdata

# create better row names
rownames(dtmp) <- paste(rownames(BDLsamples), BDLsamples$time_fac, sep=" ")
# create vectors for the horizontal and vertical lines
v_lines <- ((1:8)*5+0.5)
f_types <- c("Antibodies", "Histology", "Biochemistry", "GE_Fibrosis", "GE_Cytokines", "GE_ADME")
f_table <- table(BDLfactors$ftype)
h_lines <- 0.5 + cumsum(f_table[f_types])
# colors for the different data types
library("RColorBrewer")
col2 <- HeatmapColors()
# define colors for type of experimentell data, i.e. factor groups
colorset <- brewer.pal(length(f_types), "Set2")
color.map <- function(factor_id) {return(colorset[ which(f_types==BDLfactors$ftype[which(BDLfactors$id==factor_id)]) ])}
factorColors <- unlist(lapply(BDLfactors$id, color.map))
```

``` {r heatmap2, fig.width=10, fig.height=10, error=TRUE, fig.cap='Every the matrix is normalized within rows, i.e. every single factor is normalized for single factors to be comparable.'}
heatmap.2(t(as.matrix(dtmp)), col=colors(100), scale="row", dendrogram="none", Rowv=NULL, Colv=NULL,
          key=TRUE, trace="none", cexRow=0.5, keysize=0.8, density.info="none",
          RowSideColors=factorColors,
          add.expr=abline(v=v_lines, h=h_lines, col="black", lwd=0.5),
          main="Heatmap of BDL time course data")
          # xlab="sample", ylab="factor")
legend("left",      # location of the legend on the heatmap plot
    inset=c(-0.03,0),
    legend = rev(f_types), # category labels
    col = rev(colorset),  # color key
    lty= 1,             # line style
    lwd = 10,            # line width
    cex = 0.7,
    bty="n"
)
```



``` {r, eval=FALSE}
# TODO: generate proper heatmap of the raw data+-
library("RColorBrewer")
col2 <- HeatmapColors()
# define colors for the time point groups
Ngroups <- 8 
colorset <- brewer.pal(Ngroups, "Set1")
color.map <- function(cluster_id) {return(colorset[cluster_id])}
clusterColors <- unlist(lapply(groups, color.map))

heatmap.2(cor.cluster, col=col2(100), scale="none",
          key=TRUE, symkey=FALSE, trace="none", cexRow=0.5, cexCol=0.5,
          density.info="none", dendrogram="column", Rowv=as.dendrogram(hc), Colv=as.dendrogram(hc), keysize=0.8,
          ColSideColors=clusterColors, revC=TRUE)
heatmap
abline
```


**Results**: Various time course patterns can be observed in the raw data. With an early increased in many of the ADME genes around 6h and a decrease afterwards. The Cytokines and Fibrosis genes as well as most of the biochemical, histological and antibody markers are increse in the later stage after 2-5 days.


## Actb quality control
The Actb (Actin, cytoplasmic 1) probes were included on all Fluidigm chips (`GE_ADME`, `GE_Cytokines`, `GE_Fibrosis`) and not used for the normalization of the gene expression data. Hence, ActB can be used as quality control for the technical reproducibility of the Fluidigm chips. If the data is reproducible between chips the pairwise correlation between all the individual Actb measurements should have high correlation coefficients close to 1.
```{r CheckActB}
# Actb control figure
plot_actb_control <- function(path=NULL){
  if (!is.null(path)){
    png(filename=file.path(path, "Actb_control.png"), width=1600, height=800, res=200)  
  }
  par(mfrow=c(2,3))
  plot_single("Actb")
  plot_single("Actb.x")
  plot_single("Actb.y")
  plot_cor_pair("Actb", "Actb.x", single_plots=FALSE)
  plot_cor_pair("Actb", "Actb.y", single_plots=FALSE)
  plot_cor_pair("Actb.x", "Actb.y", single_plots=FALSE)
  par(mfrow=c(1,1))
  if (!is.null(path)){
    invisible(dev.off())
  }
}
plot_actb_control(path=resultsPath)

# calculate Spearman and Pearson correlation coefficients on N=8*5=40 data points 
actb.spearman <- cor(data.frame(Actb=BDLdata$Actb, 
                                Actb.x=BDLdata$Actb.x, 
                                Actb.y=BDLdata$Actb.y), method="spearman")
actb.pearson <- cor(data.frame(Actb=BDLdata$Actb, 
                               Actb.x=BDLdata$Actb.x, 
                               Actb.y=BDLdata$Actb.y), method="pearson")
# knitr table
# knitr::kable(actb.spearman, digits=3)
# knitr::kable(actb.pearson, digits=3)
# pander table
suppressPackageStartupMessages(library(pander))
set.caption(sub(".", " ", "Spearman correlation of Actb controls", fixed = TRUE))
pander(round(actb.spearman, digits=3))
set.caption(sub(".", " ", "Pearson correlation of Actb controls", fixed = TRUE))
pander(round(actb.pearson, digits=3))
```

``` {r fig.width=10, fig.height=7}
plot_actb_control(path=NULL)
```
**Results**: The Actb Fluidigm gene expression measurements are highly reproducible for the measured chips,  with Spearman as well as Pearson correlation coefficients all > 0.9 for pairwise Actb comparison.
# Dimension reduction via ANOVA
## ANOVA for single factor
A one-way analysis of variance (ANOVA) was applied to isolate factors showing significant ($p_{adjusted}< 0.05$) up- or down-regulation during the time course, using the Holm's procedure to correct for any artificial p-value inflation. In its simplest form, ANOVA provides a statistical test of whether or not the means of several groups are equal, and therefore generalizes the t-test to more than two groups, with the groups being the sampled time points. For every of the individual factors in the BDL data set an ANOVA was calculated.Dimension reduction of the BDL data set was than performed by filtering out factors which did not significantly changing over time.

An ANOVA is calculated for all factors (N=`r nrow(BDLdata)`). A data.frame with the p-values of the ANOVA is returned which is used for filtering the BDL data set. Significance codes are added for visual inspection.

The `BDLdata` data set of timecourses for the factors is reshaped into matrix format for the ANOVA calculation, with time points in rows and repeats as columns
```{r BDLmatrix}
BDLmatrices <- bdl_matrix_data(BDLdata, BDLsamples)
print(BDLmatrices[[1]])
```

The following shows the ANOVA calculation for a single factor, here for `bilirubin`. In the same manner the ANOVA is calculated for all factors.
``` {r singleanova}
  # example ANOVA for one factor
  mat.anova <- t(BDLmatrices[['bilirubin']])
  colnames(mat.anova) <- levels(BDLsamples$time_fac)
  
  # concatenate the data rows of df1 into a single vector r .
  r = c(t(as.matrix(mat.anova)))  # response data 
  
  # assign new variables for the treatment levels and number of observations.
  f = levels(BDLsamples$time_fac)   # treatment levels 
  k = 8                          # number of treatment levels 
  n = 5                          # observations per treatment 
  
  # create a vector of treatment factors that corresponds to each element of r in step 3 with the gl function.
  tm <- gl(k, 1, n*k, factor(f))   # matching treatments 
  
  # apply the function aov to a formula that describes the response r by the treatment factor tm.
  # fit an analysis of variance model
  av <- aov(r ~ tm) 
  
  # print out the ANOVA table with the summary function. 
  summary(av)
  # print the corresponding p-value
  p.value <- summary(av)[[1]][["Pr(>F)"]][[1]]
```

## Adjust p-values for multiple testing
A multitude of tests were performed, namely an ANOVA for every single factor. Consequently, the reported p-values of the ANOVA have to be adjusted via multiple testing procedures. Using the p.adjust function which given a set of p-values, returns p-values adjusted using one of several methods. The Bonferroni, Holm, Hochberg, Hommel are designed to give strong control of the family-wise error rate. There seems no reason to use the unmodified Bonferroni correction because it is dominated by Holm's method, which is also valid under arbitrary assumptions.

The correction is performed using Holm []

Holm, S. (1979). A simple sequentially rejective multiple test procedure. Scandinavian Journal of Statistics 6, 65-70.
TODO: add proper reference

The ANOVA is now calculated for every single factor
```{r ANOVA}
df.anova <- all_factor_anova()
df.anova$sig <- sapply(df.anova$p.value, significant_code)  # add significant codes

df.anova$p.holm <- p.adjust(df.anova$p.value, method ="holm" , n = length(df.anova$p.value))
df.anova$sig.holm <- sapply(df.anova$p.holm, significant_code) 

# order the results py the adjusted p-values
df.anova.ordered <- df.anova[with(df.anova, order(p.holm)), ]
df.anova.ordered

# save the results
write.table(df.anova.ordered, file=file.path(resultsPath, 'BDLanova.csv'), sep="\t", quote=FALSE)
BDLanova <- df.anova
save(df.anova, file=file.path(resultsPath, "BDLanova.Rdata"))
```

## Filter factors
The factors are filtered based on acceptance level, with the cutoff for the adjusted p-value being $p_{accept}$. I.e. all factors with a ANOVA with $p_{adjusted} \ge p_{accept}$ are filtered out. The filtered data sets are generated.
``` {r filter}
p.accept = 0.05  # acceptance level
idx.accept = (df.anova$p.holm < p.accept)  # accepted subset
  
# accepted
table(df.anova$p.holm<p.accept)  # 64 rejected / 90 accepted (adjusted)
table(df.anova$p.value<p.accept) # 19 rejected / 135 accepted (unadjusted)
  
# subset of filtered data
BDLdata.fil <- BDLdata[, idx.accept]
BDLmean.fil <- BDLdata[, idx.accept]
```

Heatmap of the filtered BDL data.  
TODO: add the view of the Chips and histopathology
``` {r heatmapBDL, error=TRUE, width=6, height=6}
library('ALL')
# plot of the data subset which is used for the correlation analysis
col2 <- colorRampPalette(c("#67001F", "#B2182B", "#D6604D", "#F4A582", "#FDDBC7",
                           "#FFFFFF", "#D1E5F0", "#92C5DE", "#4393C3", "#2166AC", "#053061"))
heatmap.2(t(as.matrix(BDLdata.fil)), col=col2(100), scale="row", Rowv=FALSE, Colv=FALSE,
          key=TRUE, trace="none", cexRow=0.5, keysize=0.8)

png(filename=file.path(resultsPath, "BDLdata.fil.png"), width=1600, height=1600, res=200)
heatmap.2(t(as.matrix(BDLdata.fil)), col=col2(100), scale="row", Rowv=NULL, Colv=NULL,
          key=TRUE, trace="none", cexRow=0.5, keysize=0.8)
dev.off()
```

# Correlation analysis
The correlation and cluster analysis uses a correlation measure for time series data in combination with Complete-Linkage hierarchical clustering, which provided the best enrichments on gene-expression time-series in a recent comparisons of methods {Jaskowiak2014, Jaskowiak2013}. The time-course experiment includes $n=8$ time points denoted by $t_1,..., t_n$ consisting of a total $p=154$ factors, with $N=5$ repeats per time point. Correlation analysis between factors i and j (i,j=1, ...,p) was performed using a modified correlation coefficient based similarity measure developed for clustering of time-course data ($Y_{i,j}^{S2}$ and $Y_{i,j}^{R2}$) {Son2008}. Yi,jS2and Yi,jR2 are linear combinations of a classical correlation part Ri,j*(Pearson) or Si,j*(Spearman), a component Ai,j*accounting for the similarity in changes between the time courses and a component Mi,j* comparing the location of minimum and maximum
Yi,jS2=1Si,j*+1A<i>,<j>*+2M<i>,<j>*
Yi,jR2=1Ri,j*+1A<i>,<j>*+2M<i>,<j>*
with Ri,j*and Si,j*being calculated on the individual data for factor i and j, A<i>,<j>*and M<i>,<j>*on the mean time courses <i>, <j> averaged over the N replicates.Yi,jS2and Yi,jR2were extended in a simple manner to account for the non-equidistant time points t1,..., tn=0h, 6h, 12h, 18h, 30h, 2d, 5d, 14d in the study design
Yi,jS3=1Si,j*+1A<i>,<j>**+2M<i>,<j>**
Yi,jR3=1Ri,j*+1A<i>,<j>**+2M<i>,<j>**
Herein, Ai,j**calculates the correlation of slopes analogue to the correlation in distances in Ai,j* between factors i and j 
Ai,j**=(Pearson correlation(si,sj)+1)/2
with si=(si1,si2,...,si(n-1)) being the vector of slopes sik=s(i,tk,tk+1) = xi,tk+1- xi,tktk+1-tk
Mi,j**calculates the absolute distance in maximum and minimum times instead of the distance of indices in Mi,j*
Mi,j**=1-|timin-tjmin|+|timax-tjmax|2(tn-t0)
Throughout the analysis the weights were 
For comparison Pearson, Spearman and YS2 and YR2 correlation coefficients were calculated.
See Supporting Information for details.
All computations were performed in R with all source code and data provided in the supplement.


Correlation matrices are calculated using a modified correlation score for the analysis of time course data. Standard Pearson and Spearman scores are calculated as reference values and for comparison.  
Based on the correlation scores hierarchical clustering is performed using complete linkage (hclust).

## Pearson & Spearman
Heatmaps of the Spearman & Pearson correlation matrices are plotted reordered via hierarchical clustering

``` {r correlation}
# correlation matrix
require(corrplot)
cor.pearson <- cor(BDLdata.fil, method="pearson", use="pairwise.complete.obs")
cor.spearman <- cor(BDLdata.fil, method="spearman", use="pairwise.complete.obs")

# Helper function for creating correlation plot and saving to results folder.
f_corrplot <- function(name, data, order, folder="../results",
                       width=1600, height=1600, res=200){
  # TODO: create the plot using the heatmap.2 with color based on histopathology
  fname <- sprintf("%s_%s.png", name, order)
  col2 <- HeatmapColors()
  if (!is.null(folder)){
    png(filename=file.path(folder, "correlation", fname), width=width, height=height, res=res)  
  }
  corrplot(data, order=order, hclust.method="complete", method="color", type="full", 
           tl.cex=0.3, tl.col="black", col=col2(10))
  if (!is.null(folder)){
    invisible(dev.off())  
  }
}

# Spearman
f_corrplot("cor.spearman", data=cor.spearman, order="original", folder=resultsPath)
f_corrplot("cor.spearman", data=cor.spearman, order="hclust", folder=resultsPath)
# Pearson
f_corrplot("cor.pearson", data=cor.pearson, order="original", folder=resultsPath)
f_corrplot("cor.pearson", data=cor.pearson, order="hclust", folder=resultsPath)
```

```{r}
# Spearman correlation with hierarchical clustering
f_corrplot("cor.spearman", data=cor.spearman, order="hclust", folder=NULL)
```

```{r}
# Pearson correlation with hierarchical clustering
f_corrplot("cor.pearson", data=cor.pearson, order="hclust", folder=NULL)
```

## YS & YR correlation
Calculation of time-course based correlation measurements, namely ys1, ys2, ys3, yr1, yr2, yr3
In ys1, ys2, yr1, yr2 all calculations are performed on the mean time course data. The classical correlation components are replaced with the correlations calculated on the individual data points.

``` {r ys1_yr1}
# calculate ys1, yr1 on mean data, i.e. correlation part (S*), slope part (A) and min/max part (M) are all calculated on the mean data of all repeats.
# w <- list(w1=0.5, w2=0.25, w3=0.25)
w <- list(w1=0.34, w2=0.33, w3=0.33)

# calculate all the matrices on the filtered data set (all ys, yr correlation components are calculated on the mean data)
ysr.res <- ysr.matrices(BDLmean.fil, BDLmean.time, use="pairwise.complete.obs")

# Pearson & spearman correlation on full dataset as replacement for the 
# mean Pearson/Spearman in ys and yr
cor.S_star <- ( cor(BDLdata.fil, method="spearman", use="pairwise.complete.obs") + 1 )/2
cor.R_star <- ( cor(BDLdata.fil, method="pearson", use="pairwise.complete.obs") + 1 )/2

# Calculate the ys(1,2,3) and yr(1,2,3) with single time point correlation for
# Spearman, respectively Pearson (instead of mean)
# Takes the individual correlation, slope and min/max components for the respective
# score parts and weights with the provided weighting factors (w1,w2,w3)
cor.ys1.raw <- w$w1*cor.S_star + w$w2*ysr.res$A       + w$w3*ysr.res$M
cor.ys2.raw <- w$w1*cor.S_star + w$w2*ysr.res$A_star  + w$w3*ysr.res$M_star
cor.yr1.raw <- w$w1*cor.R_star + w$w2*ysr.res$A       + w$w3*ysr.res$M
cor.yr2.raw <- w$w1*cor.R_star + w$w2*ysr.res$A_star  + w$w3*ysr.res$M_star
# extended
cor.ys3.raw <- w$w1*cor.S_star + w$w2*ysr.res$A_star2 + w$w3*ysr.res$M_star
cor.yr3.raw <- w$w1*cor.R_star + w$w2*ysr.res$A_star2 + w$w3*ysr.res$M_star 

# scaling of correlation coefficient in interval [-1,1]
cor.ys1 <- 2*(cor.ys1.raw-0.5)
cor.ys2 <- 2*(cor.ys2.raw-0.5)
cor.ys3 <- 2*(cor.ys3.raw-0.5)
cor.yr1 <- 2*(cor.yr1.raw-0.5)
cor.yr2 <- 2*(cor.yr2.raw-0.5)
cor.yr3 <- 2*(cor.yr3.raw-0.5)

# plot subset for further analysis
f_corrplot("cor.ys3", data=cor.ys2, order="hclust", folder=NULL)

# and create all files on disk
f_corrplot("cor.ys1", data=cor.ys1, order="hclust", folder=resultsPath)
f_corrplot("cor.ys2", data=cor.ys2, order="hclust", folder=resultsPath)
f_corrplot("cor.ys3", data=cor.ys3, order="hclust", folder=resultsPath)
f_corrplot("cor.yr1", data=cor.yr1, order="hclust", folder=resultsPath)
f_corrplot("cor.yr2", data=cor.yr2, order="hclust", folder=resultsPath)
f_corrplot("cor.yr3", data=cor.yr3, order="hclust", folder=resultsPath)


# extreme example where the difference between pearson and spearman matters
# plot_cor_pair("Nos2", "Cxcl15")
# plot_cor_pair("albumin", "Cyp2b10")
```

# Hierarchical clustering
The next step of dimension reduction is clustering of the correlation matrix. This groups the factors into sets with the correlation within sets being larger than between sets. This effectivly finds groups of factors which have similar time courses.

The hclust function in R was used for clustering with complete linkage method for hierarchical clustering. This particular clustering method defines the cluster distance between two clusters to be the maximum distance between their individual components.

An overview of the clusters is given in the `results/cluster` folder.

``` {r}

# Plot the clusters
require('matrixStats')

# mean plots for clusters 
f_normalize_centering <- function(a){
  a.norm <- (a - mean(a))/(max(a, na.rm=TRUE) - min(a, na.rm=TRUE))
  return(a.norm)
}

# plot of mean clusters
plot_clusters <- function(method, folder=NULL){  
  # create the figure
  if (!is.null(folder)){
    fname <- sprintf("%s_cluster_overview.png", method)
    path <- file.path(folder, 'cluster', fname)
    print(path)
    png(filename=path, width=1600, height=1600, res=200)
  }
  par(mfrow=c(ceiling(sqrt(Ngroups)),ceiling(sqrt(Ngroups))))
  # par(mfrow=c(2,3))
  
  steps <- 1:8 # time points
    for (k in 1:Ngroups){
      g <- groups.hc.order[groups.hc.order==k]
      N <- ceiling(sqrt(length(g)))
      dgroup <- BDLmean[names(g)]
    
      # centralize and normalize columns, i.e. the individual factors for comparison
      dgroup.norm <- apply(dgroup, 2, f_normalize_centering)
      
      # mean and sd for timepoints 
      g.mean <- rowMeans(dgroup.norm)
      g.sd <- rowSds(dgroup.norm)   # apply(dgroup.norm, 2, sd)
      
      # plot sd range
      plot(1, type="n", xlab="", ylab="", xlim=c(1, 8), ylim=c(-1, 1), main=sprintf("%s : Cluster %s", method, k))
      polygon(c(steps, rev(steps)), c(g.mean+g.sd, rev(g.mean-g.sd)),
              col = rgb(0.5,0.5,0.5,0.5), border = NA)
      
      # individual data
      for (name in names(g)){
        points(steps, dgroup.norm[, name], pch=16, col="black")
        lines(steps, dgroup.norm[, name], col=rgb(0.5,0.5,0.5, 0.8), lwd=1)
      }
      # mean over factors in cluster
      lines(steps, g.mean, col="blue", lwd=2)
    }
    par(mfrow=c(1,1))
  if (!is.null(folder)){
    invisible(dev.off())
  }
}

# Plot individual time courses in cluster
plot_clusters_items <- function(folder=NULL){
  for (k in 1:Ngroups){
    if (!is.null(folder)){
      fname <- sprintf("%s_cluster_%s.png", method, k)
      path <- file.path(folder, 'cluster', fname)
      png(filename=path, width=3000, height=3000, res=200)  
    }
    g <- groups.hc.order[groups.hc.order==k]
    N <- ceiling(sqrt(length(g)))
    par(mfrow=c(N,N))
    for (name in names(g)){
      plot_single(name_A=name) 
    }
    par(mfrow=c(1,1))  
    if (!is.null(folder)){
      invisible(dev.off())
    }
  }  
}

# apply hirarchical clustering based on selected correlation measure
cluster_methods <- c("pearson", "spearman", "ys1", "ys2", "ys3", "yr1", "yr2", "yr3")

for (method in cluster_methods){
  if (identical(method, "ys1")){
    cor.cluster <- cor.ys1  
  }else if (identical(method, "ys2")){
    cor.cluster <- cor.ys2
  }else if (identical(method, "ys3")){
    cor.cluster <- cor.ys3
  }else if (identical(method, "yr1")){
    cor.cluster <- cor.yr1
  }else if (identical(method, "yr2")){
    cor.cluster <- cor.yr2
  }else if (identical(method, "yr3")){
    cor.cluster <- cor.yr3
  }else if (identical(method, "pearson")){
    cor.cluster <- cor.pearson
  }else if (identical(method, "spearman")){
    cor.cluster <- cor.spearman
  }  
  
  # perform hierarchical clustering and cut into Ngroups clusters.
  hc <- hclust(dist(cor.cluster)) 
  Ngroups <-  7
  groups <- cutree(hc, k=Ngroups)
  groups.hc.order <- groups[hc$order]
  
  # plot cluster overview
  plot_clusters(method=method, folder=NULL)
  plot_clusters(method=method, folder=resultsPath)
  
  # plot individual clusters
  plot_clusters_items(folder=resultsPath)

}
```

TODO: overview over the cluster size and content

Display the clusters with the heatmap
``` {r, error=TRUE}
library("gplots")
library("RColorBrewer")
col2 <- HeatmapColors()
Ngroups = 5
hc <- hclust(dist(cor.cluster)) 
# get cluster IDs for the groups
groups <- cutree(hc, k=Ngroups)

# define colors for the Ngroups clusters

# display.brewer.all()
colorset <- brewer.pal(Ngroups, "Set1")
color.map <- function(cluster_id) {return(colorset[cluster_id])}
clusterColors <- unlist(lapply(groups, color.map))

heatmap.2(cor.cluster, col=col2(100), scale="none",
          key=TRUE, symkey=FALSE, trace="none", cexRow=0.5, cexCol=0.5,
          density.info="none", dendrogram="column", Rowv=as.dendrogram(hc), Colv=as.dendrogram(hc), keysize=0.8,
          ColSideColors=clusterColors, revC=TRUE)
```


Overview of the individual time courses in the clusters
```{r, eval=FALSE}

```

# Decision Trees
The decision trees are based on the clusters. This has the large advantage to be not dependent on a single factor, but it uses the combined information available by multiple factors. Consequently, the decision tree is applicable to any subset of factors measured. 

**TODO**

