---
title: Statistical Analysis of Pathobiochemical Signatures in Bile Duct Ligated (BDL)
  Mice
output:
  pdf_document:
    keep_tex: yes
  html_document: default
header-includes: \usepackage{graphicx}
---
<!--
%\VignetteEngine{knitr}
%\VignetteIndexEntry{BDL raw data processing}
-->

# Introduction

This document contains the statistical analysis for the publication Pathobiochemical signatures of cholestatic liver disease in bile duct ligated mice (BMC Systems Biology).

To better understand the cascade of histological and biochemical alterations after bile duct ligation (BDL), a comprehensive data set of serum markers, histological parameters and transcript profiles was compiled at 8 time points after bile duct ligation in mice, comprising different stages of the disease. 
The anlysed data set consists of $N=5$ repeats ($N=3$ for the measured antibodies) for the $n=8$ time points denoted by $t_1,..., t_n$ consisting of a total $p=154$ measured factors (Fluidigm gene expression, antibodies, serum markers, histological measurements).

The main steps of the analysis comprise

* explorative data anaysis and quality checks via ActB
* dimension reduction of data set via ANOVA
* correlation analysis based on time course correlation measure
* hierarchical clustering based on correlation measure
* decision trees based on factors

The complete data set, source code and documentation is available from 
https://github.com/matthiaskoenig/bdl-analysis .

## Read BDL data
In a first step the processed data set is loaded from the `data` folder. The data consists of the time course data for all factors (`BDLdata`), the sample definition, i.e. which sample belongs to which time point and repeat (`BDLsamples`), and a mapping of the Fluidigm (gene) probe ids to UniProt identifiers (`BDLprobes`). 
```{r BDLdata, eval=TRUE}
# definition of paths
baseLoc <- system.file(package="BDLanalysis")
extPath <- file.path(baseLoc, "extdata")
resultsPath <- "/home/mkoenig/git/bdl-analysis/results"

library("calibrate")
library('BDLanalysis')
data(BDLdata)
data(BDLsamples)
data(BDLprobes)
```

In addition to the single measurement data for the factors, the mean data averaged over the $N=5$ repeats is used in the correlation analysis.
```{r BDLmean}
BDLmean <- bdl_mean_data(BDLdata, BDLsamples)
BDLmean.time <- as.numeric(levels(as.factor(BDLsamples$time)))
```

## Explorative data analysis
In a first step overview plots of the raw and mean data for all factors are generated. These are available in the `resultsPath/factors` folder
resultsPath folder. The example plot for `bilirubin` is shown below. 
```{r factorPlots, eval=FALSE}
# Single factor visualization
plot_single_factor(name=colnames(BDLdata)[2])

# Creates plots of all factors in BDLdata
plot_all_factors(path=resultsPath)

# bilirubin
plot_single_factor('bilirubin', path =NULL)
```
The data points are not equidistant: `r levels(BDLsamples$time_fac)`.

### Actb controls
Actb was measured on all Fluidigm chips and serves as quality control of the measurement/correlation analysis. The pairwise correlation between all Actb measurements should result in high correlation values
```{r CheckActB}
# Actb control figure
png(filename=file.path(resultsPath, "Actb_control.png"), width=1600, height=600, res=200)
par(mfrow=c(1,3))
plot_cor_pair("Actb", "Actb.x", single_plots=FALSE)
plot_cor_pair("Actb", "Actb.y", single_plots=FALSE)
plot_cor_pair("Actb.x", "Actb.y", single_plots=FALSE)
par(mfrow=c(1,1))
dev.off()

# calculate the correlations
plot_single("Actb")
plot_single_factor("Actb", path=NULL)
plot_single("Actb.x")
plot_single("Actb.y")
plot_cor_pair("Actb", "Actb.x", single_plots=FALSE)
plot_cor_pair("Actb", "Actb.y", single_plots=FALSE)
plot_cor_pair("Actb.x", "Actb.y", single_plots=FALSE)

# calculate Spearman and Pearson correlation coefficients on mean and individual
# data
actb.spearman <- cor(data.frame(Actb=BDLdata$Actb, 
                                Actb.x=BDLdata$Actb.x, 
                                Actb.y=BDLdata$Actb.y), method="spearman")

actb.spearman.mean <- cor(data.frame(Actb=BDLmean$Actb, 
                                     Actb.x=BDLmean$Actb.x, 
                                     Actb.y=BDLmean$Actb.y), method="spearman")

actb.pearson <- cor(data.frame(Actb=BDLdata$Actb, 
                               Actb.x=BDLdata$Actb.x, 
                               Actb.y=BDLdata$Actb.y), method="pearson")

actb.pearson.mean <- cor(data.frame(Actb=BDLmean$Actb, 
                                    Actb.x=BDLmean$Actb.x, 
                                    Actb.y=BDLmean$Actb.y), method="pearson")
print(actb.spearman)
print(actb.spearman.mean)
print(actb.pearson)
print(actb.pearson.mean)
```

Heatmap of the full data set provides an overview over the data quality
```{r heatmap, fig.width=10, fig.height=10, error=TRUE}
library("gplots")
colors <- HeatmapColors() 
heatmap.2(t(as.matrix(BDLdata)), col=colors(100), scale="row", Rowv=NULL, Colv=NULL,
          key=TRUE, trace="none", cexRow=0.5, keysize=0.8)
```

# Read BDL data
The BDLdata set of all timecourses for the factors is reshaped into matrix form for the ANOVA calculation
```{r BDLmatrix}
BDLmatrices <- bdl_matrix_data(BDLdata, BDLsamples)
print(BDLmatrices[[1]])
```

# Dimension reduction via ANOVA
A one-way analysis of variance (ANOVA) was applied to isolate factors showing significant ($p_{adjusted}< 0.05$) up- or down-regulation during the time course, using the Holm's procedure to correct for any artificial p-value inflation. In its simplest form, ANOVA provides a statistical test of whether or not the means of several groups are equal, and therefore generalizes the t-test to more than two groups, with the groups being the sampled time points. For every of the individual factors in the BDL data set an ANOVA was calculated.Dimension reduction of the BDL data set was than performed by filtering out factors which did not significantly changing over time.

An ANOVA is calculated for all factors (N=`r nrow(BDLdata)`). A data.frame with the p-values of the ANOVA is returned which is used for filtering the BDL data set. Significance codes are added for visual inspection.


## Adjust p-values for multiple testing
A multitude of tests were performed, namely an ANOVA for every single factor. Consequently, the reported p-values of the ANOVA have to be adjusted via multiple testing procedures. Using the p.adjust function which given a set of p-values, returns p-values adjusted using one of several methods. The Bonferroni, Holm, Hochberg, Hommel are designed to give strong control of the family-wise error rate. There seems no reason to use the unmodified Bonferroni correction because it is dominated by Holm's method, which is also valid under arbitrary assumptions.

The correction is performed using Holm  

> Holm, S. (1979). A simple sequentially rejective multiple test procedure. Scandinavian Journal of Statistics 6, 65-70.

# TODO: Add the code for single factor ANOVA


The ANOVA is now calculated for every single factor
```{r ANOVA}
df.anova <- all_factor_anova()
df.anova$sig <- sapply(df.anova$p.value, significant_code)  # add significant codes

df.anova$p.holm <- p.adjust(df.anova$p.value, method ="holm" , n = length(df.anova$p.value))
df.anova$sig.holm <- sapply(df.anova$p.holm, significant_code) 

# order the results py the adjusted p-values
df.anova.ordered <- df.anova[with(df.anova, order(p.holm)), ]
df.anova.ordered

# save the results
write.table(df.anova.ordered, file=file.path(resultsPath, 'BDLanova.csv'), sep="\t", quote=FALSE)
BDLanova <- df.anova
save(df.anova, file=file.path(resultsPath, "BDLanova.Rdata"))
```

## Filter factors
The factors are filtered based on acceptance level, with the cutoff for the adjusted p-value being $p_{accept}$. I.e. all factors with a ANOVA with $p_{adjusted} \ge p_{accept}$ are filtered out. The filtered data sets are generated.
``` {r filter}
p.accept = 0.05  # acceptance level
idx.accept = (df.anova$p.holm < p.accept)  # accepted subset
  
# accepted
table(df.anova$p.holm<p.accept)  # 64 rejected / 90 accepted (adjusted)
table(df.anova$p.value<p.accept) # 19 rejected / 135 accepted (unadjusted)
  
# subset of filtered data
BDLdata.fil <- BDLdata[, idx.accept]
BDLmean.fil <- BDLdata[, idx.accept]
```

Heatmap of the filtered BDL data.  
TODO: add the view of the Chips and histopathology
``` {r heatmapBDL, error=TRUE}
library('ALL')
# plot of the data subset which is used for the correlation analysis
col2 <- colorRampPalette(c("#67001F", "#B2182B", "#D6604D", "#F4A582", "#FDDBC7",
                           "#FFFFFF", "#D1E5F0", "#92C5DE", "#4393C3", "#2166AC", "#053061"))
heatmap.2(t(as.matrix(BDLdata.fil)), col=col2(100), scale="row", Rowv=FALSE, Colv=FALSE,
          key=TRUE, trace="none", cexRow=0.5, keysize=0.8)

png(filename=file.path(resultsPath, "BDLdata.fil.png"), width=1600, height=1600, res=200)
heatmap.2(t(as.matrix(BDLdata.fil)), col=col2(100), scale="row", Rowv=NULL, Colv=NULL,
          key=TRUE, trace="none", cexRow=0.5, keysize=0.8)
dev.off()
```

# Correlation analysis
The correlation and cluster analysis uses a correlation measure for time series data in combination with Complete-Linkage hierarchical clustering, which provided the best enrichments on gene-expression time-series in a recent comparisons of methods {Jaskowiak2014, Jaskowiak2013}. The time-course experiment includes $n=8$ time points denoted by $t_1,..., t_n$ consisting of a total $p=154$ factors, with $N=5$ repeats per time point. Correlation analysis between factors i and j (i,j=1, â€¦,p) was performed using a modified correlation coefficient based similarity measure developed for clustering of time-course data ($Y_{i,j}^{S2}$ and $Y_{i,j}^{R2}$) {Son2008}. Yi,jS2and Yi,jR2 are linear combinations of a classical correlation part Ri,j*(Pearson) or Si,j*(Spearman), a component Ai,j*accounting for the similarity in changes between the time courses and a component Mi,j* comparing the location of minimum and maximum
Yi,jS2=1Si,j*+1A<i>,<j>*+2M<i>,<j>*
Yi,jR2=1Ri,j*+1A<i>,<j>*+2M<i>,<j>*
with Ri,j*and Si,j*being calculated on the individual data for factor i and j, A<i>,<j>*and M<i>,<j>*on the mean time courses <i>, <j> averaged over the N replicates.Yi,jS2and Yi,jR2were extended in a simple manner to account for the non-equidistant time points t1,..., tn=0h, 6h, 12h, 18h, 30h, 2d, 5d, 14d in the study design
Yi,jS3=1Si,j*+1A<i>,<j>**+2M<i>,<j>**
Yi,jR3=1Ri,j*+1A<i>,<j>**+2M<i>,<j>**
Herein, Ai,j**calculates the correlation of slopes analogue to the correlation in distances in Ai,j* between factors i and j 
Ai,j**=(Pearson correlation(si,sj)+1)/2
with si=(si1,si2,...,si(n-1)) being the vector of slopes sik=s(i,tk,tk+1) = xi,tk+1- xi,tktk+1-tk
Mi,j**calculates the absolute distance in maximum and minimum times instead of the distance of indices in Mi,j*
Mi,j**=1-|timin-tjmin|+|timax-tjmax|2(tn-t0)
Throughout the analysis the weights were 
For comparison Pearson, Spearman and YS2 and YR2 correlation coefficients were calculated.
See Supporting Information for details.
All computations were performed in R with all source code and data provided in the supplement.




Correlation matrices are calculated using a modified correlation score for the analysis of time course data. Standard Pearson and Spearman scores are calculated as reference values and for comparison.  
Based on the correlation scores hierarchical clustering is performed using complete linkage (hclust).

Spearman & Pearson correlation matrices are plotted either in normal order or based on the reordering via hierarchical clustering

``` {r correlation}
# correlation matrix
require(corrplot)
cor.pearson <- cor(BDLdata.fil, method="pearson", use="pairwise.complete.obs")
cor.spearman <- cor(BDLdata.fil, method="spearman", use="pairwise.complete.obs")

# Helper function for creating correlation plot and saving to results folder.
f_corrplot <- function(name, data, order, folder="../results",
                       width=1600, height=1600, res=200){
  fname <- sprintf("%s_%s.png", name, order)
  col2 <- HeatmapColors()
  if (!is.null(folder)){
    png(filename=file.path(folder, "correlation", fname), width=width, height=height, res=res)  
  }
  corrplot(data, order=order, hclust.method="complete", method="color", type="full", 
           tl.cex=0.3, tl.col="black", col=col2(10))
  if (!is.null(folder)){
    invisible(dev.off())  
  }
}

# Spearman
f_corrplot("cor.spearman", data=cor.spearman, order="original", folder=resultsPath)
f_corrplot("cor.spearman", data=cor.spearman, order="hclust", folder=resultsPath)
# Pearson
f_corrplot("cor.pearson", data=cor.pearson, order="original", folder=resultsPath)
f_corrplot("cor.pearson", data=cor.pearson, order="hclust", folder=resultsPath)
```

```{r}
# Spearman correlation with hierarchical clustering
f_corrplot("cor.spearman", data=cor.spearman, order="hclust", folder=NULL)
```

```{r}
# Pearson correlation with hierarchical clustering
f_corrplot("cor.pearson", data=cor.pearson, order="hclust", folder=NULL)
```

# YS and YR correlation
Calculation of time-course based correlation measurements, namely ys1, ys2, ys3, yr1, yr2, yr3
In ys1, ys2, yr1, yr2 all calculations are performed on the mean time course data. The classical correlation components are replaced with the correlations calculated on the individual data points.

``` {r ys1_yr1}
# calculate ys1, yr1 on mean data, i.e. correlation part (S*), slope part (A) and min/max part (M) are all calculated on the mean data of all repeats.
w <- list(w1=0.5, w2=0.25, w3=0.25)
ys1.mean <- ys1.df(BDLmean.fil, BDLmean.time, w1=w$w1, w2=w$w2, w3=w$w3, use="pairwise.complete.obs")
ys2.mean <- ys2.df(BDLmean.fil, BDLmean.time, w1=w$w1, w2=w$w2, w3=w$w3, use="pairwise.complete.obs")

# scaling to interval [-1, 1]
# The dimensions correspond to the filtered dataset
cor.ys1.mean <- 2*(ys1.mean$value - 0.5)  
cor.ys2.mean <- 2*(ys2.mean$value - 0.5)

# Pearson & spearman correlation on full dataset as replacement for the 
# mean Pearson/Spearman in ys1, ys2, yr1, yr2
# Now calculate the scores with full correlation
cor.S_star <- ( cor(BDLmean.fil, method="spearman", use="pairwise.complete.obs") + 1 )/2
cor.R_star <- ( cor(BDLmean.fil, method="pearson", use="pairwise.complete.obs") + 1 )/2

# Calculate the ys(1,2,3) and yr(1,2,3) with single time point correlation for
# Spearman, respectively Pearson (instead of mean)
# Takes the individual correlation, slope and min/max components for the respective
# score parts and weights with the provided weighting factors (w1,w2,w3)
cor.ys1.raw <- w$w1*cor.S_star + w$w2*ys1.mean$A + w$w3*ys1.mean$M
cor.ys2.raw <- w$w1*cor.S_star + w$w2*ys2.mean$A_star + w$w3*ys2.mean$M_star

# considering slope and time difference
cor.ys3.raw <- w$w1*cor.S_star + w$w2*ys2.mean$A_star2 + w$w3*ys2.mean$M_star2

cor.yr1.raw <- w$w1*cor.R_star + w$w2*ys1.mean$A + w$w3*ys1.mean$M
cor.yr2.raw <- w$w1*cor.R_star + w$w2*ys2.mean$A_star + w$w3*ys2.mean$M_star
# considering slope and time difference
cor.yr3.raw <- w$w1*cor.R_star + w$w2*ys2.mean$A_star2 + w$w3*ys2.mean$M_star2

# scaling of correlation coefficient in interval [-1,1]
cor.ys1 <- 2*(cor.ys1.raw-0.5)
cor.ys2 <- 2*(cor.ys2.raw-0.5)
cor.ys3 <- 2*(cor.ys3.raw-0.5)
cor.yr1 <- 2*(cor.yr1.raw-0.5)
cor.yr2 <- 2*(cor.yr2.raw-0.5)
cor.yr3 <- 2*(cor.yr3.raw-0.5)

# plot subset for further analysis
f_corrplot("cor.ys2", data=cor.ys2, order="hclust", folder=NULL)

# and create all files on disk
f_corrplot("cor.ys1", data=cor.ys1, order="hclust", folder=resultsPath)
f_corrplot("cor.ys2", data=cor.ys2, order="hclust", folder=resultsPath)
f_corrplot("cor.ys3", data=cor.ys3, order="hclust", folder=resultsPath)
f_corrplot("cor.yr1", data=cor.yr1, order="hclust", folder=resultsPath)
f_corrplot("cor.yr2", data=cor.yr2, order="hclust", folder=resultsPath)
f_corrplot("cor.yr3", data=cor.yr3, order="hclust", folder=resultsPath)


# extreme example where the difference between pearson and spearman matters
# plot_cor_pair("Nos2", "Cxcl15")
# plot_cor_pair("albumin", "Cyp2b10")
```

# Hierarchical clustering
The next step of dimension reduction is clustering of the correlation matrix. This groups the factors into sets with the correlation within sets being larger than between sets. This effectivly finds groups of factors which have similar time courses.

The hclust function in R was used for clustering with complete linkage method for hierarchical clustering. This particular clustering method defines the cluster distance between two clusters to be the maximum distance between their individual components.

An overview of the clusters is given in the `results/cluster` folder.

``` {r}
# apply hirarchical clustering based on selected correlation measure
method = "ys2"
if (identical(method, "ys1")){
  cor.cluster <- cor.ys1  
}else if (identical(method, "ys2")){
  cor.cluster <- cor.ys2
}else if (identical(method, "ys3")){
  cor.cluster <- cor.ys3
}else if (identical(method, "yr1")){
  cor.cluster <- cor.yr1
}else if (identical(method, "yr2")){
  cor.cluster <- cor.yr2
}else if (identical(method, "yr3")){
  cor.cluster <- cor.yr3
}

# perform hierarchical clustering and cut into Ngroups clusters.
hc <- hclust(dist(cor.cluster)) 
Ngroups <-  5
groups <- cutree(hc, k=Ngroups)
groups.hc.order <- groups[hc$order]

# Plot the clusters
require('matrixStats')

# mean plots for clusters 
f_normalize_centering <- function(a){
  a.norm <- (a - mean(a))/(max(a, na.rm=TRUE) - min(a, na.rm=TRUE))
  return(a.norm)
}

# plot of mean clusters
plot_clusters <- function(folder=NULL){
  if (!is.null(folder)){
    fname <- sprintf("%s_cluster_overview.png", method)
    png(filename=sprintf("../results/cluster/%s", fname), width=1600, height=1600, res=200)
  }
  # par(mfrow=c(ceiling(sqrt(Ngroups)),ceiling(sqrt(Ngroups))))
  par(mfrow=c(2,3))
  
  steps <- 1:8
    for (k in 1:Ngroups){
      g <- groups.hc.order[groups.hc.order==k]
      N <- ceiling(sqrt(length(g)))
      dgroup <- BDLmean[names(g)]
    
      # centralize and normalize columns, i.e. the individual factors for comparison
      dgroup.norm <- apply(dgroup, 2, f_normalize_centering)
      
      # mean and sd for timepoints 
      g.mean <- rowMeans(dgroup.norm)
      g.sd <- rowSds(dgroup.norm)   # apply(dgroup.norm, 2, sd)
      
      # plot sd range
      plot(1, type="n", xlab="", ylab="", xlim=c(1, 8), ylim=c(-1, 1), main=sprintf("%s : Cluster %s", method, k))
      polygon(c(steps, rev(steps)), c(g.mean+g.sd, rev(g.mean-g.sd)),
              col = rgb(0.5,0.5,0.5,0.5), border = NA)
      
      # individual data
      for (name in names(g)){
        points(steps, dgroup.norm[, name], pch=16, col="black")
        lines(steps, dgroup.norm[, name], col=rgb(0.5,0.5,0.5, 0.8), lwd=1)
      }
      # mean over factors in cluster
      lines(steps, g.mean, col="blue", lwd=2)
    }
    par(mfrow=c(1,1))
  if (!is.null(folder)){
    invisible(dev.off())
  }
}

plot_clusters(folder=NULL)
```

TODO: overview over the cluster size and content

Display the clusters with the heatmap
``` {r, error=TRUE}
library("gplots")
library("RColorBrewer")
col2 <- HeatmapColors()
Ngroups = 5
hc <- hclust(dist(cor.cluster)) 
# get cluster IDs for the groups
groups <- cutree(hc, k=Ngroups)

# define colors for the Ngroups clusters

# display.brewer.all()
colorset <- brewer.pal(Ngroups, "Set1")
color.map <- function(cluster_id) {return(colorset[cluster_id])}
clusterColors <- unlist(lapply(groups, color.map))

heatmap.2(cor.cluster, col=col2(100), scale="none",
          key=TRUE, symkey=FALSE, trace="none", cexRow=0.5, cexCol=0.5,
          density.info="none", dendrogram="column", Rowv=as.dendrogram(hc), Colv=as.dendrogram(hc), keysize=0.8,
          ColSideColors=clusterColors, revC=TRUE)
```


Overview of the individual time courses in the clusters
```{r, eval=FALSE}
# Plot individual time courses in cluster
plot_clusters_items <- function(folder=NULL){
  for (k in 1:Ngroups){
    if (!is.null(folder)){
      fname <- sprintf("%s_cluster_%s.png", method, k)
      png(filename=sprintf("../results/cluster/%s", fname), width=3000, height=3000, res=200)  
    }
    g <- groups.hc.order[groups.hc.order==k]
    N <- ceiling(sqrt(length(g)))
    par(mfrow=c(N,N))
    for (name in names(g)){
      plot_single(name_A=name) 
    }
    par(mfrow=c(1,1))  
    if (!is.null(folder)){
      invisible(dev.off())
    }
  }  
}

plot_clusters_items(folder=NULL)
```

# Decision Trees
The decision trees are based on the clusters. This has the large advantage to be not dependent on a single factor, but it uses the combined information available by multiple factors. Consequently, the decision tree is applicable to any subset of factors measured. 

**TODO**

